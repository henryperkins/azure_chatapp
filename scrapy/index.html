<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Scrapy 2.12.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster.custom.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster.bundle.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-shadow.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-punk.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-noir.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-light.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/tooltipster-sideTip-borderless.min.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/micromodal.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/sphinx_rtd_theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css" />

  
    <link rel="canonical" href="https://docs.scrapy.org/en/latest/index.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/js/hoverxref.js"></script>
        <script src="_static/js/tooltipster.bundle.min.js"></script>
        <script src="_static/js/micromodal.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Scrapy
          </a>
              <div class="version">
                2.12
              </div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">First steps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/overview">Scrapy at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/install">Installation guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/tutorial">Scrapy Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/examples">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/commands">Command line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spiders">Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/selectors">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/items">Items</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/loaders">Item Loaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/shell">Scrapy shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/item-pipeline">Item Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/feed-exports">Feed exports</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/request-response">Requests and Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/link-extractors">Link Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/settings">Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exceptions">Exceptions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/logging">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/stats">Stats Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/email">Sending e-mail</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/telnetconsole">Telnet Console</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/debug">Debugging Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/contracts">Spiders Contracts</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/practices">Common Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/broad-crawls">Broad Crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/developer-tools">Using your browser’s Developer Tools for scraping</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/dynamic-content">Selecting dynamically-loaded content</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/leaks">Debugging memory leaks</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/media-pipeline">Downloading and processing files and images</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/deploy">Deploying Spiders</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/autothrottle">AutoThrottle extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/jobs">Jobs: pausing and resuming crawls</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/coroutines">Coroutines</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/asyncio">asyncio</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending Scrapy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/architecture">Architecture overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/addons">Add-ons</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/downloader-middleware">Downloader Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spider-middleware">Spider Middleware</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/extensions">Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/signals">Signals</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/scheduler">Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exporters">Item Exporters</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/components">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/api">Core API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-news">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing to Scrapy</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-versioning">Versioning and API stability</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Scrapy</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Scrapy 2.12.0 documentation</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="scrapy-version-documentation">
<span id="topics-index"></span><h1>Scrapy 2.12 documentation<a class="headerlink" href="#scrapy-version-documentation" title="Permalink to this heading">¶</a></h1>
<p>Scrapy is a fast high-level <a class="reference external" href="https://en.wikipedia.org/wiki/Web_crawler">web crawling</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web scraping</a> framework, used
to crawl websites and extract structured data from their pages. It can be used
for a wide range of purposes, from data mining to monitoring and automated
testing.</p>
<section id="getting-help">
<span id="id1"></span><h2>Getting help<a class="headerlink" href="#getting-help" title="Permalink to this heading">¶</a></h2>
<p>Having trouble? We’d like to help!</p>
<ul class="simple">
<li><p>Try the <a class="reference internal" href="index.html#document-faq"><span class="doc">FAQ</span></a> – it’s got answers to some common questions.</p></li>
<li><p>Looking for specific information? Try the <a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a> or <a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a>.</p></li>
<li><p>Ask or search questions in <a class="reference external" href="https://stackoverflow.com/tags/scrapy">StackOverflow using the scrapy tag</a>.</p></li>
<li><p>Ask or search questions in the <a class="reference external" href="https://www.reddit.com/r/scrapy/">Scrapy subreddit</a>.</p></li>
<li><p>Search for questions on the archives of the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users mailing list</a>.</p></li>
<li><p>Ask a question in the <a class="reference external" href="irc://irc.freenode.net/scrapy">#scrapy IRC channel</a>,</p></li>
<li><p>Report bugs with Scrapy in our <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>.</p></li>
<li><p>Join the Discord community <a class="reference external" href="https://discord.com/invite/mv3yErfpvq">Scrapy Discord</a>.</p></li>
</ul>
</section>
<section id="first-steps">
<h2>First steps<a class="headerlink" href="#first-steps" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-intro/overview"></span><section id="scrapy-at-a-glance">
<span id="intro-overview"></span><h3>Scrapy at a glance<a class="headerlink" href="#scrapy-at-a-glance" title="Permalink to this heading">¶</a></h3>
<p>Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and extracting
structured data which can be used for a wide range of useful applications, like
data mining, information processing or historical archival.</p>
<p>Even though Scrapy was originally designed for <a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web scraping</a>, it can also be
used to extract data using APIs (such as <a class="reference external" href="https://affiliate-program.amazon.com/welcome/ecs">Amazon Associates Web Services</a>) or
as a general purpose web crawler.</p>
<section id="walk-through-of-an-example-spider">
<h4>Walk-through of an example spider<a class="headerlink" href="#walk-through-of-an-example-spider" title="Permalink to this heading">¶</a></h4>
<p>In order to show you what Scrapy brings to the table, we’ll walk you through an
example of a Scrapy Spider using the simplest way to run a spider.</p>
<p>Here’s the code for a spider that scrapes famous quotes from website
<a class="reference external" href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>, following the pagination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;https://quotes.toscrape.com/tag/humor/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;span/small/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(&quot;href&quot;)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Put this in a text file, name it something like <code class="docutils literal notranslate"><span class="pre">quotes_spider.py</span></code>
and run the spider using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">quotes_spider</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">.</span><span class="n">jsonl</span>
</pre></div>
</div>
<p>When this finishes you will have in the <code class="docutils literal notranslate"><span class="pre">quotes.jsonl</span></code> file a list of the
quotes in JSON Lines format, containing the text and author, which will look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Jane Austen&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.</span><span class="se">\u201d</span><span class="s2">&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Steve Martin&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">A day without sunshine is like, you know, night.</span><span class="se">\u201d</span><span class="s2">&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Garrison Keillor&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.</span><span class="se">\u201d</span><span class="s2">&quot;</span><span class="p">}</span>
<span class="o">...</span>
</pre></div>
</div>
<section id="what-just-happened">
<h5>What just happened?<a class="headerlink" href="#what-just-happened" title="Permalink to this heading">¶</a></h5>
<p>When you ran the command <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">quotes_spider.py</span></code>, Scrapy looked for a
Spider definition inside it and ran it through its crawler engine.</p>
<p>The crawl started by making requests to the URLs defined in the <code class="docutils literal notranslate"><span class="pre">start_urls</span></code>
attribute (in this case, only the URL for quotes in the <em>humor</em> category)
and called the default callback method <code class="docutils literal notranslate"><span class="pre">parse</span></code>, passing the response object as
an argument. In the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback, we loop through the quote elements
using a CSS Selector, yield a Python dict with the extracted quote text and author,
look for a link to the next page and schedule another request using the same
<code class="docutils literal notranslate"><span class="pre">parse</span></code> method as callback.</p>
<p>Here you will notice one of the main advantages of Scrapy: requests are
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-architecture"><span class="std std-ref">scheduled and processed asynchronously</span></a>.  This
means that Scrapy doesn’t need to wait for a request to be finished and
processed, it can send another request or do other things in the meantime. This
also means that other requests can keep going even if a request fails or an
error happens while handling it.</p>
<p>While this enables you to do very fast crawls (sending multiple concurrent
requests at the same time, in a fault-tolerant way) Scrapy also gives you
control over the politeness of the crawl through <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">a few settings</span></a>. You can do things like setting a download delay between
each request, limiting the amount of concurrent requests per domain or per IP, and
even <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">using an auto-throttling extension</span></a> that tries
to figure these settings out automatically.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> to generate the
JSON file, you can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>, for example).  You can also write an
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipeline</span></a> to store the items in a database.</p>
</div>
</section>
</section>
<section id="what-else">
<span id="topics-whatelse"></span><h4>What else?<a class="headerlink" href="#what-else" title="Permalink to this heading">¶</a></h4>
<p>You’ve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:</p>
<ul class="simple">
<li><p>Built-in support for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selecting and extracting</span></a> data
from HTML/XML sources using extended CSS selectors and XPath expressions,
with helper methods for extraction using regular expressions.</p></li>
<li><p>An <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">interactive shell console</span></a> (IPython aware) for trying
out the CSS and XPath expressions to scrape data, which is very useful when writing or
debugging your spiders.</p></li>
<li><p>Built-in support for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">generating feed exports</span></a> in
multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,
S3, local filesystem)</p></li>
<li><p>Robust encoding support and auto-detection, for dealing with foreign,
non-standard and broken encoding declarations.</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#extending-scrapy"><span class="std std-ref">Strong extensibility support</span></a>, allowing you to plug
in your own functionality using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">signals</span></a> and a
well-defined API (middlewares, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions</span></a>, and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">pipelines</span></a>).</p></li>
<li><p>A wide range of built-in extensions and middlewares for handling:</p>
<ul>
<li><p>cookies and session handling</p></li>
<li><p>HTTP features like compression, authentication, caching</p></li>
<li><p>user-agent spoofing</p></li>
<li><p>robots.txt</p></li>
<li><p>crawl depth restriction</p></li>
<li><p>and more</p></li>
</ul>
</li>
<li><p>A <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet console</span></a> for hooking into a Python
console running inside your Scrapy process, to introspect and debug your
crawler</p></li>
<li><p>Plus other goodies like reusable spiders to crawl sites from <a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a> and
XML/CSV feeds, a media pipeline for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">automatically downloading images</span></a> (or any other media) associated with the scraped
items, a caching DNS resolver, and much more!</p></li>
</ul>
</section>
<section id="what-s-next">
<h4>What’s next?<a class="headerlink" href="#what-s-next" title="Permalink to this heading">¶</a></h4>
<p>The next steps for you are to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-install"><span class="std std-ref">install Scrapy</span></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-tutorial"><span class="std std-ref">follow through the tutorial</span></a> to learn how to create
a full-blown Scrapy project and <a class="reference external" href="https://scrapy.org/community/">join the community</a>. Thanks for your
interest!</p>
</section>
</section>
<span id="document-intro/install"></span><section id="installation-guide">
<span id="intro-install"></span><h3>Installation guide<a class="headerlink" href="#installation-guide" title="Permalink to this heading">¶</a></h3>
<section id="supported-python-versions">
<span id="faq-python-versions"></span><h4>Supported Python versions<a class="headerlink" href="#supported-python-versions" title="Permalink to this heading">¶</a></h4>
<p>Scrapy requires Python 3.9+, either the CPython implementation (default) or
the PyPy implementation (see <a class="reference external" href="https://docs.python.org/3/reference/introduction.html#implementations" title="(in Python v3.13)"><span>Alternate Implementations</span></a>).</p>
</section>
<section id="installing-scrapy">
<span id="intro-install-scrapy"></span><h4>Installing Scrapy<a class="headerlink" href="#installing-scrapy" title="Permalink to this heading">¶</a></h4>
<p>If you’re using <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a>, you can install the package from
the <a class="reference external" href="https://conda-forge.org/">conda-forge</a> channel, which has up-to-date packages for Linux, Windows
and macOS.</p>
<p>To install Scrapy using <code class="docutils literal notranslate"><span class="pre">conda</span></code>, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scrapy</span>
</pre></div>
</div>
<p>Alternatively, if you’re already familiar with installation of Python packages,
you can install Scrapy and its dependencies from PyPI with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Scrapy</span>
</pre></div>
</div>
<p>We strongly recommend that you install Scrapy in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-using-virtualenv"><span class="std std-ref">a dedicated virtualenv</span></a>,
to avoid conflicting with your system packages.</p>
<p>Note that sometimes this may require solving compilation issues for some Scrapy
dependencies depending on your operating system, so be sure to check the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-install-platform-notes"><span class="std std-ref">Platform specific installation notes</span></a>.</p>
<p>For more detailed and platform-specific instructions, as well as
troubleshooting information, read on.</p>
<section id="things-that-are-good-to-know">
<h5>Things that are good to know<a class="headerlink" href="#things-that-are-good-to-know" title="Permalink to this heading">¶</a></h5>
<p>Scrapy is written in pure Python and depends on a few key Python packages (among others):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://lxml.de/index.html">lxml</a>, an efficient XML and HTML parser</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/parsel/">parsel</a>, an HTML/XML data extraction library written on top of lxml,</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/w3lib/">w3lib</a>, a multi-purpose helper for dealing with URLs and web page encodings</p></li>
<li><p><a class="reference external" href="https://twisted.org/">twisted</a>, an asynchronous networking framework</p></li>
<li><p><a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a> and <a class="reference external" href="https://pypi.org/project/pyOpenSSL/">pyOpenSSL</a>, to deal with various network-level security needs</p></li>
</ul>
<p>Some of these packages themselves depend on non-Python packages
that might require additional installation steps depending on your platform.
Please check <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides below</span></a>.</p>
<p>In case of any trouble related to these dependencies,
please refer to their respective installation instructions:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://lxml.de/installation.html">lxml installation</a></p></li>
<li><p><a class="reference external" href="https://cryptography.io/en/latest/installation/" title="(in Cryptography v44.0.0.dev1)"><span class="xref std std-doc">cryptography installation</span></a></p></li>
</ul>
</section>
<section id="using-a-virtual-environment-recommended">
<span id="intro-using-virtualenv"></span><h5>Using a virtual environment (recommended)<a class="headerlink" href="#using-a-virtual-environment-recommended" title="Permalink to this heading">¶</a></h5>
<p>TL;DR: We recommend installing Scrapy inside a virtual environment
on all platforms.</p>
<p>Python packages can be installed either globally (a.k.a system wide),
or in user-space. We do not recommend installing Scrapy system wide.</p>
<p>Instead, we recommend that you install Scrapy within a so-called
“virtual environment” (<a class="reference external" href="https://docs.python.org/3/library/venv.html#module-venv" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">venv</span></code></a>).
Virtual environments allow you to not conflict with already-installed Python
system packages (which could break some of your system tools and scripts),
and still install packages normally with <code class="docutils literal notranslate"><span class="pre">pip</span></code> (without <code class="docutils literal notranslate"><span class="pre">sudo</span></code> and the likes).</p>
<p>See <a class="reference external" href="https://docs.python.org/3/tutorial/venv.html#tut-venv" title="(in Python v3.13)"><span>Virtual Environments and Packages</span></a> on how to create your virtual environment.</p>
<p>Once you have created a virtual environment, you can install Scrapy inside it with <code class="docutils literal notranslate"><span class="pre">pip</span></code>,
just like any other Python package.
(See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides</span></a>
below for non-Python dependencies that you may need to install beforehand).</p>
</section>
</section>
<section id="platform-specific-installation-notes">
<span id="intro-install-platform-notes"></span><h4>Platform specific installation notes<a class="headerlink" href="#platform-specific-installation-notes" title="Permalink to this heading">¶</a></h4>
<section id="windows">
<span id="intro-install-windows"></span><h5>Windows<a class="headerlink" href="#windows" title="Permalink to this heading">¶</a></h5>
<p>Though it’s possible to install Scrapy on Windows using pip, we recommend you
install <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a> and use the package from the
<a class="reference external" href="https://conda-forge.org/">conda-forge</a> channel, which will avoid most installation issues.</p>
<p>Once you’ve installed <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a>, install Scrapy with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scrapy</span>
</pre></div>
</div>
<p>To install Scrapy on Windows using <code class="docutils literal notranslate"><span class="pre">pip</span></code>:</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This installation method requires “Microsoft Visual C++” for installing some
Scrapy dependencies, which demands significantly more disk space than Anaconda.</p>
</div>
<ol class="arabic">
<li><p>Download and execute <a class="reference external" href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">Microsoft C++ Build Tools</a> to install the Visual Studio Installer.</p></li>
<li><p>Run the Visual Studio Installer.</p></li>
<li><p>Under the Workloads section, select <strong>C++ build tools</strong>.</p></li>
<li><p>Check the installation details and make sure following packages are selected as optional components:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>MSVC</strong>  (e.g MSVC v142 - VS 2019 C++ x64/x86 build tools (v14.23) )</p></li>
<li><p><strong>Windows SDK</strong>  (e.g Windows 10 SDK (10.0.18362.0))</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Install the Visual Studio Build Tools.</p></li>
</ol>
<p>Now, you should be able to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-install-scrapy"><span class="std std-ref">install Scrapy</span></a> using <code class="docutils literal notranslate"><span class="pre">pip</span></code>.</p>
</section>
<section id="ubuntu-14-04-or-above">
<span id="intro-install-ubuntu"></span><h5>Ubuntu 14.04 or above<a class="headerlink" href="#ubuntu-14-04-or-above" title="Permalink to this heading">¶</a></h5>
<p>Scrapy is currently tested with recent-enough versions of lxml,
twisted and pyOpenSSL, and is compatible with recent Ubuntu distributions.
But it should support older versions of Ubuntu too, like Ubuntu 14.04,
albeit with potential issues with TLS connections.</p>
<p><strong>Don’t</strong> use the <code class="docutils literal notranslate"><span class="pre">python-scrapy</span></code> package provided by Ubuntu, they are
typically too old and slow to catch up with the latest Scrapy release.</p>
<p>To install Scrapy on Ubuntu (or Ubuntu-based) systems, you need to install
these dependencies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python3</span> <span class="n">python3</span><span class="o">-</span><span class="n">dev</span> <span class="n">python3</span><span class="o">-</span><span class="n">pip</span> <span class="n">libxml2</span><span class="o">-</span><span class="n">dev</span> <span class="n">libxslt1</span><span class="o">-</span><span class="n">dev</span> <span class="n">zlib1g</span><span class="o">-</span><span class="n">dev</span> <span class="n">libffi</span><span class="o">-</span><span class="n">dev</span> <span class="n">libssl</span><span class="o">-</span><span class="n">dev</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python3-dev</span></code>, <code class="docutils literal notranslate"><span class="pre">zlib1g-dev</span></code>, <code class="docutils literal notranslate"><span class="pre">libxml2-dev</span></code> and <code class="docutils literal notranslate"><span class="pre">libxslt1-dev</span></code>
are required for <code class="docutils literal notranslate"><span class="pre">lxml</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">libssl-dev</span></code> and <code class="docutils literal notranslate"><span class="pre">libffi-dev</span></code> are required for <code class="docutils literal notranslate"><span class="pre">cryptography</span></code></p></li>
</ul>
<p>Inside a <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-using-virtualenv"><span class="std std-ref">virtualenv</span></a>,
you can install Scrapy with <code class="docutils literal notranslate"><span class="pre">pip</span></code> after that:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">scrapy</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The same non-Python dependencies can be used to install Scrapy in Debian
Jessie (8.0) and above.</p>
</div>
</section>
<section id="macos">
<span id="intro-install-macos"></span><h5>macOS<a class="headerlink" href="#macos" title="Permalink to this heading">¶</a></h5>
<p>Building Scrapy’s dependencies requires the presence of a C compiler and
development headers. On macOS this is typically provided by Apple’s Xcode
development tools. To install the Xcode command-line tools, open a terminal
window and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xcode</span><span class="o">-</span><span class="n">select</span> <span class="o">--</span><span class="n">install</span>
</pre></div>
</div>
<p>There’s a <a class="reference external" href="https://github.com/pypa/pip/issues/2468">known issue</a> that
prevents <code class="docutils literal notranslate"><span class="pre">pip</span></code> from updating system packages. This has to be addressed to
successfully install Scrapy and its dependencies. Here are some proposed
solutions:</p>
<ul>
<li><p><em>(Recommended)</em> <strong>Don’t</strong> use system Python. Install a new, updated version
that doesn’t conflict with the rest of your system. Here’s how to do it using
the <a class="reference external" href="https://brew.sh/">homebrew</a> package manager:</p>
<ul>
<li><p>Install <a class="reference external" href="https://brew.sh/">homebrew</a> following the instructions in <a class="reference external" href="https://brew.sh/">https://brew.sh/</a></p></li>
<li><p>Update your <code class="docutils literal notranslate"><span class="pre">PATH</span></code> variable to state that homebrew packages should be
used before system packages (Change <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> to <code class="docutils literal notranslate"><span class="pre">.zshrc</span></code> accordingly
if you’re using <a class="reference external" href="https://www.zsh.org/">zsh</a> as default shell):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">echo</span> <span class="s2">&quot;export PATH=/usr/local/bin:/usr/local/sbin:$PATH&quot;</span> <span class="o">&gt;&gt;</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
</li>
<li><p>Reload <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> to ensure the changes have taken place:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
</li>
<li><p>Install python:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">brew</span> <span class="n">install</span> <span class="n">python</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><em>(Optional)</em> <a class="hxr-hoverxref hxr-tooltip reference internal" href="#intro-using-virtualenv"><span class="std std-ref">Install Scrapy inside a Python virtual environment</span></a>.</p></li>
</ul>
<blockquote>
<div><p>This method is a workaround for the above macOS issue, but it’s an overall
good practice for managing dependencies and can complement the first method.</p>
</div></blockquote>
<p>After any of these workarounds you should be able to install Scrapy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Scrapy</span>
</pre></div>
</div>
</section>
<section id="pypy">
<h5>PyPy<a class="headerlink" href="#pypy" title="Permalink to this heading">¶</a></h5>
<p>We recommend using the latest PyPy version.
For PyPy3, only Linux installation was tested.</p>
<p>Most Scrapy dependencies now have binary wheels for CPython, but not for PyPy.
This means that these dependencies will be built during installation.
On macOS, you are likely to face an issue with building the Cryptography
dependency. The solution to this problem is described
<a class="reference external" href="https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481">here</a>,
that is to <code class="docutils literal notranslate"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">openssl</span></code> and then export the flags that this command
recommends (only needed when installing Scrapy). Installing on Linux has no special
issues besides installing build dependencies.
Installing Scrapy with PyPy on Windows is not tested.</p>
<p>You can check that Scrapy is installed correctly by running <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">bench</span></code>.
If this command gives errors such as
<code class="docutils literal notranslate"><span class="pre">TypeError:</span> <span class="pre">...</span> <span class="pre">got</span> <span class="pre">2</span> <span class="pre">unexpected</span> <span class="pre">keyword</span> <span class="pre">arguments</span></code>, this means
that setuptools was unable to pick up one PyPy-specific dependency.
To fix this issue, run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">'PyPyDispatcher&gt;=2.1.0'</span></code>.</p>
</section>
</section>
<section id="troubleshooting">
<span id="intro-install-troubleshooting"></span><h4>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this heading">¶</a></h4>
<section id="attributeerror-module-object-has-no-attribute-op-no-tlsv1-1">
<h5>AttributeError: ‘module’ object has no attribute ‘OP_NO_TLSv1_1’<a class="headerlink" href="#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1" title="Permalink to this heading">¶</a></h5>
<p>After you install or upgrade Scrapy, Twisted or pyOpenSSL, you may get an
exception with the following traceback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[…]
  File &quot;[…]/site-packages/twisted/protocols/tls.py&quot;, line 63, in &lt;module&gt;
    from twisted.internet._sslverify import _setAcceptableProtocols
  File &quot;[…]/site-packages/twisted/internet/_sslverify.py&quot;, line 38, in &lt;module&gt;
    TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
AttributeError: &#39;module&#39; object has no attribute &#39;OP_NO_TLSv1_1&#39;
</pre></div>
</div>
<p>The reason you get this exception is that your system or virtual environment
has a version of pyOpenSSL that your version of Twisted does not support.</p>
<p>To install a version of pyOpenSSL that your version of Twisted supports,
reinstall Twisted with the <code class="code docutils literal notranslate"><span class="pre">tls</span></code> extra option:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">twisted</span><span class="p">[</span><span class="n">tls</span><span class="p">]</span>
</pre></div>
</div>
<p>For details, see <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2473">Issue #2473</a>.</p>
</section>
</section>
</section>
<span id="document-intro/tutorial"></span><section id="scrapy-tutorial">
<span id="intro-tutorial"></span><h3>Scrapy Tutorial<a class="headerlink" href="#scrapy-tutorial" title="Permalink to this heading">¶</a></h3>
<p>In this tutorial, we’ll assume that Scrapy is already installed on your system.
If that’s not the case, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-install"><span class="std std-ref">Installation guide</span></a>.</p>
<p>We are going to scrape <a class="reference external" href="https://quotes.toscrape.com/">quotes.toscrape.com</a>, a website
that lists quotes from famous authors.</p>
<p>This tutorial will walk you through these tasks:</p>
<ol class="arabic simple">
<li><p>Creating a new Scrapy project</p></li>
<li><p>Writing a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">spider</span></a> to crawl a site and extract data</p></li>
<li><p>Exporting the scraped data using the command line</p></li>
<li><p>Changing spider to recursively follow links</p></li>
<li><p>Using spider arguments</p></li>
</ol>
<p>Scrapy is written in <a class="reference external" href="https://www.python.org/">Python</a>. The more you learn about Python, the more you
can get out of Scrapy.</p>
<p>If you’re already familiar with other languages and want to learn Python quickly, the
<a class="reference external" href="https://docs.python.org/3/tutorial">Python Tutorial</a> is a good resource.</p>
<p>If you’re new to programming and want to start with Python, the following books
may be useful to you:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://automatetheboringstuff.com/">Automate the Boring Stuff With Python</a></p></li>
<li><p><a class="reference external" href="http://openbookproject.net/thinkcs/python/english3e/">How To Think Like a Computer Scientist</a></p></li>
<li><p><a class="reference external" href="https://learnpythonthehardway.org/python3/">Learn Python 3 The Hard Way</a></p></li>
</ul>
<p>You can also take a look at <a class="reference external" href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this list of Python resources for non-programmers</a>,
as well as the <a class="reference external" href="https://www.reddit.com/r/learnpython/wiki/index#wiki_new_to_python.3F">suggested resources in the learnpython-subreddit</a>.</p>
<section id="creating-a-project">
<h4>Creating a project<a class="headerlink" href="#creating-a-project" title="Permalink to this heading">¶</a></h4>
<p>Before you start scraping, you will have to set up a new Scrapy project. Enter a
directory where you’d like to store your code and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">startproject</span> <span class="n">tutorial</span>
</pre></div>
</div>
<p>This will create a <code class="docutils literal notranslate"><span class="pre">tutorial</span></code> directory with the following contents:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tutorial</span><span class="o">/</span>
    <span class="n">scrapy</span><span class="o">.</span><span class="n">cfg</span>            <span class="c1"># deploy configuration file</span>

    <span class="n">tutorial</span><span class="o">/</span>             <span class="c1"># project&#39;s Python module, you&#39;ll import your code from here</span>
        <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>

        <span class="n">items</span><span class="o">.</span><span class="n">py</span>          <span class="c1"># project items definition file</span>

        <span class="n">middlewares</span><span class="o">.</span><span class="n">py</span>    <span class="c1"># project middlewares file</span>

        <span class="n">pipelines</span><span class="o">.</span><span class="n">py</span>      <span class="c1"># project pipelines file</span>

        <span class="n">settings</span><span class="o">.</span><span class="n">py</span>       <span class="c1"># project settings file</span>

        <span class="n">spiders</span><span class="o">/</span>          <span class="c1"># a directory where you&#39;ll later put your spiders</span>
            <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="our-first-spider">
<h4>Our first Spider<a class="headerlink" href="#our-first-spider" title="Permalink to this heading">¶</a></h4>
<p>Spiders are classes that you define and that Scrapy uses to scrape information from a website
(or a group of websites). They must subclass <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> and define the initial
requests to be made, and optionally, how to follow links in pages and parse the downloaded
page content to extract data.</p>
<p>This is the code for our first Spider. Save it in a file named
<code class="docutils literal notranslate"><span class="pre">quotes_spider.py</span></code> under the <code class="docutils literal notranslate"><span class="pre">tutorial/spiders</span></code> directory in your project:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">,</span>
            <span class="s2">&quot;https://quotes.toscrape.com/page/2/&quot;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;quotes-</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s2">.html&quot;</span>
        <span class="n">Path</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">write_bytes</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved file </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>As you can see, our Spider subclasses <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Spider</span></code></a>
and defines some attributes and methods:</p>
<ul>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.name" title="scrapy.Spider.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code></a>: identifies the Spider. It must be
unique within a project, that is, you can’t set the same name for different
Spiders.</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a>: must return an iterable of
Requests (you can return a list of requests or write a generator function)
which the Spider will begin to crawl from. Subsequent requests will be
generated successively from these initial requests.</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a>: a method that will be called to handle
the response downloaded for each of the requests made. The response parameter
is an instance of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> that holds
the page content and has further helpful methods to handle it.</p>
<p>The <a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method usually parses the response, extracting
the scraped data as dicts and also finding new URLs to
follow and creating new requests (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>) from them.</p>
</li>
</ul>
<section id="how-to-run-our-spider">
<h5>How to run our spider<a class="headerlink" href="#how-to-run-our-spider" title="Permalink to this heading">¶</a></h5>
<p>To put our spider to work, go to the project’s top level directory and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span>
</pre></div>
</div>
<p>This command runs the spider named <code class="docutils literal notranslate"><span class="pre">quotes</span></code> that we’ve just added, that
will send some requests for the <code class="docutils literal notranslate"><span class="pre">quotes.toscrape.com</span></code> domain. You will get an output
similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="p">(</span><span class="n">omitted</span> <span class="k">for</span> <span class="n">brevity</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">0</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">telnet</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Telnet</span> <span class="n">console</span> <span class="n">listening</span> <span class="n">on</span> <span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">6023</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">404</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">robots</span><span class="o">.</span><span class="n">txt</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">2</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">quotes</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Saved</span> <span class="n">file</span> <span class="n">quotes</span><span class="o">-</span><span class="mf">1.</span><span class="n">html</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">quotes</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Saved</span> <span class="n">file</span> <span class="n">quotes</span><span class="o">-</span><span class="mf">2.</span><span class="n">html</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Closing</span> <span class="n">spider</span> <span class="p">(</span><span class="n">finished</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Now, check the files in the current directory. You should notice that two new
files have been created: <em>quotes-1.html</em> and <em>quotes-2.html</em>, with the content
for the respective URLs, as our <code class="docutils literal notranslate"><span class="pre">parse</span></code> method instructs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are wondering why we haven’t parsed the HTML yet, hold
on, we will cover that soon.</p>
</div>
<section id="what-just-happened-under-the-hood">
<h6>What just happened under the hood?<a class="headerlink" href="#what-just-happened-under-the-hood" title="Permalink to this heading">¶</a></h6>
<p>Scrapy schedules the <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Request</span></code> objects
returned by the <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> method of the Spider. Upon receiving a
response for each one, it instantiates <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects
and calls the callback method associated with the request (in this case, the
<code class="docutils literal notranslate"><span class="pre">parse</span></code> method) passing the response as an argument.</p>
</section>
</section>
<section id="a-shortcut-to-the-start-requests-method">
<h5>A shortcut to the start_requests method<a class="headerlink" href="#a-shortcut-to-the-start-requests-method" title="Permalink to this heading">¶</a></h5>
<p>Instead of implementing a <a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> method
that generates <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Request</span></code> objects from URLs,
you can just define a <a class="reference internal" href="index.html#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> class attribute
with a list of URLs. This list will then be used by the default implementation
of <a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> to create the initial requests
for your spider.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/2/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;quotes-</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s2">.html&quot;</span>
        <span class="n">Path</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">write_bytes</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method will be called to handle each
of the requests for those URLs, even though we haven’t explicitly told Scrapy
to do so. This happens because <a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> is Scrapy’s
default callback method, which is called for requests without an explicitly
assigned callback.</p>
</section>
<section id="extracting-data">
<h5>Extracting data<a class="headerlink" href="#extracting-data" title="Permalink to this heading">¶</a></h5>
<p>The best way to learn how to extract data with Scrapy is trying selectors
using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a>. Run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s1">&#39;https://quotes.toscrape.com/page/1/&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember to always enclose URLs in quotes when running Scrapy shell from the
command line, otherwise URLs containing arguments (i.e. <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> character)
will not work.</p>
<p>On Windows, use double quotes instead:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span>
</pre></div>
</div>
</div>
<p>You will see something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="o">...</span> <span class="n">Scrapy</span> <span class="n">log</span> <span class="n">here</span> <span class="o">...</span> <span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">12</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">27</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">scrapy</span>     <span class="n">scrapy</span> <span class="n">module</span> <span class="p">(</span><span class="n">contains</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">,</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">,</span> <span class="n">etc</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa91d888c90</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">item</span>       <span class="p">{}</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">request</span>    <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">response</span>   <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">settings</span>   <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">Settings</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa91d888c10</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">spider</span>     <span class="o">&lt;</span><span class="n">DefaultSpider</span> <span class="s1">&#39;default&#39;</span> <span class="n">at</span> <span class="mh">0x7fa91c8af990</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Useful</span> <span class="n">shortcuts</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">shelp</span><span class="p">()</span>           <span class="n">Shell</span> <span class="n">help</span> <span class="p">(</span><span class="nb">print</span> <span class="n">this</span> <span class="n">help</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">req_or_url</span><span class="p">)</span> <span class="n">Fetch</span> <span class="n">request</span> <span class="p">(</span><span class="ow">or</span> <span class="n">URL</span><span class="p">)</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>    <span class="n">View</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">browser</span>
</pre></div>
</div>
<p>Using the shell, you can try selecting elements using <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> with the response
object:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;descendant-or-self::title&#39; data=&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;&gt;]</span>
</pre></div>
</div>
<p>The result of running <code class="docutils literal notranslate"><span class="pre">response.css('title')</span></code> is a list-like object called
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>, which represents a list of
<code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> objects that wrap around XML/HTML elements
and allow you to run further queries to refine the selection or extract the
data.</p>
<p>To extract the text from the title above, you can do:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;Quotes to Scrape&#39;]</span>
</pre></div>
</div>
<p>There are two things to note here: one is that we’ve added <code class="docutils literal notranslate"><span class="pre">::text</span></code> to the
CSS query, to mean we want to select only the text elements directly inside
<code class="docutils literal notranslate"><span class="pre">&lt;title&gt;</span></code> element.  If we don’t specify <code class="docutils literal notranslate"><span class="pre">::text</span></code>, we’d get the full title
element, including its tags:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;]</span>
</pre></div>
</div>
<p>The other thing is that the result of calling <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> is a list: it is
possible that a selector returns more than one result, so we extract them all.
When you know you just want the first result, as in this case, you can do:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>As an alternative, you could’ve written:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>Accessing an index on a <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance will
raise an <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#IndexError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IndexError</span></code></a> exception if there are no results:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;noelement&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="c">...</span>
<span class="gr">IndexError</span>: <span class="n">list index out of range</span>
</pre></div>
</div>
<p>You might want to use <code class="docutils literal notranslate"><span class="pre">.get()</span></code> directly on the
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance instead, which returns <code class="docutils literal notranslate"><span class="pre">None</span></code>
if there are no results:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;noelement&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</pre></div>
</div>
<p>There’s a lesson here: for most scraping code, you want it to be resilient to
errors due to things not being found on a page, so that even if some parts fail
to be scraped, you can at least get <strong>some</strong> data.</p>
<p>Besides the <a class="reference internal" href="index.html#scrapy.selector.SelectorList.getall" title="scrapy.selector.SelectorList.getall"><code class="xref py py-meth docutils literal notranslate"><span class="pre">getall()</span></code></a> and
<a class="reference internal" href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a> methods, you can also use
the <a class="reference internal" href="index.html#scrapy.selector.SelectorList.re" title="scrapy.selector.SelectorList.re"><code class="xref py py-meth docutils literal notranslate"><span class="pre">re()</span></code></a> method to extract using
<a class="reference external" href="https://docs.python.org/3/library/re.html" title="(in Python v3.13)"><span class="xref std std-doc">regular expressions</span></a>:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Quotes.*&quot;</span><span class="p">)</span>
<span class="go">[&#39;Quotes to Scrape&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Q\w+&quot;</span><span class="p">)</span>
<span class="go">[&#39;Quotes&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(\w+) to (\w+)&quot;</span><span class="p">)</span>
<span class="go">[&#39;Quotes&#39;, &#39;Scrape&#39;]</span>
</pre></div>
</div>
<p>In order to find the proper CSS selectors to use, you might find it useful to open
the response page from the shell in your web browser using <code class="docutils literal notranslate"><span class="pre">view(response)</span></code>.
You can use your browser’s developer tools to inspect the HTML and come up
with a selector (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-developer-tools"><span class="std std-ref">Using your browser’s Developer Tools for scraping</span></a>).</p>
<p><a class="reference external" href="https://selectorgadget.com/">Selector Gadget</a> is also a nice tool to quickly find CSS selector for
visually selected elements, which works in many browsers.</p>
<section id="xpath-a-brief-intro">
<h6>XPath: a brief intro<a class="headerlink" href="#xpath-a-brief-intro" title="Permalink to this heading">¶</a></h6>
<p>Besides <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a>, Scrapy selectors also support using <a class="reference external" href="https://www.w3.org/TR/xpath-10/">XPath</a> expressions:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;//title&#39; data=&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>XPath expressions are very powerful, and are the foundation of Scrapy
Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You
can see that if you read the text representation of the selector
objects in the shell closely.</p>
<p>While perhaps not as popular as CSS selectors, XPath expressions offer more
power because besides navigating the structure, it can also look at the
content. Using XPath, you’re able to select things like: <em>the link
that contains the text “Next Page”</em>. This makes XPath very fitting to the task
of scraping, and we encourage you to learn XPath even if you already know how to
construct CSS selectors, it will make scraping much easier.</p>
<p>We won’t cover much of XPath here, but you can read more about <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">using XPath
with Scrapy Selectors here</span></a>. To learn more about XPath, we
recommend <a class="reference external" href="http://zvon.org/comp/r/tut-XPath_1.html">this tutorial to learn XPath through examples</a>, and <a class="reference external" href="http://plasmasturm.org/log/xpath101/">this tutorial to learn “how
to think in XPath”</a>.</p>
</section>
<section id="extracting-quotes-and-authors">
<h6>Extracting quotes and authors<a class="headerlink" href="#extracting-quotes-and-authors" title="Permalink to this heading">¶</a></h6>
<p>Now that you know a bit about selection and extraction, let’s complete our
spider by writing the code to extract the quotes from the web page.</p>
<p>Each quote in <a class="reference external" href="https://quotes.toscrape.com">https://quotes.toscrape.com</a> is represented by HTML elements that look
like this:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>
        by <span class="p">&lt;</span><span class="nt">small</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;author&quot;</span><span class="p">&gt;</span>Albert Einstein<span class="p">&lt;/</span><span class="nt">small</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/author/Albert-Einstein&quot;</span><span class="p">&gt;</span>(about)<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>
        Tags:
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/change/page/1/&quot;</span><span class="p">&gt;</span>change<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/deep-thoughts/page/1/&quot;</span><span class="p">&gt;</span>deep-thoughts<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/thinking/page/1/&quot;</span><span class="p">&gt;</span>thinking<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/world/page/1/&quot;</span><span class="p">&gt;</span>world<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>Let’s open up scrapy shell and play a bit to find out how to extract the data
we want:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s1">&#39;https://quotes.toscrape.com&#39;</span>
</pre></div>
</div>
<p>We get a list of selectors for the quote HTML elements with:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&quot;descendant-or-self::div[@class and contains(concat(&#39; &#39;, normalize-space(@class), &#39; &#39;), &#39; quote &#39;)]&quot; data=&#39;&lt;div class=&quot;quote&quot; itemscope itemtype...&#39;&gt;,</span>
<span class="go">&lt;Selector query=&quot;descendant-or-self::div[@class and contains(concat(&#39; &#39;, normalize-space(@class), &#39; &#39;), &#39; quote &#39;)]&quot; data=&#39;&lt;div class=&quot;quote&quot; itemscope itemtype...&#39;&gt;,</span>
<span class="go">...]</span>
</pre></div>
</div>
<p>Each of the selectors returned by the query above allows us to run further
queries over their sub-elements. Let’s assign the first selector to a
variable, so that we can run our CSS selectors directly on a particular quote:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">quote</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, let’s extract the <code class="docutils literal notranslate"><span class="pre">text</span></code>, <code class="docutils literal notranslate"><span class="pre">author</span></code> and <code class="docutils literal notranslate"><span class="pre">tags</span></code> from that quote
using the <code class="docutils literal notranslate"><span class="pre">quote</span></code> object we just created:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span>
<span class="go">&#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">author</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">author</span>
<span class="go">&#39;Albert Einstein&#39;</span>
</pre></div>
</div>
<p>Given that the tags are a list of strings, we can use the <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> method
to get all of them:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tags</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tags</span>
<span class="go">[&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;]</span>
</pre></div>
</div>
<p>Having figured out how to extract each bit, we can now iterate over all the
quote elements and put them together into a Python dictionary:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">text</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">author</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">tags</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="n">author</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">))</span>
<span class="gp">...</span>
<span class="go">{&#39;text&#39;: &#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;, &#39;author&#39;: &#39;Albert Einstein&#39;, &#39;tags&#39;: [&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;]}</span>
<span class="go">{&#39;text&#39;: &#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#39;, &#39;author&#39;: &#39;J.K. Rowling&#39;, &#39;tags&#39;: [&#39;abilities&#39;, &#39;choices&#39;]}</span>
<span class="go">...</span>
</pre></div>
</div>
</section>
</section>
<section id="extracting-data-in-our-spider">
<h5>Extracting data in our spider<a class="headerlink" href="#extracting-data-in-our-spider" title="Permalink to this heading">¶</a></h5>
<p>Let’s get back to our spider. Until now, it hasn’t extracted any data in
particular, just saving the whole HTML page to a local file. Let’s integrate the
extraction logic above into our spider.</p>
<p>A Scrapy spider typically generates many dictionaries containing the data
extracted from the page. To do that, we use the <code class="docutils literal notranslate"><span class="pre">yield</span></code> Python keyword
in the callback, as you can see below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/2/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;tags&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>
</pre></div>
</div>
<p>To run this spider, exit the scrapy shell by entering:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">quit</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span>
</pre></div>
</div>
<p>Now, it should output the extracted data with the log:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">18</span><span class="p">:</span><span class="mi">57</span><span class="p">:</span><span class="mi">19</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scraper</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Scraped</span> <span class="kn">from</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">{</span><span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;life&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">],</span> <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;André Gide&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;“It is better to be hated for what you are than to be loved for what you are not.”&#39;</span><span class="p">}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">18</span><span class="p">:</span><span class="mi">57</span><span class="p">:</span><span class="mi">19</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scraper</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Scraped</span> <span class="kn">from</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">{</span><span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;edison&#39;</span><span class="p">,</span> <span class="s1">&#39;failure&#39;</span><span class="p">,</span> <span class="s1">&#39;inspirational&#39;</span><span class="p">,</span> <span class="s1">&#39;paraphrased&#39;</span><span class="p">],</span> <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;Thomas A. Edison&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s2">&quot;“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”&quot;</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="storing-the-scraped-data">
<span id="storing-data"></span><h4>Storing the scraped data<a class="headerlink" href="#storing-the-scraped-data" title="Permalink to this heading">¶</a></h4>
<p>The simplest way to store the scraped data is by using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>, with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">O</span> <span class="n">quotes</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>That will generate a <code class="docutils literal notranslate"><span class="pre">quotes.json</span></code> file containing all scraped items,
serialized in <a class="reference external" href="https://en.wikipedia.org/wiki/JSON">JSON</a>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">-O</span></code> command-line switch overwrites any existing file; use <code class="docutils literal notranslate"><span class="pre">-o</span></code> instead
to append new content to any existing file. However, appending to a JSON file
makes the file contents invalid JSON. When appending to a file, consider
using a different serialization format, such as <a class="reference external" href="https://jsonlines.org">JSON Lines</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">.</span><span class="n">jsonl</span>
</pre></div>
</div>
<p>The <a class="reference external" href="https://jsonlines.org">JSON Lines</a> format is useful because it’s stream-like, so you can easily
append new records to it. It doesn’t have the same problem as JSON when you run
twice. Also, as each record is a separate line, you can process big files
without having to fit everything in memory, there are tools like <a class="reference external" href="https://stedolan.github.io/jq">JQ</a> to help
do that at the command-line.</p>
<p>In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>. A placeholder file
for Item Pipelines has been set up for you when the project is created, in
<code class="docutils literal notranslate"><span class="pre">tutorial/pipelines.py</span></code>. Though you don’t need to implement any item
pipelines if you just want to store the scraped items.</p>
</section>
<section id="following-links">
<h4>Following links<a class="headerlink" href="#following-links" title="Permalink to this heading">¶</a></h4>
<p>Let’s say, instead of just scraping the stuff from the first two pages
from <a class="reference external" href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>, you want quotes from all the pages in the website.</p>
<p>Now that you know how to extract data from pages, let’s see how to follow links
from them.</p>
<p>The first thing to do is extract the link to the page we want to follow.  Examining
our page, we can see there is a link to the next page with the following
markup:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">ul</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;pager&quot;</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">li</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;next&quot;</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/page/2/&quot;</span><span class="p">&gt;</span>Next <span class="p">&lt;</span><span class="nt">span</span> <span class="na">aria-hidden</span><span class="o">=</span><span class="s">&quot;true&quot;</span><span class="p">&gt;</span><span class="ni">&amp;rarr;</span><span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">ul</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>We can try extracting it in the shell:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&#39;</span>
</pre></div>
</div>
<p>This gets the anchor element, but we want the attribute <code class="docutils literal notranslate"><span class="pre">href</span></code>. For that,
Scrapy supports a CSS extension that lets you select the attribute contents,
like this:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;/page/2/&#39;</span>
</pre></div>
</div>
<p>There is also an <code class="docutils literal notranslate"><span class="pre">attrib</span></code> property available
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a> for more):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;href&quot;</span><span class="p">]</span>
<span class="go">&#39;/page/2/&#39;</span>
</pre></div>
</div>
<p>Now let’s see our spider, modified to recursively follow the link to the next
page, extracting data from it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;tags&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_page</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, after extracting the data, the <code class="docutils literal notranslate"><span class="pre">parse()</span></code> method looks for the link to
the next page, builds a full absolute URL using the
<a class="reference internal" href="index.html#scrapy.http.Response.urljoin" title="scrapy.http.Response.urljoin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">urljoin()</span></code></a> method (since the links can be
relative) and yields a new request to the next page, registering itself as
callback to handle the data extraction for the next page and to keep the
crawling going through all the pages.</p>
<p>What you see here is Scrapy’s mechanism of following links: when you yield
a Request in a callback method, Scrapy will schedule that request to be sent
and register a callback method to be executed when that request finishes.</p>
<p>Using this, you can build complex crawlers that follow links according to rules
you define, and extract different kinds of data depending on the page it’s
visiting.</p>
<p>In our example, it creates a sort of loop, following all the links to the next page
until it doesn’t find one – handy for crawling blogs, forums and other sites with
pagination.</p>
<section id="a-shortcut-for-creating-requests">
<span id="response-follow-example"></span><h5>A shortcut for creating Requests<a class="headerlink" href="#a-shortcut-for-creating-requests" title="Permalink to this heading">¶</a></h5>
<p>As a shortcut for creating Request objects you can use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span small::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;tags&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Unlike scrapy.Request, <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> supports relative URLs directly - no
need to call urljoin. Note that <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> just returns a Request
instance; you still have to yield this Request.</p>
<p>You can also pass a selector to <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> instead of a string;
this selector should extract necessary attributes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;ul.pager a::attr(href)&quot;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">href</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>For <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> elements there is a shortcut: <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> uses their href
attribute automatically. So the code can be shortened further:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;ul.pager a&quot;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>To create multiple requests from an iterable, you can use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.follow_all" title="scrapy.http.TextResponse.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow_all</span></code></a> instead:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">anchors</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;ul.pager a&quot;</span><span class="p">)</span>
<span class="k">yield from</span> <span class="n">response</span><span class="o">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>or, shortening it further:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">yield from</span> <span class="n">response</span><span class="o">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">css</span><span class="o">=</span><span class="s2">&quot;ul.pager a&quot;</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="more-examples-and-patterns">
<h5>More examples and patterns<a class="headerlink" href="#more-examples-and-patterns" title="Permalink to this heading">¶</a></h5>
<p>Here is another spider that illustrates callbacks and following links,
this time for scraping author information:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">AuthorSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;author&quot;</span>

    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://quotes.toscrape.com/&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">author_page_links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;.author + a&quot;</span><span class="p">)</span>
        <span class="k">yield from</span> <span class="n">response</span><span class="o">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">author_page_links</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_author</span><span class="p">)</span>

        <span class="n">pagination_links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a&quot;</span><span class="p">)</span>
        <span class="k">yield from</span> <span class="n">response</span><span class="o">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">pagination_links</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_author</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">extract_with_css</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="k">yield</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s2">&quot;h3.author-title::text&quot;</span><span class="p">),</span>
            <span class="s2">&quot;birthdate&quot;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s2">&quot;.author-born-date::text&quot;</span><span class="p">),</span>
            <span class="s2">&quot;bio&quot;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s2">&quot;.author-description::text&quot;</span><span class="p">),</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>This spider will start from the main page, it will follow all the links to the
authors pages calling the <code class="docutils literal notranslate"><span class="pre">parse_author</span></code> callback for each of them, and also
the pagination links with the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback as we saw before.</p>
<p>Here we’re passing callbacks to
<a class="reference internal" href="index.html#scrapy.http.TextResponse.follow_all" title="scrapy.http.TextResponse.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow_all</span></code></a> as positional
arguments to make the code shorter; it also works for
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">parse_author</span></code> callback defines a helper function to extract and cleanup the
data from a CSS query and yields the Python dict with the author data.</p>
<p>Another interesting thing this spider demonstrates is that, even if there are
many quotes from the same author, we don’t need to worry about visiting the
same author page multiple times. By default, Scrapy filters out duplicated
requests to URLs already visited, avoiding the problem of hitting servers too
much because of a programming mistake. This can be configured in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> setting.</p>
<p>Hopefully by now you have a good understanding of how to use the mechanism
of following links and callbacks with Scrapy.</p>
<p>As yet another example spider that leverages the mechanism of following links,
check out the <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a> class for a generic
spider that implements a small rules engine that you can use to write your
crawlers on top of it.</p>
<p>Also, a common pattern is to build an item with data from more than one page,
using a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">trick to pass additional data to the callbacks</span></a>.</p>
</section>
</section>
<section id="using-spider-arguments">
<h4>Using spider arguments<a class="headerlink" href="#using-spider-arguments" title="Permalink to this heading">¶</a></h4>
<p>You can provide command line arguments to your spiders by using the <code class="docutils literal notranslate"><span class="pre">-a</span></code>
option when running them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">O</span> <span class="n">quotes</span><span class="o">-</span><span class="n">humor</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">a</span> <span class="n">tag</span><span class="o">=</span><span class="n">humor</span>
</pre></div>
</div>
<p>These arguments are passed to the Spider’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method and become
spider attributes by default.</p>
<p>In this example, the value provided for the <code class="docutils literal notranslate"><span class="pre">tag</span></code> argument will be available
via <code class="docutils literal notranslate"><span class="pre">self.tag</span></code>. You can use this to make your spider fetch only quotes
with a specific tag, building the URL based on the argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://quotes.toscrape.com/&quot;</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tag&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tag</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">url</span> <span class="o">+</span> <span class="s2">&quot;tag/&quot;</span> <span class="o">+</span> <span class="n">tag</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;li.next a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>If you pass the <code class="docutils literal notranslate"><span class="pre">tag=humor</span></code> argument to this spider, you’ll notice that it
will only visit URLs from the <code class="docutils literal notranslate"><span class="pre">humor</span></code> tag, such as
<code class="docutils literal notranslate"><span class="pre">https://quotes.toscrape.com/tag/humor</span></code>.</p>
<p>You can <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">learn more about handling spider arguments here</span></a>.</p>
</section>
<section id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h4>
<p>This tutorial covered only the basics of Scrapy, but there’s a lot of other
features not mentioned here. Check the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-whatelse"><span class="std std-ref">What else?</span></a> section in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-overview"><span class="std std-ref">Scrapy at a glance</span></a> chapter for a quick overview of the most important ones.</p>
<p>You can continue from the section <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#section-basics"><span class="std std-ref">Basic concepts</span></a> to know more about the
command-line tool, spiders, selectors and other things the tutorial hasn’t covered like
modeling the scraped data. If you’d prefer to play with an example project, check
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-examples"><span class="std std-ref">Examples</span></a> section.</p>
</section>
</section>
<span id="document-intro/examples"></span><section id="examples">
<span id="intro-examples"></span><h3>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h3>
<p>The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a>, that you can use to
play and learn more about Scrapy. It contains two spiders for
<a class="reference external" href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>, one using CSS selectors and another one using XPath
expressions.</p>
<p>The <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a> project is available at: <a class="reference external" href="https://github.com/scrapy/quotesbot">https://github.com/scrapy/quotesbot</a>.
You can find more information about it in the project’s README.</p>
<p>If you’re familiar with git, you can checkout the code. Otherwise you can
download the project as a zip file by clicking
<a class="reference external" href="https://github.com/scrapy/quotesbot/archive/master.zip">here</a>.</p>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-intro/overview"><span class="doc">Scrapy at a glance</span></a></dt><dd><p>Understand what Scrapy is and how it can help you.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-intro/install"><span class="doc">Installation guide</span></a></dt><dd><p>Get Scrapy installed on your computer.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-intro/tutorial"><span class="doc">Scrapy Tutorial</span></a></dt><dd><p>Write your first Scrapy project.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-intro/examples"><span class="doc">Examples</span></a></dt><dd><p>Learn more by playing with a pre-made Scrapy project.</p>
</dd>
</dl>
</section>
<section id="basic-concepts">
<span id="section-basics"></span><h2>Basic concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/commands"></span><section id="command-line-tool">
<span id="topics-commands"></span><h3>Command line tool<a class="headerlink" href="#command-line-tool" title="Permalink to this heading">¶</a></h3>
<p>Scrapy is controlled through the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command-line tool, to be referred to
here as the “Scrapy tool” to differentiate it from the sub-commands, which we
just call “commands” or “Scrapy commands”.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.</p>
<p>(The <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">deploy</span></code> command has been removed in 1.0 in favor of the
standalone <code class="docutils literal notranslate"><span class="pre">scrapyd-deploy</span></code>. See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">Deploying your project</a>.)</p>
<section id="configuration-settings">
<span id="topics-config-settings"></span><h4>Configuration settings<a class="headerlink" href="#configuration-settings" title="Permalink to this heading">¶</a></h4>
<p>Scrapy will look for configuration parameters in ini-style <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> files
in standard locations:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/etc/scrapy.cfg</span></code> or <code class="docutils literal notranslate"><span class="pre">c:\scrapy\scrapy.cfg</span></code> (system-wide),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">~/.config/scrapy.cfg</span></code> (<code class="docutils literal notranslate"><span class="pre">$XDG_CONFIG_HOME</span></code>) and <code class="docutils literal notranslate"><span class="pre">~/.scrapy.cfg</span></code> (<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>)
for global (user-wide) settings, and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> inside a Scrapy project’s root (see next section).</p></li>
</ol>
<p>Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults
and project-wide settings will override all others, when defined.</p>
<p>Scrapy also understands, and can be configured through, a number of environment
variables. Currently these are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings-module-envvar"><span class="std std-ref">Designating the settings</span></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SCRAPY_PROJECT</span></code> (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-project-envvar"><span class="std std-ref">Sharing the root directory between projects</span></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a>)</p></li>
</ul>
</section>
<section id="default-structure-of-scrapy-projects">
<span id="topics-project-structure"></span><h4>Default structure of Scrapy projects<a class="headerlink" href="#default-structure-of-scrapy-projects" title="Permalink to this heading">¶</a></h4>
<p>Before delving into the command-line tool and its sub-commands, let’s first
understand the directory structure of a Scrapy project.</p>
<p>Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scrapy.cfg
myproject/
    __init__.py
    items.py
    middlewares.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
</pre></div>
</div>
<p>The directory where the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file resides is known as the <em>project
root directory</em>. That file contains the name of the python module that defines
the project settings. Here is an example:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[settings]</span>
<span class="na">default</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">myproject.settings</span>
</pre></div>
</div>
</section>
<section id="sharing-the-root-directory-between-projects">
<span id="topics-project-envvar"></span><h4>Sharing the root directory between projects<a class="headerlink" href="#sharing-the-root-directory-between-projects" title="Permalink to this heading">¶</a></h4>
<p>A project root directory, the one that contains the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code>, may be
shared by multiple Scrapy projects, each with its own settings module.</p>
<p>In that case, you must define one or more aliases for those settings modules
under <code class="docutils literal notranslate"><span class="pre">[settings]</span></code> in your <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[settings]</span>
<span class="na">default</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">myproject1.settings</span>
<span class="na">project1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">myproject1.settings</span>
<span class="na">project2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">myproject2.settings</span>
</pre></div>
</div>
<p>By default, the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command-line tool will use the <code class="docutils literal notranslate"><span class="pre">default</span></code> settings.
Use the <code class="docutils literal notranslate"><span class="pre">SCRAPY_PROJECT</span></code> environment variable to specify a different project
for <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> to use:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
</pre></div>
</div>
</section>
<section id="using-the-scrapy-tool">
<h4>Using the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool<a class="headerlink" href="#using-the-scrapy-tool" title="Permalink to this heading">¶</a></h4>
<p>You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Scrapy X.Y - no active project

Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
</pre></div>
</div>
<p>The first line will print the currently active project if you’re inside a
Scrapy project. In this example it was run from outside a project. If run from inside
a project it would have printed something like this:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Scrapy X.Y - project: myproject

Usage:
  scrapy &lt;command&gt; [options] [args]

[...]
</pre></div>
</div>
<section id="creating-projects">
<h5>Creating projects<a class="headerlink" href="#creating-projects" title="Permalink to this heading">¶</a></h5>
<p>The first thing you typically do with the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool is create your Scrapy
project:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scrapy startproject myproject [project_dir]
</pre></div>
</div>
<p>That will create a Scrapy project under the <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> directory.
If <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal notranslate"><span class="pre">myproject</span></code>.</p>
<p>Next, you go inside the new project directory:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>cd project_dir
</pre></div>
</div>
<p>And you’re ready to use the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command to manage and control your
project from there.</p>
</section>
<section id="controlling-projects">
<h5>Controlling projects<a class="headerlink" href="#controlling-projects" title="Permalink to this heading">¶</a></h5>
<p>You use the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool from inside your projects to control and manage
them.</p>
<p>For example, to create a new spider:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scrapy genspider mydomain mydomain.com
</pre></div>
</div>
<p>Some Scrapy commands (like <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a>) must be run from inside a Scrapy
project. See the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-commands-ref"><span class="std std-ref">commands reference</span></a> below for more
information on which commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider-overridden behaviours (such as the <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> attribute to override
the user-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the <code class="docutils literal notranslate"><span class="pre">fetch</span></code> command is meant to be used to
check how spiders are downloading pages.</p>
</section>
</section>
<section id="available-tool-commands">
<span id="topics-commands-ref"></span><h4>Available tool commands<a class="headerlink" href="#available-tool-commands" title="Permalink to this heading">¶</a></h4>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more info
about each command by running:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scrapy &lt;command&gt; -h
</pre></div>
</div>
<p>And you can see all available commands with:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>scrapy -h
</pre></div>
</div>
<p>There are two kinds of commands, those that only work from inside a Scrapy
project (Project-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly differently
when run from inside a project (as they would use the project overridden
settings).</p>
<p>Global commands:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-settings"><code class="xref std std-command docutils literal notranslate"><span class="pre">settings</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-view"><code class="xref std std-command docutils literal notranslate"><span class="pre">view</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-version"><code class="xref std std-command docutils literal notranslate"><span class="pre">version</span></code></a></p></li>
</ul>
<p>Project-only commands:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-list"><code class="xref std std-command docutils literal notranslate"><span class="pre">list</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-command-bench"><code class="xref std std-command docutils literal notranslate"><span class="pre">bench</span></code></a></p></li>
</ul>
<section id="startproject">
<span id="std-command-startproject"></span><h5>startproject<a class="headerlink" href="#startproject" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">&lt;project_name&gt;</span> <span class="pre">[project_dir]</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Creates a new Scrapy project named <code class="docutils literal notranslate"><span class="pre">project_name</span></code>, under the <code class="docutils literal notranslate"><span class="pre">project_dir</span></code>
directory.
If <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal notranslate"><span class="pre">project_name</span></code>.</p>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy startproject myproject
</pre></div>
</div>
</section>
<section id="genspider">
<span id="std-command-genspider"></span><h5>genspider<a class="headerlink" href="#genspider" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">[-t</span> <span class="pre">template]</span> <span class="pre">&lt;name&gt;</span> <span class="pre">&lt;domain</span> <span class="pre">or</span> <span class="pre">URL&gt;</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0: </span>The ability to pass a URL instead of a domain.</p>
</div>
<p>Creates a new spider in the current folder or in the current project’s <code class="docutils literal notranslate"><span class="pre">spiders</span></code> folder, if called from inside a project. The <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;</span></code> parameter is set as the spider’s <code class="docutils literal notranslate"><span class="pre">name</span></code>, while <code class="docutils literal notranslate"><span class="pre">&lt;domain</span> <span class="pre">or</span> <span class="pre">URL&gt;</span></code> is used to generate the <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> and <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> spider’s attributes.</p>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider &#39;example&#39; using template &#39;basic&#39;

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider &#39;scrapyorg&#39; using template &#39;crawl&#39;
</pre></div>
</div>
<p>This is just a convenient shortcut command for creating spiders based on
pre-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.</p>
</section>
<section id="crawl">
<span id="std-command-crawl"></span><h5>crawl<a class="headerlink" href="#crawl" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">&lt;spider&gt;</span></code></p></li>
<li><p>Requires project: <em>yes</em></p></li>
</ul>
<p>Start crawling using a spider.</p>
<p>Supported options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-h,</span> <span class="pre">--help</span></code>: show a help message and exit</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">NAME=VALUE</span></code>: set a spider argument (may be repeated)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output</span> <span class="pre">FILE</span></code> or <code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">FILE</span></code>: append scraped items to the end of FILE (use - for stdout). To define the output format, set a colon at the end of the output URI (i.e. <code class="docutils literal notranslate"><span class="pre">-o</span> <span class="pre">FILE:FORMAT</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--overwrite-output</span> <span class="pre">FILE</span></code> or <code class="docutils literal notranslate"><span class="pre">-O</span> <span class="pre">FILE</span></code>: dump scraped items into FILE, overwriting any existing file. To define the output format, set a colon at the end of the output URI (i.e. <code class="docutils literal notranslate"><span class="pre">-O</span> <span class="pre">FILE:FORMAT</span></code>)</p></li>
</ul>
<p>Usage examples:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy crawl myspider
[ ... myspider starts crawling ... ]

$ scrapy crawl -o myfile:csv myspider
[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]

$ scrapy crawl -O myfile:json myspider
[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]
</pre></div>
</div>
</section>
<section id="check">
<span id="std-command-check"></span><h5>check<a class="headerlink" href="#check" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span> <span class="pre">[-l]</span> <span class="pre">&lt;spider&gt;</span></code></p></li>
<li><p>Requires project: <em>yes</em></p></li>
</ul>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; &#39;RetailPricex&#39; field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</pre></div>
</div>
</section>
<section id="list">
<span id="std-command-list"></span><h5>list<a class="headerlink" href="#list" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">list</span></code></p></li>
<li><p>Requires project: <em>yes</em></p></li>
</ul>
<p>List all available spiders in the current project. The output is one spider per
line.</p>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy list
spider1
spider2
</pre></div>
</div>
</section>
<section id="edit">
<span id="std-command-edit"></span><h5>edit<a class="headerlink" href="#edit" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">edit</span> <span class="pre">&lt;spider&gt;</span></code></p></li>
<li><p>Requires project: <em>yes</em></p></li>
</ul>
<p>Edit the given spider using the editor defined in the <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment
variable or (if unset) the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EDITOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EDITOR</span></code></a> setting.</p>
<p>This command is provided only as a convenient shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug spiders.</p>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy edit spider1
</pre></div>
</div>
</section>
<section id="fetch">
<span id="std-command-fetch"></span><h5>fetch<a class="headerlink" href="#fetch" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">&lt;url&gt;</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.</p>
<p>The interesting thing about this command is that it fetches the page the way the
spider would download it. For example, if the spider has a <code class="docutils literal notranslate"><span class="pre">USER_AGENT</span></code>
attribute which overrides the User Agent, it will use that one.</p>
<p>So this command can be used to “see” how your spider would fetch a certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be applied
and it will just use the default Scrapy downloader settings.</p>
<p>Supported options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--headers</span></code>: print the response’s HTTP headers instead of the response’s body</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</p></li>
</ul>
<p>Usage examples:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],
 &#39;Age&#39;: [&#39;1263   &#39;],
 &#39;Connection&#39;: [&#39;close     &#39;],
 &#39;Content-Length&#39;: [&#39;596&#39;],
 &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],
 &#39;Date&#39;: [&#39;Wed, 18 Aug 2010 23:59:46 GMT&#39;],
 &#39;Etag&#39;: [&#39;&quot;573c1-254-48c9c87349680&quot;&#39;],
 &#39;Last-Modified&#39;: [&#39;Fri, 30 Jul 2010 15:30:18 GMT&#39;],
 &#39;Server&#39;: [&#39;Apache/2.2.3 (CentOS)&#39;]}
</pre></div>
</div>
</section>
<section id="view">
<span id="std-command-view"></span><h5>view<a class="headerlink" href="#view" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">view</span> <span class="pre">&lt;url&gt;</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Opens the given URL in a browser, as your Scrapy spider would “see” it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider “sees” and confirm it’s what you expect.</p>
<p>Supported options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</p></li>
</ul>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
</pre></div>
</div>
</section>
<section id="shell">
<span id="std-command-shell"></span><h5>shell<a class="headerlink" href="#shell" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">[url]</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is
given. Also supports UNIX-style local file paths, either relative with
<code class="docutils literal notranslate"><span class="pre">./</span></code> or <code class="docutils literal notranslate"><span class="pre">../</span></code> prefixes or absolute file paths.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> for more info.</p>
<p>Supported options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">code</span></code>: evaluate the code in the shell, print the result and exit</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them);
this only affects the URL you may pass as argument on the command line;
once you are inside the shell, <code class="docutils literal notranslate"><span class="pre">fetch(url)</span></code> will still follow HTTP redirects by default.</p></li>
</ul>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c &#39;(response.status, response.url)&#39;
(200, &#39;http://www.example.com/&#39;)

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &#39;(response.status, response.url)&#39;
(200, &#39;http://example.com/&#39;)

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &#39;(response.status, response.url)&#39;
(302, &#39;http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F&#39;)
</pre></div>
</div>
</section>
<section id="parse">
<span id="std-command-parse"></span><h5>parse<a class="headerlink" href="#parse" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">&lt;url&gt;</span> <span class="pre">[options]</span></code></p></li>
<li><p>Requires project: <em>yes</em></p></li>
</ul>
<p>Fetches the given URL and parses it with the spider that handles it, using the
method passed with the <code class="docutils literal notranslate"><span class="pre">--callback</span></code> option, or <code class="docutils literal notranslate"><span class="pre">parse</span></code> if not given.</p>
<p>Supported options:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--a</span> <span class="pre">NAME=VALUE</span></code>: set spider argument (may be repeated)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--callback</span></code> or <code class="docutils literal notranslate"><span class="pre">-c</span></code>: spider method to use as callback for parsing the
response</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--meta</span></code> or <code class="docutils literal notranslate"><span class="pre">-m</span></code>: additional request meta that will be passed to the callback
request. This must be a valid json string. Example: –meta=’{“foo” : “bar”}’</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--cbkwargs</span></code>: additional keyword arguments that will be passed to the callback.
This must be a valid json string. Example: –cbkwargs=’{“foo” : “bar”}’</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--pipelines</span></code>: process items through pipelines</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rules</span></code> or <code class="docutils literal notranslate"><span class="pre">-r</span></code>: use <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>
rules to discover the callback (i.e. spider method) to use for parsing the
response</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--noitems</span></code>: don’t show scraped items</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nolinks</span></code>: don’t show extracted links</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--nocolour</span></code>: avoid using pygments to colorize the output</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--depth</span></code> or <code class="docutils literal notranslate"><span class="pre">-d</span></code>: depth level for which the requests should be followed
recursively (default: 1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--verbose</span></code> or <code class="docutils literal notranslate"><span class="pre">-v</span></code>: display information for each depth level</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--output</span></code> or <code class="docutils literal notranslate"><span class="pre">-o</span></code>: dump scraped items to a file</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
</li>
</ul>
<p>Usage example:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;name&#39;: &#39;Example item&#39;,
 &#39;category&#39;: &#39;Furniture&#39;,
 &#39;length&#39;: &#39;12 cm&#39;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
</section>
<section id="settings">
<span id="std-command-settings"></span><h5>settings<a class="headerlink" href="#settings" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">settings</span> <span class="pre">[options]</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it’ll show the project setting value, otherwise it’ll
show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
</pre></div>
</div>
</section>
<section id="runspider">
<span id="std-command-runspider"></span><h5>runspider<a class="headerlink" href="#runspider" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">&lt;spider_file.py&gt;</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
</pre></div>
</div>
</section>
<section id="version">
<span id="std-command-version"></span><h5>version<a class="headerlink" href="#version" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">[-v]</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Prints the Scrapy version. If used with <code class="docutils literal notranslate"><span class="pre">-v</span></code> it also prints Python, Twisted
and Platform info, which is useful for bug reports.</p>
</section>
<section id="bench">
<span id="std-command-bench"></span><h5>bench<a class="headerlink" href="#bench" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">bench</span></code></p></li>
<li><p>Requires project: <em>no</em></p></li>
</ul>
<p>Run a quick benchmark test. <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#benchmarking"><span class="std std-ref">Benchmarking</span></a>.</p>
</section>
</section>
<section id="custom-project-commands">
<h4>Custom project commands<a class="headerlink" href="#custom-project-commands" title="Permalink to this heading">¶</a></h4>
<p>You can also add your custom project commands by using the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-COMMANDS_MODULE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COMMANDS_MODULE</span></code></a> setting. See the Scrapy commands in
<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/commands">scrapy/commands</a> for examples on how to implement your commands.</p>
<section id="commands-module">
<span id="std-setting-COMMANDS_MODULE"></span><h5>COMMANDS_MODULE<a class="headerlink" href="#commands-module" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to add custom
commands for your Scrapy project.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">COMMANDS_MODULE</span> <span class="o">=</span> <span class="s2">&quot;mybot.commands&quot;</span>
</pre></div>
</div>
</section>
<section id="register-commands-via-setup-py-entry-points">
<h5>Register commands via setup.py entry points<a class="headerlink" href="#register-commands-via-setup-py-entry-points" title="Permalink to this heading">¶</a></h5>
<p>You can also add Scrapy commands from an external library by adding a
<code class="docutils literal notranslate"><span class="pre">scrapy.commands</span></code> section in the entry points of the library <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>
file.</p>
<p>The following example adds <code class="docutils literal notranslate"><span class="pre">my_command</span></code> command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scrapy-mymodule&quot;</span><span class="p">,</span>
    <span class="n">entry_points</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;scrapy.commands&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;my_command=my_scrapy_module.commands:MyCommand&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-topics/spiders"></span><section id="spiders">
<span id="topics-spiders"></span><h3>Spiders<a class="headerlink" href="#spiders" title="Permalink to this heading">¶</a></h3>
<p>Spiders are classes which define how a certain site (or a group of sites) will be
scraped, including how to perform the crawl (i.e. follow links) and how to
extract structured data from their pages (i.e. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<ol class="arabic">
<li><p>You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.</p>
<p>The first requests to perform are obtained by calling the
<a class="reference internal" href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> method which (by default)
generates <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> for the URLs specified in the
<a class="reference internal" href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> and the
<a class="reference internal" href="#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-attr docutils literal notranslate"><span class="pre">parse</span></code></a> method as callback function for the
Requests.</p>
</li>
<li><p>In the callback function, you parse the response (web page) and return
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects, or an iterable of these objects.
Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.</p></li>
<li><p>In callback functions, you parse the page contents, typically using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a> (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.</p></li>
<li><p>Finally, the items returned from the spider will be typically persisted to a
database (in some <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>) or written to
a file using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p></li>
</ol>
<p>Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.</p>
<section id="scrapy-spider">
<span id="topics-spiders-ref"></span><h4>scrapy.Spider<a class="headerlink" href="#scrapy-spider" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.Spider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">Spider</span></span><a class="headerlink" href="#scrapy.spiders.Spider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.Spider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.</span></span><span class="sig-name descname"><span class="pre">Spider</span></span><a class="headerlink" href="#scrapy.Spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the simplest spider, and the one from which every other spider
must inherit (including spiders that come bundled with Scrapy, as well as spiders
that you write yourself). It doesn’t provide any special functionality. It just
provides a default <a class="reference internal" href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> implementation which sends requests from
the <a class="reference internal" href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> spider attribute and calls the spider’s method <code class="docutils literal notranslate"><span class="pre">parse</span></code>
for each of the resulting responses.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.name">
<span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#scrapy.Spider.name" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it’s required.</p>
<p>If the spider scrapes a single domain, a common practice is to name the
spider after the domain, with or without the <a class="reference external" href="https://en.wikipedia.org/wiki/Top-level_domain">TLD</a>. So, for example, a
spider that crawls <code class="docutils literal notranslate"><span class="pre">mywebsite.com</span></code> would often be called
<code class="docutils literal notranslate"><span class="pre">mywebsite</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.allowed_domains">
<span class="sig-name descname"><span class="pre">allowed_domains</span></span><a class="headerlink" href="#scrapy.Spider.allowed_domains" title="Permalink to this definition">¶</a></dt>
<dd><p>An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list (or their subdomains) won’t be followed if
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a> is
enabled.</p>
<p>Let’s say your target url is <code class="docutils literal notranslate"><span class="pre">https://www.example.com/1.html</span></code>,
then add <code class="docutils literal notranslate"><span class="pre">'example.com'</span></code> to the list.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.start_urls">
<span class="sig-name descname"><span class="pre">start_urls</span></span><a class="headerlink" href="#scrapy.Spider.start_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> will be generated successively from data
contained in the start URLs.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.custom_settings">
<span class="sig-name descname"><span class="pre">custom_settings</span></span><a class="headerlink" href="#scrapy.Spider.custom_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary of settings that will be overridden from the project wide
configuration when running this spider. It must be defined as a class
attribute since the settings are updated before instantiation.</p>
<p>For a list of available built-in settings see:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.crawler">
<span class="sig-name descname"><span class="pre">crawler</span></span><a class="headerlink" href="#scrapy.Spider.crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is set by the <a class="reference internal" href="index.html#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> class method after
initializing the class, and links to the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object to which this spider instance is
bound.</p>
<p>Crawlers encapsulate a lot of components in the project for their single
entry access (such as extensions, middlewares, signals managers, etc).
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a> to know more about them.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.settings">
<span class="sig-name descname"><span class="pre">settings</span></span><a class="headerlink" href="#scrapy.Spider.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Configuration for running this spider. This is a
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> instance, see the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a> topic for a detailed introduction on this subject.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.logger">
<span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#scrapy.Spider.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Python logger created with the Spider’s <a class="reference internal" href="#scrapy.Spider.name" title="scrapy.Spider.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code></a>. You can use it to
send log messages through it as described on
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Spider.state">
<span class="sig-name descname"><span class="pre">state</span></span><a class="headerlink" href="#scrapy.Spider.state" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict you can use to persist some spider state between batches.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-keeping-persistent-state-between-batches"><span class="std std-ref">Keeping persistent state between batches</span></a> to know more about it.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.from_crawler">
<span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the class method used by Scrapy to create your spiders.</p>
<p>You probably won’t need to override this directly because the default
implementation acts as a proxy to the <a class="reference internal" href="index.html#init__" title="__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method, calling
it with the given arguments <code class="docutils literal notranslate"><span class="pre">args</span></code> and named arguments <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<p>Nonetheless, this method sets the <a class="reference internal" href="#scrapy.Spider.crawler" title="scrapy.Spider.crawler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawler</span></code></a> and <a class="reference internal" href="#scrapy.Spider.settings" title="scrapy.Spider.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">settings</span></code></a>
attributes in the new instance so they can be accessed later inside the
spider’s code.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.11: </span>The settings in <code class="docutils literal notranslate"><span class="pre">crawler.settings</span></code> can now be modified in this
method, which is handy if you want to modify them based on
arguments. As a consequence, these settings aren’t the final values
as they can be modified later by e.g. <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-addons"><span class="std std-ref">add-ons</span></a>. For the same reason, most of the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> attributes aren’t initialized at
this point.</p>
<p>The final settings and the initialized
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> attributes are available in the
<a class="reference internal" href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> method, handlers of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> signal and later.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance) – crawler to which the spider will be bound</p></li>
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – arguments passed to the <a class="reference internal" href="index.html#init__" title="__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – keyword arguments passed to the <a class="reference internal" href="index.html#init__" title="__init__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code></a> method</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.update_settings">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">update_settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.update_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="docutils literal notranslate"><span class="pre">update_settings()</span></code> method is used to modify the spider’s settings
and is called during initialization of a spider instance.</p>
<p>It takes a <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object as a parameter and
can add or update the spider’s configuration values. This method is a
class method, meaning that it is called on the <a class="reference internal" href="#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>
class and allows all instances of the spider to share the same
configuration.</p>
<p>While per-spider settings can be set in
<a class="reference internal" href="#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a>, using <code class="docutils literal notranslate"><span class="pre">update_settings()</span></code>
allows you to dynamically add, remove or change settings based on other
settings, spider attributes or other factors and use setting priorities
other than <code class="docutils literal notranslate"><span class="pre">'spider'</span></code>. Also, it’s easy to extend <code class="docutils literal notranslate"><span class="pre">update_settings()</span></code>
in a subclass by overriding it, while doing the same with
<a class="reference internal" href="#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a> can be hard.</p>
<p>For example, suppose a spider needs to modify <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">custom_feed</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;/home/user/documents/items.json&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span>
            <span class="s2">&quot;indent&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">update_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
        <span class="n">settings</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;FEEDS&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">custom_feed</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.start_requests">
<span class="sig-name descname"><span class="pre">start_requests</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.start_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>This method must return an iterable with the first Requests to crawl and/or with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a> for
this spider. It is called by Scrapy when the spider is opened for
scraping. Scrapy calls it only once, so it is safe to implement
<a class="reference internal" href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> as a generator.</p>
<p>The default implementation generates <code class="docutils literal notranslate"><span class="pre">Request(url,</span> <span class="pre">dont_filter=True)</span></code>
for each url in <a class="reference internal" href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a>.</p>
<p>If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span>
                <span class="s2">&quot;http://www.example.com/login&quot;</span><span class="p">,</span>
                <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;user&quot;</span><span class="p">:</span> <span class="s2">&quot;john&quot;</span><span class="p">,</span> <span class="s2">&quot;pass&quot;</span><span class="p">:</span> <span class="s2">&quot;secret&quot;</span><span class="p">},</span>
                <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># here you would extract links to follow and return Requests for</span>
        <span class="c1"># each of them, with another callback</span>
        <span class="k">pass</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.parse">
<span class="sig-name descname"><span class="pre">parse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.parse" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the default callback used by Scrapy to process downloaded
responses, when their requests don’t specify a callback.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">parse</span></code> method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the <code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code> class.</p>
<p>This method, as well as any other Request callback, must return a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a>, an
iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects and/or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a>, or <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>) – the response to parse</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">message</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">level</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">component</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper that sends a log message through the Spider’s <a class="reference internal" href="#scrapy.Spider.logger" title="scrapy.Spider.logger"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logger</span></code></a>,
kept for backward compatibility. For more information see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Spider.closed">
<span class="sig-name descname"><span class="pre">closed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reason</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Spider.closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider closes. This method provides a shortcut to
signals.connect() for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> signal.</p>
</dd></dl>

</dd></dl>

<p>Let’s see an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.example.com/1.html&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.example.com/2.html&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.example.com/3.html&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;A response from </span><span class="si">%s</span><span class="s2"> just arrived!&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>Return multiple Requests and items from a single callback:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.example.com/1.html&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.example.com/2.html&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.example.com/3.html&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">h3</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Instead of <a class="reference internal" href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> you can use <a class="reference internal" href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> directly;
to give data more structure you can use <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/1.html&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/2.html&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/3.html&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h3&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="spider-arguments">
<span id="spiderargs"></span><h4>Spider arguments<a class="headerlink" href="#spider-arguments" title="Permalink to this heading">¶</a></h4>
<p>Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.</p>
<p>Spider arguments are passed through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> command using the
<code class="docutils literal notranslate"><span class="pre">-a</span></code> option. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">a</span> <span class="n">category</span><span class="o">=</span><span class="n">electronics</span>
</pre></div>
</div>
<p>Spiders can access arguments in their <cite>__init__</cite> methods:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;http://www.example.com/categories/</span><span class="si">{</span><span class="n">category</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span>
        <span class="c1"># ...</span>
</pre></div>
</div>
<p>The default <cite>__init__</cite> method will take any spider arguments
and copy them to the spider as attributes.
The above example can also be written as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://www.example.com/categories/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">category</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If you are <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">running Scrapy from a script</span></a>, you can
specify spider arguments when calling
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess.crawl</span></code></a> or
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">()</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="s2">&quot;electronics&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that spider arguments are only strings.
The spider will not do any parsing on its own.
If you were to set the <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> attribute from the command line,
you would have to parse it on your own into a list
using something like <a class="reference external" href="https://docs.python.org/3/library/ast.html#ast.literal_eval" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">ast.literal_eval()</span></code></a> or <a class="reference external" href="https://docs.python.org/3/library/json.html#json.loads" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">json.loads()</span></code></a>
and then set it as an attribute.
Otherwise, you would cause iteration over a <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> string
(a very common python pitfall)
resulting in each character being seen as a separate url.</p>
<p>A valid use case is to set the http auth credentials
used by <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>
or the user agent
used by <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UserAgentMiddleware</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">a</span> <span class="n">http_user</span><span class="o">=</span><span class="n">myuser</span> <span class="o">-</span><span class="n">a</span> <span class="n">http_pass</span><span class="o">=</span><span class="n">mypassword</span> <span class="o">-</span><span class="n">a</span> <span class="n">user_agent</span><span class="o">=</span><span class="n">mybot</span>
</pre></div>
</div>
<p>Spider arguments can also be passed through the Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> API.
See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/">Scrapyd documentation</a>.</p>
</section>
<section id="generic-spiders">
<span id="builtin-spiders"></span><h4>Generic Spiders<a class="headerlink" href="#generic-spiders" title="Permalink to this heading">¶</a></h4>
<p>Scrapy comes with some useful generic spiders that you can use to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from <a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a>, or parsing an XML/CSV feed.</p>
<p>For the examples used in the following spiders, we’ll assume you have a project
with a <code class="docutils literal notranslate"><span class="pre">TestItem</span></code> declared in a <code class="docutils literal notranslate"><span class="pre">myproject.items</span></code> module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">TestItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<section id="crawlspider">
<h5>CrawlSpider<a class="headerlink" href="#crawlspider" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.CrawlSpider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">CrawlSpider</span></span><a class="headerlink" href="#scrapy.spiders.CrawlSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it’s generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.</p>
<p>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.CrawlSpider.rules">
<span class="sig-name descname"><span class="pre">rules</span></span><a class="headerlink" href="#scrapy.spiders.CrawlSpider.rules" title="Permalink to this definition">¶</a></dt>
<dd><p>Which is a list of one (or more) <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> objects.  Each <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a>
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they’re defined in this attribute.</p>
</dd></dl>

<p>This spider also exposes an overridable method:</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.CrawlSpider.parse_start_url">
<span class="sig-name descname"><span class="pre">parse_start_url</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CrawlSpider.parse_start_url" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each response produced for the URLs in
the spider’s <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> attribute. It allows to parse
the initial responses and must return either an
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a>, a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>
object, or an iterable containing any of them.</p>
</dd></dl>

</dd></dl>

<section id="crawling-rules">
<h6>Crawling rules<a class="headerlink" href="#crawling-rules" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.Rule">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">Rule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">link_extractor</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">LinkExtractor</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CallbackT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">follow</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_links</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">ProcessLinksT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">ProcessRequestT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Failure</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Rule" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">link_extractor</span></code> is a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractor</span></a> object which
defines how links will be extracted from each crawled page. Each produced link will
be used to generate a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, which will contain the
link’s text in its <code class="docutils literal notranslate"><span class="pre">meta</span></code> dictionary (under the <code class="docutils literal notranslate"><span class="pre">link_text</span></code> key).
If omitted, a default link extractor created with no arguments will be used,
resulting in all links being extracted.</p>
<p><code class="docutils literal notranslate"><span class="pre">callback</span></code> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link extractor. This callback receives a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
as its first argument and must return either a single instance or an iterable of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a> and/or <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects
(or any subclass of them). As mentioned above, the received <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object will contain the text of the link that produced the <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>
in its <code class="docutils literal notranslate"><span class="pre">meta</span></code> dictionary (under the <code class="docutils literal notranslate"><span class="pre">link_text</span></code> key)</p>
<p><code class="docutils literal notranslate"><span class="pre">cb_kwargs</span></code> is a dict containing the keyword arguments to be passed to the
callback function.</p>
<p><code class="docutils literal notranslate"><span class="pre">follow</span></code> is a boolean which specifies if links should be followed from each
response extracted with this rule. If <code class="docutils literal notranslate"><span class="pre">callback</span></code> is None <code class="docutils literal notranslate"><span class="pre">follow</span></code> defaults
to <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise it defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">process_links</span></code> is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified <code class="docutils literal notranslate"><span class="pre">link_extractor</span></code>.
This is mainly used for filtering purposes.</p>
<p><code class="docutils literal notranslate"><span class="pre">process_request</span></code> is a callable (or a string, in which case a method from
the spider object with that name will be used) which will be called for every
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> extracted by this rule. This callable should
take said request as first argument and the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
from which the request originated as second argument. It must return a
<code class="docutils literal notranslate"><span class="pre">Request</span></code> object or <code class="docutils literal notranslate"><span class="pre">None</span></code> (to filter out the request).</p>
<p><code class="docutils literal notranslate"><span class="pre">errback</span></code> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called if any exception is
raised while processing a request generated by the rule.
It receives a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Twisted</span> <span class="pre">Failure</span></code></a>
instance as first parameter.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Because of its internal implementation, you must explicitly set
callbacks for new requests when writing <a class="reference internal" href="#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>-based spiders;
unexpected behaviour can occur otherwise.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0: </span>The <em>errback</em> parameter.</p>
</div>
</dd></dl>

</section>
<section id="crawlspider-example">
<h6>CrawlSpider example<a class="headerlink" href="#crawlspider-example" title="Permalink to this heading">¶</a></h6>
<p>Let’s now take a look at an example CrawlSpider with rules:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># Extract links matching &#39;category.php&#39; (but not matching &#39;subsection.php&#39;)</span>
        <span class="c1"># and follow links from them (since no callback means follow=True by default).</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;category\.php&quot;</span><span class="p">,),</span> <span class="n">deny</span><span class="o">=</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;subsection\.php&quot;</span><span class="p">,))),</span>
        <span class="c1"># Extract links matching &#39;item.php&#39; and parse them with the spider&#39;s method parse_item</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;item\.php&quot;</span><span class="p">,)),</span> <span class="n">callback</span><span class="o">=</span><span class="s2">&quot;parse_item&quot;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Hi, this is an item page! </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;ID: (\d+)&quot;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;description&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
            <span class="s1">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;link_text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;link_text&quot;</span><span class="p">]</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;additional_data&quot;]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span>
            <span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_additional_page</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_additional_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;additional_data&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
            <span class="s1">&#39;//p[@id=&quot;additional_data&quot;]/text()&#39;</span>
        <span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>This spider would start crawling example.com’s home page, collecting category
links, and item links, parsing the latter with the <code class="docutils literal notranslate"><span class="pre">parse_item</span></code> method. For
each item response, some data will be extracted from the HTML using XPath, and
an <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> will be filled with it.</p>
</section>
</section>
<section id="xmlfeedspider">
<h5>XMLFeedSpider<a class="headerlink" href="#xmlfeedspider" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">XMLFeedSpider</span></span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: <code class="docutils literal notranslate"><span class="pre">iternodes</span></code>, <code class="docutils literal notranslate"><span class="pre">xml</span></code>,
and <code class="docutils literal notranslate"><span class="pre">html</span></code>.  It’s recommended to use the <code class="docutils literal notranslate"><span class="pre">iternodes</span></code> iterator for
performance reasons, since the <code class="docutils literal notranslate"><span class="pre">xml</span></code> and <code class="docutils literal notranslate"><span class="pre">html</span></code> iterators generate the
whole DOM at once in order to parse it.  However, using <code class="docutils literal notranslate"><span class="pre">html</span></code> as the
iterator may be useful when parsing XML with bad markup.</p>
<p>To set the iterator and the tag name, you must define the following class
attributes:</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.iterator">
<span class="sig-name descname"><span class="pre">iterator</span></span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the iterator to use. It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'iternodes'</span></code> - a fast iterator based on regular expressions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'html'</span></code> - an iterator which uses <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'xml'</span></code> - an iterator which uses <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</p></li>
</ul>
</div></blockquote>
<p>It defaults to: <code class="docutils literal notranslate"><span class="pre">'iternodes'</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.itertag">
<span class="sig-name descname"><span class="pre">itertag</span></span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.itertag" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the name of the node (or element) to iterate in. Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;product&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.namespaces">
<span class="sig-name descname"><span class="pre">namespaces</span></span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of <code class="docutils literal notranslate"><span class="pre">(prefix,</span> <span class="pre">uri)</span></code> tuples which define the namespaces
available in that document that will be processed with this spider. The
<code class="docutils literal notranslate"><span class="pre">prefix</span></code> and <code class="docutils literal notranslate"><span class="pre">uri</span></code> will be used to automatically register
namespaces using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">register_namespace()</span></code> method.</p>
<p>You can then specify nodes with namespaces in the <a class="reference internal" href="#scrapy.spiders.XMLFeedSpider.itertag" title="scrapy.spiders.XMLFeedSpider.itertag"><code class="xref py py-attr docutils literal notranslate"><span class="pre">itertag</span></code></a>
attribute.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">YourSpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>

    <span class="n">namespaces</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;http://www.sitemaps.org/schemas/sitemap/0.9&#39;</span><span class="p">)]</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;n:url&#39;</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
</dd></dl>

<p>Apart from these new attributes, this spider has the following overridable
methods too:</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.adapt_response">
<span class="sig-name descname"><span class="pre">adapt_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.adapt_response" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.parse_node">
<span class="sig-name descname"><span class="pre">parse_node</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selector</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.parse_node" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the nodes matching the provided tag name
(<code class="docutils literal notranslate"><span class="pre">itertag</span></code>).  Receives the response and an
<code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> for each node.  Overriding this
method is mandatory. Otherwise, you spider won’t work.  This method
must return an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a>, a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, or an iterable containing any of
them.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.XMLFeedSpider.process_results">
<span class="sig-name descname"><span class="pre">process_results</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.process_results" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each result (item or request) returned by the
spider, and it’s intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (items or requests).</p>
</dd></dl>

<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Because of its internal implementation, you must explicitly set
callbacks for new requests when writing <a class="reference internal" href="#scrapy.spiders.XMLFeedSpider" title="scrapy.spiders.XMLFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code></a>-based spiders;
unexpected behaviour can occur otherwise.</p>
</div>
</dd></dl>

<section id="xmlfeedspider-example">
<h6>XMLFeedSpider example<a class="headerlink" href="#xmlfeedspider-example" title="Permalink to this heading">¶</a></h6>
<p>These spiders are pretty easy to use, let’s have a look at one example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">XMLFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/feed.xml&quot;</span><span class="p">]</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="s2">&quot;iternodes&quot;</span>  <span class="c1"># This is actually unnecessary, since it&#39;s the default value</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s2">&quot;item&quot;</span>

    <span class="k">def</span> <span class="nf">parse_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Hi, this is a &lt;</span><span class="si">%s</span><span class="s2">&gt; node!: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">itertag</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">getall</span><span class="p">())</span>
        <span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;@id&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;description&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;description&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically what we did up there was to create a spider that downloads a feed from
the given <code class="docutils literal notranslate"><span class="pre">start_urls</span></code>, and then iterates through each of its <code class="docutils literal notranslate"><span class="pre">item</span></code> tags,
prints them out, and stores some random data in an <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>.</p>
</section>
</section>
<section id="csvfeedspider">
<h5>CSVFeedSpider<a class="headerlink" href="#csvfeedspider" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.CSVFeedSpider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">CSVFeedSpider</span></span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is <a class="reference internal" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="scrapy.spiders.CSVFeedSpider.parse_row"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse_row()</span></code></a>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.CSVFeedSpider.delimiter">
<span class="sig-name descname"><span class="pre">delimiter</span></span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.delimiter" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the separator character for each field in the CSV file
Defaults to <code class="docutils literal notranslate"><span class="pre">','</span></code> (comma).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.CSVFeedSpider.quotechar">
<span class="sig-name descname"><span class="pre">quotechar</span></span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.quotechar" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the enclosure character for each field in the CSV file
Defaults to <code class="docutils literal notranslate"><span class="pre">'&quot;'</span></code> (quotation mark).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.CSVFeedSpider.headers">
<span class="sig-name descname"><span class="pre">headers</span></span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of the column names in the CSV file.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.CSVFeedSpider.parse_row">
<span class="sig-name descname"><span class="pre">parse_row</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">row</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override <code class="docutils literal notranslate"><span class="pre">adapt_response</span></code> and <code class="docutils literal notranslate"><span class="pre">process_results</span></code> methods
for pre- and post-processing purposes.</p>
</dd></dl>

</dd></dl>

<section id="csvfeedspider-example">
<h6>CSVFeedSpider example<a class="headerlink" href="#csvfeedspider-example" title="Permalink to this heading">¶</a></h6>
<p>Let’s see an example similar to the previous one, but using a
<a class="reference internal" href="#scrapy.spiders.CSVFeedSpider" title="scrapy.spiders.CSVFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVFeedSpider</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CSVFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">TestItem</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CSVFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/feed.csv&quot;</span><span class="p">]</span>
    <span class="n">delimiter</span> <span class="o">=</span> <span class="s2">&quot;;&quot;</span>
    <span class="n">quotechar</span> <span class="o">=</span> <span class="s2">&quot;&#39;&quot;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Hi, this is a row!: </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;description&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;description&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</section>
</section>
<section id="sitemapspider">
<h5>SitemapSpider<a class="headerlink" href="#sitemapspider" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiders.</span></span><span class="sig-name descname"><span class="pre">SitemapSpider</span></span><a class="headerlink" href="#scrapy.spiders.SitemapSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>SitemapSpider allows you to crawl a site by discovering the URLs using
<a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a>.</p>
<p>It supports nested sitemaps and discovering sitemap urls from
<a class="reference external" href="https://www.robotstxt.org/">robots.txt</a>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider.sitemap_urls">
<span class="sig-name descname"><span class="pre">sitemap_urls</span></span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of urls pointing to the sitemaps whose urls you want to crawl.</p>
<p>You can also point to a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> and it will be parsed to extract
sitemap urls from it.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider.sitemap_rules">
<span class="sig-name descname"><span class="pre">sitemap_rules</span></span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_rules" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of tuples <code class="docutils literal notranslate"><span class="pre">(regex,</span> <span class="pre">callback)</span></code> where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">regex</span></code> is a regular expression to match urls extracted from sitemaps.
<code class="docutils literal notranslate"><span class="pre">regex</span></code> can be either a str or a compiled regex object.</p></li>
<li><p>callback is the callback to use for processing the urls that match
the regular expression. <code class="docutils literal notranslate"><span class="pre">callback</span></code> can be a string (indicating the
name of a spider method) or a callable.</p></li>
</ul>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;/product/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_product&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Rules are applied in order, and only the first one that matches will be
used.</p>
<p>If you omit this attribute, all urls found in sitemaps will be
processed with the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider.sitemap_follow">
<span class="sig-name descname"><span class="pre">sitemap_follow</span></span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_follow" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of regexes of sitemap that should be followed. This is only
for sites that use <a class="reference external" href="https://www.sitemaps.org/protocol.html#index">Sitemap index files</a> that point to other sitemap
files.</p>
<p>By default, all sitemaps are followed.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider.sitemap_alternate_links">
<span class="sig-name descname"><span class="pre">sitemap_alternate_links</span></span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_alternate_links" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies if alternate links for one <code class="docutils literal notranslate"><span class="pre">url</span></code> should be followed. These
are links for the same website in another language passed within
the same <code class="docutils literal notranslate"><span class="pre">url</span></code> block.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">loc</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/&lt;/</span><span class="n">loc</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">xhtml</span><span class="p">:</span><span class="n">link</span> <span class="n">rel</span><span class="o">=</span><span class="s2">&quot;alternate&quot;</span> <span class="n">hreflang</span><span class="o">=</span><span class="s2">&quot;de&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;http://example.com/de&quot;</span><span class="o">/&gt;</span>
<span class="o">&lt;/</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> set, this would retrieve both URLs. With
<code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> disabled, only <code class="docutils literal notranslate"><span class="pre">http://example.com/</span></code> would be
retrieved.</p>
<p>Default is <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> disabled.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiders.SitemapSpider.sitemap_filter">
<span class="sig-name descname"><span class="pre">sitemap_filter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">entries</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a filter function that could be overridden to select sitemap entries
based on their attributes.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">loc</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/&lt;/</span><span class="n">loc</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">lastmod</span><span class="o">&gt;</span><span class="mi">2005</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span><span class="o">&lt;/</span><span class="n">lastmod</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>We can define a <code class="docutils literal notranslate"><span class="pre">sitemap_filter</span></code> function to filter <code class="docutils literal notranslate"><span class="pre">entries</span></code> by date:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>


<span class="k">class</span> <span class="nc">FilteredSitemapSpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;filtered_sitemap_spider&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://example.com/sitemap.xml&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">sitemap_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">entries</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">entries</span><span class="p">:</span>
            <span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="s2">&quot;lastmod&quot;</span><span class="p">],</span> <span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">date_time</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;=</span> <span class="mi">2005</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">entry</span>
</pre></div>
</div>
<p>This would retrieve only <code class="docutils literal notranslate"><span class="pre">entries</span></code> modified on 2005 and the following
years.</p>
<p>Entries are dict objects extracted from the sitemap document.
Usually, the key is the tag name and the value is the text inside it.</p>
<p>It’s important to notice that:</p>
<ul class="simple">
<li><p>as the loc attribute is required, entries without this tag are discarded</p></li>
<li><p>alternate links are stored in a list with the key <code class="docutils literal notranslate"><span class="pre">alternate</span></code>
(see <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code>)</p></li>
<li><p>namespaces are removed, so lxml tags named as <code class="docutils literal notranslate"><span class="pre">{namespace}tagname</span></code> become only <code class="docutils literal notranslate"><span class="pre">tagname</span></code></p></li>
</ul>
<p>If you omit this method, all entries found in sitemaps will be
processed, observing other attributes and their settings.</p>
</dd></dl>

</dd></dl>

<section id="sitemapspider-examples">
<h6>SitemapSpider examples<a class="headerlink" href="#sitemapspider-examples" title="Permalink to this heading">¶</a></h6>
<p>Simplest example: process all urls discovered through sitemaps using the
<code class="docutils literal notranslate"><span class="pre">parse</span></code> callback:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/sitemap.xml&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape item here ...</span>
</pre></div>
</div>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/sitemap.xml&quot;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;/product/&quot;</span><span class="p">,</span> <span class="s2">&quot;parse_product&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;/category/&quot;</span><span class="p">,</span> <span class="s2">&quot;parse_category&quot;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape product ...</span>

    <span class="k">def</span> <span class="nf">parse_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape category ...</span>
</pre></div>
</div>
<p>Follow sitemaps defined in the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file and only follow sitemaps
whose url contains <code class="docutils literal notranslate"><span class="pre">/sitemap_shop</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/robots.txt&quot;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;/shop/&quot;</span><span class="p">,</span> <span class="s2">&quot;parse_shop&quot;</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">sitemap_follow</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;/sitemap_shops&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape shop here ...</span>
</pre></div>
</div>
<p>Combine SitemapSpider with other sources of urls:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">SitemapSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/robots.txt&quot;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;/shop/&quot;</span><span class="p">,</span> <span class="s2">&quot;parse_shop&quot;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">other_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/about&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">requests</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">start_requests</span><span class="p">())</span>
        <span class="n">requests</span> <span class="o">+=</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_other</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_urls</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">requests</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape shop here ...</span>

    <span class="k">def</span> <span class="nf">parse_other</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>  <span class="c1"># ... scrape other here ...</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>
<span id="document-topics/selectors"></span><section id="selectors">
<span id="topics-selectors"></span><h3>Selectors<a class="headerlink" href="#selectors" title="Permalink to this heading">¶</a></h3>
<p>When you’re scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this, such as:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> is a very popular web scraping library among Python
programmers which constructs a Python object based on the structure of the
HTML code and also deals with bad markup reasonably well, but it has one
drawback: it’s slow.</p></li>
<li><p><a class="reference external" href="https://lxml.de/">lxml</a> is an XML parsing library (which also parses HTML) with a pythonic
API based on <a class="reference external" href="https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">ElementTree</span></code></a>. (lxml is not part of the Python
standard library.)</p></li>
</ul>
<p>Scrapy comes with its own mechanism for extracting data. They’re called
selectors because they “select” certain parts of the HTML document specified
either by <a class="reference external" href="https://www.w3.org/TR/xpath/all/">XPath</a> or <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> expressions.</p>
<p><a class="reference external" href="https://www.w3.org/TR/xpath/all/">XPath</a> is a language for selecting nodes in XML documents, which can also be
used with HTML. <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scrapy Selectors is a thin wrapper around <a class="reference external" href="https://parsel.readthedocs.io/en/latest/">parsel</a> library; the purpose of
this wrapper is to provide better integration with Scrapy Response objects.</p>
<p><a class="reference external" href="https://parsel.readthedocs.io/en/latest/">parsel</a> is a stand-alone web scraping library which can be used without
Scrapy. It uses <a class="reference external" href="https://lxml.de/">lxml</a> library under the hood, and implements an
easy API on top of lxml API. It means Scrapy selectors are very similar
in speed and parsing accuracy to lxml.</p>
</div>
<section id="using-selectors">
<h4>Using selectors<a class="headerlink" href="#using-selectors" title="Permalink to this heading">¶</a></h4>
<section id="constructing-selectors">
<h5>Constructing selectors<a class="headerlink" href="#constructing-selectors" title="Permalink to this heading">¶</a></h5>
<p>Response objects expose a <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> instance
on <code class="docutils literal notranslate"><span class="pre">.selector</span></code> attribute:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//span/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Querying responses using XPath and CSS is so common that responses include two
more shortcuts: <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code> and <code class="docutils literal notranslate"><span class="pre">response.css()</span></code>:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//span/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Scrapy selectors are instances of <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> class
constructed by passing either <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> object or
markup as a string (in <code class="docutils literal notranslate"><span class="pre">text</span></code> argument).</p>
<p>Usually there is no need to construct Scrapy selectors manually:
<code class="docutils literal notranslate"><span class="pre">response</span></code> object is available in Spider callbacks, so in most cases
it is more convenient to use <code class="docutils literal notranslate"><span class="pre">response.css()</span></code> and <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code>
shortcuts. By using <code class="docutils literal notranslate"><span class="pre">response.selector</span></code> or one of these shortcuts
you can also ensure the response body is parsed only once.</p>
<p>But if required, it is possible to use <code class="docutils literal notranslate"><span class="pre">Selector</span></code> directly.
Constructing from text:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">body</span> <span class="o">=</span> <span class="s2">&quot;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//span/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Constructing from response - <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> is one of
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> subclasses:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://example.com&quot;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//span/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Selector</span></code> automatically chooses the best parsing rules
(XML vs HTML) based on input type.</p>
</section>
<section id="id1">
<h5>Using selectors<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<p>To explain how to use the selectors we’ll use the <code class="docutils literal notranslate"><span class="pre">Scrapy</span> <span class="pre">shell</span></code> (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<blockquote>
<div><p><a class="reference external" href="https://docs.scrapy.org/en/latest/_static/selectors-sample1.html">https://docs.scrapy.org/en/latest/_static/selectors-sample1.html</a></p>
</div></blockquote>
<p id="topics-selectors-htmlcode">For the sake of completeness, here’s its full HTML code:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="cp">&lt;!DOCTYPE html&gt;</span>

<span class="p">&lt;</span><span class="nt">html</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">head</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">base</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;http://example.com/&#39;</span> <span class="p">/&gt;</span>
    <span class="p">&lt;</span><span class="nt">title</span><span class="p">&gt;</span>Example website<span class="p">&lt;/</span><span class="nt">title</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">head</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">body</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">&#39;images&#39;</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image1.html&#39;</span><span class="p">&gt;</span>Name: My image 1 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image1_thumb.jpg&#39;</span> <span class="na">alt</span><span class="o">=</span><span class="s">&#39;image1&#39;</span><span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image2.html&#39;</span><span class="p">&gt;</span>Name: My image 2 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image2_thumb.jpg&#39;</span> <span class="na">alt</span><span class="o">=</span><span class="s">&#39;image2&#39;</span><span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image3.html&#39;</span><span class="p">&gt;</span>Name: My image 3 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image3_thumb.jpg&#39;</span> <span class="na">alt</span><span class="o">=</span><span class="s">&#39;image3&#39;</span><span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image4.html&#39;</span><span class="p">&gt;</span>Name: My image 4 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image4_thumb.jpg&#39;</span> <span class="na">alt</span><span class="o">=</span><span class="s">&#39;image4&#39;</span><span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image5.html&#39;</span><span class="p">&gt;</span>Name: My image 5 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image5_thumb.jpg&#39;</span> <span class="na">alt</span><span class="o">=</span><span class="s">&#39;image5&#39;</span><span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">body</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">html</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>First, let’s open the shell:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>scrapy<span class="w"> </span>shell<span class="w"> </span>https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
</pre></div>
</div>
<p>Then, after the shell loads, you’ll have the response available as <code class="docutils literal notranslate"><span class="pre">response</span></code>
shell variable, and its attached selector in <code class="docutils literal notranslate"><span class="pre">response.selector</span></code> attribute.</p>
<p>Since we’re dealing with HTML, the selector will automatically use an HTML parser.</p>
<p>So, by looking at the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> of that
page, let’s construct an XPath for selecting the text inside the title tag:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;//title/text()&#39; data=&#39;Example website&#39;&gt;]</span>
</pre></div>
</div>
<p>To actually extract the textual data, you must call the selector <code class="docutils literal notranslate"><span class="pre">.get()</span></code>
or <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods, as follows:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;Example website&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.get()</span></code> always returns a single result; if there are several matches,
content of a first match is returned; if there are no matches, None
is returned. <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> returns a list with all results.</p>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
<p>As you can see, <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> and <code class="docutils literal notranslate"><span class="pre">.css()</span></code> methods return a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance, which is a list of new
selectors. This API can be used for quickly selecting nested data:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;@src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go">&#39;image2_thumb.jpg&#39;,</span>
<span class="go">&#39;image3_thumb.jpg&#39;,</span>
<span class="go">&#39;image4_thumb.jpg&#39;,</span>
<span class="go">&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>If you want to extract only the first matched element, you can call the
selector <code class="docutils literal notranslate"><span class="pre">.get()</span></code> (or its alias <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> commonly used in
previous Scrapy versions):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;images&quot;]/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Name: My image 1 &#39;</span>
</pre></div>
</div>
<p>It returns <code class="docutils literal notranslate"><span class="pre">None</span></code> if no element was found:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span>
<span class="go">True</span>
</pre></div>
</div>
<p>A default return value can be provided as an argument, to be used instead
of <code class="docutils literal notranslate"><span class="pre">None</span></code>:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;not-found&quot;</span><span class="p">)</span>
<span class="go">&#39;not-found&#39;</span>
</pre></div>
</div>
<p>Instead of using e.g. <code class="docutils literal notranslate"><span class="pre">'&#64;src'</span></code> XPath it is possible to query for attributes
using <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of a <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code>:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">img</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;src&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img&quot;</span><span class="p">)]</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go">&#39;image2_thumb.jpg&#39;,</span>
<span class="go">&#39;image3_thumb.jpg&#39;,</span>
<span class="go">&#39;image4_thumb.jpg&#39;,</span>
<span class="go">&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>As a shortcut, <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> is also available on SelectorList directly;
it returns attributes for the first matching element:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;src&quot;</span><span class="p">]</span>
<span class="go">&#39;image1_thumb.jpg&#39;</span>
</pre></div>
</div>
<p>This is most useful when only a single result is expected, e.g. when selecting
by id, or selecting unique elements on a web page:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;href&quot;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>
</pre></div>
</div>
<p>Now we’re going to get the base URL and some image links:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//base/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;base::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;href&quot;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go">&#39;image2.html&#39;,</span>
<span class="go">&#39;image3.html&#39;,</span>
<span class="go">&#39;image4.html&#39;,</span>
<span class="go">&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a[href*=image]::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go">&#39;image2.html&#39;,</span>
<span class="go">&#39;image3.html&#39;,</span>
<span class="go">&#39;image4.html&#39;,</span>
<span class="go">&#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go">&#39;image2_thumb.jpg&#39;,</span>
<span class="go">&#39;image3_thumb.jpg&#39;,</span>
<span class="go">&#39;image4_thumb.jpg&#39;,</span>
<span class="go">&#39;image5_thumb.jpg&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a[href*=image] img::attr(src)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go">&#39;image2_thumb.jpg&#39;,</span>
<span class="go">&#39;image3_thumb.jpg&#39;,</span>
<span class="go">&#39;image4_thumb.jpg&#39;,</span>
<span class="go">&#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</section>
<section id="extensions-to-css-selectors">
<span id="topics-selectors-css-extensions"></span><h5>Extensions to CSS Selectors<a class="headerlink" href="#extensions-to-css-selectors" title="Permalink to this heading">¶</a></h5>
<p>Per W3C standards, <a class="reference external" href="https://www.w3.org/TR/selectors-3/#selectors">CSS selectors</a> do not support selecting text nodes
or attribute values.
But selecting these is so essential in a web scraping context
that Scrapy (parsel) implements a couple of <strong>non-standard pseudo-elements</strong>:</p>
<ul class="simple">
<li><p>to select text nodes, use <code class="docutils literal notranslate"><span class="pre">::text</span></code></p></li>
<li><p>to select attribute values, use <code class="docutils literal notranslate"><span class="pre">::attr(name)</span></code> where <em>name</em> is the
name of the attribute that you want the value of</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>These pseudo-elements are Scrapy-/Parsel-specific.
They will most probably not work with other libraries like
<a class="reference external" href="https://lxml.de/">lxml</a> or <a class="reference external" href="https://pypi.org/project/pyquery/">PyQuery</a>.</p>
</div>
<p>Examples:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">title::text</span></code> selects children text nodes of a descendant <code class="docutils literal notranslate"><span class="pre">&lt;title&gt;</span></code> element:</p></li>
</ul>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;title::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">*::text</span></code> selects all descendant text nodes of the current selector context:</p></li>
</ul>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;#images *::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;\n   &#39;,</span>
<span class="go">&#39;Name: My image 1 &#39;,</span>
<span class="go">&#39;\n   &#39;,</span>
<span class="go">&#39;Name: My image 2 &#39;,</span>
<span class="go">&#39;\n   &#39;,</span>
<span class="go">&#39;Name: My image 3 &#39;,</span>
<span class="go">&#39;\n   &#39;,</span>
<span class="go">&#39;Name: My image 4 &#39;,</span>
<span class="go">&#39;\n   &#39;,</span>
<span class="go">&#39;Name: My image 5 &#39;,</span>
<span class="go">&#39;\n  &#39;]</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">foo::text</span></code> returns no results if <code class="docutils literal notranslate"><span class="pre">foo</span></code> element exists, but contains
no text (i.e. text is empty):</p></li>
</ul>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[]</span>

<span class="go">This means ``.css(&#39;foo::text&#39;).get()`` could return None even if an element</span>
<span class="go">exists. Use ``default=&#39;&#39;`` if you always want a string:</span>
</pre></div>
</div>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;img::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="go">&#39;&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a::attr(href)</span></code> selects the <em>href</em> attribute value of descendant links:</p></li>
</ul>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go">&#39;image2.html&#39;,</span>
<span class="go">&#39;image3.html&#39;,</span>
<span class="go">&#39;image4.html&#39;,</span>
<span class="go">&#39;image5.html&#39;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You cannot chain these pseudo-elements. But in practice it would not
make much sense: text nodes do not have attributes, and attribute values
are string values already and do not have children nodes.</p>
</div>
</section>
<section id="nesting-selectors">
<span id="topics-selectors-nesting-selectors"></span><h5>Nesting selectors<a class="headerlink" href="#nesting-selectors" title="Permalink to this heading">¶</a></h5>
<p>The selection methods (<code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> or <code class="docutils literal notranslate"><span class="pre">.css()</span></code>) return a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here’s an example:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">links</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot; alt=&quot;image1&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go">&#39;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot; alt=&quot;image2&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go">&#39;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot; alt=&quot;image3&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go">&#39;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot; alt=&quot;image4&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go">&#39;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot; alt=&quot;image5&quot;&gt;&lt;/a&gt;&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">link</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">links</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">href_xpath</span> <span class="o">=</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">img_xpath</span> <span class="o">=</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;img/@src&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Link number </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> points to url </span><span class="si">{</span><span class="n">href_xpath</span><span class="si">!r}</span><span class="s2"> and image </span><span class="si">{</span><span class="n">img_xpath</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">Link number 0 points to url &#39;image1.html&#39; and image &#39;image1_thumb.jpg&#39;</span>
<span class="go">Link number 1 points to url &#39;image2.html&#39; and image &#39;image2_thumb.jpg&#39;</span>
<span class="go">Link number 2 points to url &#39;image3.html&#39; and image &#39;image3_thumb.jpg&#39;</span>
<span class="go">Link number 3 points to url &#39;image4.html&#39; and image &#39;image4_thumb.jpg&#39;</span>
<span class="go">Link number 4 points to url &#39;image5.html&#39; and image &#39;image5_thumb.jpg&#39;</span>
</pre></div>
</div>
</section>
<section id="selecting-element-attributes">
<span id="selecting-attributes"></span><h5>Selecting element attributes<a class="headerlink" href="#selecting-element-attributes" title="Permalink to this heading">¶</a></h5>
<p>There are several ways to get a value of an attribute. First, one can use
XPath syntax:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>XPath syntax has a few advantages: it is a standard XPath feature, and
<code class="docutils literal notranslate"><span class="pre">&#64;attributes</span></code> can be used in other parts of an XPath expression - e.g.
it is possible to filter by attribute value.</p>
<p>Scrapy also provides an extension to CSS selectors (<code class="docutils literal notranslate"><span class="pre">::attr(...)</span></code>)
which allows to get attribute values:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>In addition to that, there is a <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of Selector.
You can use it if you prefer to lookup attributes in Python
code, without using XPaths or CSS extensions:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;href&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)]</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>This property is also available on SelectorList; it returns a dictionary
with attributes of a first matching element. It is convenient to use when
a selector is expected to give a single result (e.g. when selecting by element
ID, or when selecting an unique element on a page):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span>
<span class="go">{&#39;href&#39;: &#39;http://example.com/&#39;}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;href&quot;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of an empty SelectorList is empty:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span>
<span class="go">{}</span>
</pre></div>
</div>
</section>
<section id="using-selectors-with-regular-expressions">
<h5>Using selectors with regular expressions<a class="headerlink" href="#using-selectors-with-regular-expressions" title="Permalink to this heading">¶</a></h5>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> also has a <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for extracting
data using regular expressions. However, unlike using <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> or
<code class="docutils literal notranslate"><span class="pre">.css()</span></code> methods, <code class="docutils literal notranslate"><span class="pre">.re()</span></code> returns a list of strings. So you
can’t construct nested <code class="docutils literal notranslate"><span class="pre">.re()</span></code> calls.</p>
<p>Here’s an example used to extract image names from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> above:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Name:\s*(.*)&quot;</span><span class="p">)</span>
<span class="go">[&#39;My image 1 &#39;,</span>
<span class="go">&#39;My image 2 &#39;,</span>
<span class="go">&#39;My image 3 &#39;,</span>
<span class="go">&#39;My image 4 &#39;,</span>
<span class="go">&#39;My image 5 &#39;]</span>
</pre></div>
</div>
<p>There’s an additional helper reciprocating <code class="docutils literal notranslate"><span class="pre">.get()</span></code> (and its
alias <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code>) for <code class="docutils literal notranslate"><span class="pre">.re()</span></code>, named <code class="docutils literal notranslate"><span class="pre">.re_first()</span></code>.
Use it to extract just the first matching string:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Name:\s*(.*)&quot;</span><span class="p">)</span>
<span class="go">&#39;My image 1 &#39;</span>
</pre></div>
</div>
</section>
<section id="extract-and-extract-first">
<span id="old-extraction-api"></span><h5>extract() and extract_first()<a class="headerlink" href="#extract-and-extract-first" title="Permalink to this heading">¶</a></h5>
<p>If you’re a long-time Scrapy user, you’re probably familiar
with <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> and <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> selector methods. Many blog posts
and tutorials are using them as well. These methods are still supported
by Scrapy, there are <strong>no plans</strong> to deprecate them.</p>
<p>However, Scrapy usage docs are now written using <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and
<code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods. We feel that these new methods result in a more concise
and readable code.</p>
<p>The following examples show how these methods map to each other.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SelectorList.get()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">SelectorList.extract_first()</span></code>:</p></li>
</ol>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><code class="docutils literal notranslate"><span class="pre">SelectorList.getall()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">SelectorList.extract()</span></code>:</p></li>
</ol>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><code class="docutils literal notranslate"><span class="pre">Selector.get()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">Selector.extract()</span></code>:</p></li>
</ol>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>For consistency, there is also <code class="docutils literal notranslate"><span class="pre">Selector.getall()</span></code>, which returns a list:</p></li>
</ol>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;a::attr(href)&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;]</span>
</pre></div>
</div>
<p>So, the main difference is that output of <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods
is more predictable: <code class="docutils literal notranslate"><span class="pre">.get()</span></code> always returns a single result, <code class="docutils literal notranslate"><span class="pre">.getall()</span></code>
always returns a list of all extracted results. With <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> method
it was not always obvious if a result is a list or not; to get a single
result either <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> or <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> should be called.</p>
</section>
</section>
<section id="working-with-xpaths">
<span id="topics-selectors-xpaths"></span><h4>Working with XPaths<a class="headerlink" href="#working-with-xpaths" title="Permalink to this heading">¶</a></h4>
<p>Here are some tips which may help you to use XPath with Scrapy selectors
effectively. If you are not much familiar with XPath yet,
you may want to take a look first at this <a class="reference external" href="http://www.zvon.org/comp/r/tut-XPath_1.html">XPath tutorial</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some of the tips are based on <a class="reference external" href="https://www.zyte.com/blog/xpath-tips-from-the-web-scraping-trenches/">this post from Zyte’s blog</a>.</p>
</div>
<section id="working-with-relative-xpaths">
<span id="topics-selectors-relative-xpaths"></span><h5>Working with relative XPaths<a class="headerlink" href="#working-with-relative-xpaths" title="Permalink to this heading">¶</a></h5>
<p>Keep in mind that if you are nesting selectors and use an XPath that starts
with <code class="docutils literal notranslate"><span class="pre">/</span></code>, that XPath will be absolute to the document and not relative to the
<code class="docutils literal notranslate"><span class="pre">Selector</span></code> you’re calling it from.</p>
<p>For example, suppose you want to extract all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> elements inside <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code>
elements. First, you would get all <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//div&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> elements from the document, not only those
inside <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//p&quot;</span><span class="p">):</span>  <span class="c1"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>This is the proper way to do it (note the dot prefixing the <code class="docutils literal notranslate"><span class="pre">.//p</span></code> XPath):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;.//p&quot;</span><span class="p">):</span>  <span class="c1"># extracts all &lt;p&gt; inside</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>Another common case would be to extract all direct <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> children:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>For more details about relative XPaths see the <a class="reference external" href="https://www.w3.org/TR/xpath-10/#location-paths">Location Paths</a> section in the
XPath specification.</p>
</section>
<section id="when-querying-by-class-consider-using-css">
<h5>When querying by class, consider using CSS<a class="headerlink" href="#when-querying-by-class-consider-using-css" title="Permalink to this heading">¶</a></h5>
<p>Because an element can contain multiple CSS classes, the XPath way to select elements
by class is the rather verbose:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">*</span><span class="p">[</span><span class="n">contains</span><span class="p">(</span><span class="n">concat</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">normalize</span><span class="o">-</span><span class="n">space</span><span class="p">(</span><span class="nd">@class</span><span class="p">),</span> <span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="s1">&#39; someclass &#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>If you use <code class="docutils literal notranslate"><span class="pre">&#64;class='someclass'</span></code> you may end up missing elements that have
other classes, and if you just use <code class="docutils literal notranslate"><span class="pre">contains(&#64;class,</span> <span class="pre">'someclass')</span></code> to make up
for that you may end up with more elements that you want, if they have a different
class name that shares the string <code class="docutils literal notranslate"><span class="pre">someclass</span></code>.</p>
<p>As it turns out, Scrapy selectors allow you to chain selectors, so most of the time
you can just select by class using CSS and then switch to XPath when needed:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;div class=&quot;hero shout&quot;&gt;&lt;time datetime=&quot;2014-07-23 19:00&quot;&gt;Special date&lt;/time&gt;&lt;/div&gt;&#39;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;.shout&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;./time/@datetime&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;2014-07-23 19:00&#39;]</span>
</pre></div>
</div>
<p>This is cleaner than using the verbose XPath trick shown above. Just remember
to use the <code class="docutils literal notranslate"><span class="pre">.</span></code> in the XPath expressions that will follow.</p>
</section>
<section id="beware-of-the-difference-between-node-1-and-node-1">
<h5>Beware of the difference between //node[1] and (//node)[1]<a class="headerlink" href="#beware-of-the-difference-between-node-1-and-node-1" title="Permalink to this heading">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">//node[1]</span></code> selects all the nodes occurring first under their respective parents.</p>
<p><code class="docutils literal notranslate"><span class="pre">(//node)[1]</span></code> selects all the nodes in the document, and then gets only the first of them.</p>
<p>Example:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">    &lt;ul class=&quot;list&quot;&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;1&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;2&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;3&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;ul class=&quot;list&quot;&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;4&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;5&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li&gt;6&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/ul&gt;&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  elements under whatever it is its parent:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//li[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;, &#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  element in the whole document:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//li)[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  elements under an <code class="docutils literal notranslate"><span class="pre">&lt;ul&gt;</span></code>  parent:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//ul/li[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;, &#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  element under an <code class="docutils literal notranslate"><span class="pre">&lt;ul&gt;</span></code>  parent in the whole document:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//ul/li)[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
</section>
<section id="using-text-nodes-in-a-condition">
<h5>Using text nodes in a condition<a class="headerlink" href="#using-text-nodes-in-a-condition" title="Permalink to this heading">¶</a></h5>
<p>When you need to use the text content as argument to an <a class="reference external" href="https://www.w3.org/TR/xpath-10/#section-String-Functions">XPath string function</a>,
avoid using <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> and use just <code class="docutils literal notranslate"><span class="pre">.</span></code> instead.</p>
<p>This is because the expression <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> yields a collection of text elements – a <em>node-set</em>.
And when a node-set is converted to a string, which happens when it is passed as argument to
a string function like <code class="docutils literal notranslate"><span class="pre">contains()</span></code> or <code class="docutils literal notranslate"><span class="pre">starts-with()</span></code>, it results in the text for the first element only.</p>
<p>Example:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Converting a <em>node-set</em> to string:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a//text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># take a peek at the node-set</span>
<span class="go">[&#39;Click here to go to the &#39;, &#39;Next Page&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1]//text())&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># convert it to string</span>
<span class="go">[&#39;Click here to go to the &#39;]</span>
</pre></div>
</div>
<p>A <em>node</em> converted to a string, however, puts together the text of itself plus of all its descendants:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[1]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># select the first node</span>
<span class="go">[&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1])&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># convert it to string</span>
<span class="go">[&#39;Click here to go to the Next Page&#39;]</span>
</pre></div>
</div>
<p>So, using the <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> node-set won’t select anything in this case:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(.//text(), &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But using the <code class="docutils literal notranslate"><span class="pre">.</span></code> to mean the node, works:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(., &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
</pre></div>
</div>
</section>
<section id="variables-in-xpath-expressions">
<span id="topics-selectors-xpath-variables"></span><h5>Variables in XPath expressions<a class="headerlink" href="#variables-in-xpath-expressions" title="Permalink to this heading">¶</a></h5>
<p>XPath allows you to reference variables in your XPath expressions, using
the <code class="docutils literal notranslate"><span class="pre">$somevariable</span></code> syntax. This is somewhat similar to parameterized
queries or prepared statements in the SQL world where you replace
some arguments in your queries with placeholders like <code class="docutils literal notranslate"><span class="pre">?</span></code>,
which are then substituted with values passed with the query.</p>
<p>Here’s an example to match an element based on its “id” attribute value,
without hard-coding it (that was shown previously):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># `$val` used in the expression, a `val` argument needs to be passed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//div[@id=$val]/a/text()&quot;</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="s2">&quot;images&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Name: My image 1 &#39;</span>
</pre></div>
</div>
<p>Here’s another example, to find the “id” attribute of a <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> tag containing
five <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> children (here we pass the value <code class="docutils literal notranslate"><span class="pre">5</span></code> as an integer):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//div[count(a)=$cnt]/@id&quot;</span><span class="p">,</span> <span class="n">cnt</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;images&#39;</span>
</pre></div>
</div>
<p>All variable references must have a binding value when calling <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code>
(otherwise you’ll get a <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">XPath</span> <span class="pre">error:</span></code> exception).
This is done by passing as many named arguments as necessary.</p>
<p><a class="reference external" href="https://parsel.readthedocs.io/en/latest/">parsel</a>, the library powering Scrapy selectors, has more details and examples
on <a class="reference external" href="https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions">XPath variables</a>.</p>
</section>
<section id="removing-namespaces">
<span id="id2"></span><h5>Removing namespaces<a class="headerlink" href="#removing-namespaces" title="Permalink to this heading">¶</a></h5>
<p>When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> method for that.</p>
<p>Let’s show an example that illustrates this with the Python Insider blog atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>scrapy<span class="w"> </span>shell<span class="w"> </span>https://feeds.feedburner.com/PythonInsider
</pre></div>
</div>
<p>This is how the file starts:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>&lt;?xml<span class="w"> </span><span class="nv">version</span><span class="o">=</span><span class="s2">&quot;1.0&quot;</span><span class="w"> </span><span class="nv">encoding</span><span class="o">=</span><span class="s2">&quot;UTF-8&quot;</span>?&gt;
&lt;?xml-stylesheet<span class="w"> </span>...
&lt;feed<span class="w"> </span><span class="nv">xmlns</span><span class="o">=</span><span class="s2">&quot;http://www.w3.org/2005/Atom&quot;</span>
<span class="w">      </span>xmlns:openSearch<span class="o">=</span><span class="s2">&quot;http://a9.com/-/spec/opensearchrss/1.0/&quot;</span>
<span class="w">      </span>xmlns:blogger<span class="o">=</span><span class="s2">&quot;http://schemas.google.com/blogger/2008&quot;</span>
<span class="w">      </span>xmlns:georss<span class="o">=</span><span class="s2">&quot;http://www.georss.org/georss&quot;</span>
<span class="w">      </span>xmlns:gd<span class="o">=</span><span class="s2">&quot;http://schemas.google.com/g/2005&quot;</span>
<span class="w">      </span>xmlns:thr<span class="o">=</span><span class="s2">&quot;http://purl.org/syndication/thread/1.0&quot;</span>
<span class="w">      </span>xmlns:feedburner<span class="o">=</span><span class="s2">&quot;http://rssnamespace.org/feedburner/ext/1.0&quot;</span>&gt;
<span class="w">  </span>...
</pre></div>
</div>
<p>You can see several namespace declarations including a default
<code class="docutils literal notranslate"><span class="pre">&quot;http://www.w3.org/2005/Atom&quot;</span></code> and another one using the <code class="docutils literal notranslate"><span class="pre">gd:</span></code> prefix for
<code class="docutils literal notranslate"><span class="pre">&quot;http://schemas.google.com/g/2005&quot;</span></code>.</p>
<p>Once in the shell we can try selecting all <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> objects and see that it
doesn’t work (because the Atom XML namespace is obfuscating those nodes):</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But once we call the <code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> method, all
nodes can be accessed directly by their names:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">remove_namespaces</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;//link&#39; data=&#39;&lt;link rel=&quot;alternate&quot; type=&quot;text/html&quot; h&#39;&gt;,</span>
<span class="go">    &lt;Selector query=&#39;//link&#39; data=&#39;&lt;link rel=&quot;next&quot; type=&quot;application/atom+&#39;&gt;,</span>
<span class="go">    ...</span>
</pre></div>
</div>
<p>If you wonder why the namespace removal procedure isn’t always called by default
instead of having to call it manually, this is because of two reasons, which, in order
of relevance, are:</p>
<ol class="arabic simple">
<li><p>Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to perform by default
for all documents crawled by Scrapy</p></li>
<li><p>There could be some cases where using namespaces is actually required, in
case some element names clash between namespaces. These cases are very rare
though.</p></li>
</ol>
</section>
<section id="using-exslt-extensions">
<h5>Using EXSLT extensions<a class="headerlink" href="#using-exslt-extensions" title="Permalink to this heading">¶</a></h5>
<p>Being built atop <a class="reference external" href="https://lxml.de/">lxml</a>, Scrapy selectors support some <a class="reference external" href="http://exslt.org/">EXSLT</a> extensions
and come with these pre-registered namespaces to use in XPath expressions:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>prefix</p></th>
<th class="head"><p>namespace</p></th>
<th class="head"><p>usage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>re</p></td>
<td><p>http://exslt.org/regular-expressions</p></td>
<td><p><a class="reference external" href="http://exslt.org/regexp/index.html">regular expressions</a></p></td>
</tr>
<tr class="row-odd"><td><p>set</p></td>
<td><p>http://exslt.org/sets</p></td>
<td><p><a class="reference external" href="http://exslt.org/set/index.html">set manipulation</a></p></td>
</tr>
</tbody>
</table>
<section id="regular-expressions">
<h6>Regular expressions<a class="headerlink" href="#regular-expressions" title="Permalink to this heading">¶</a></h6>
<p>The <code class="docutils literal notranslate"><span class="pre">test()</span></code> function, for example, can prove quite useful when XPath’s
<code class="docutils literal notranslate"><span class="pre">starts-with()</span></code> or <code class="docutils literal notranslate"><span class="pre">contains()</span></code> are not sufficient.</p>
<p>Example selecting links in list item with a “class” attribute ending with a digit:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;ul&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//li//@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;link1.html&#39;, &#39;link2.html&#39;, &#39;link3.html&#39;, &#39;link4.html&#39;, &#39;link5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[re:test(@class, &quot;item-\d$&quot;)]//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;link1.html&#39;, &#39;link2.html&#39;, &#39;link4.html&#39;, &#39;link5.html&#39;]</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>C library <code class="docutils literal notranslate"><span class="pre">libxslt</span></code> doesn’t natively support EXSLT regular
expressions so <a class="reference external" href="https://lxml.de/">lxml</a>’s implementation uses hooks to Python’s <code class="docutils literal notranslate"><span class="pre">re</span></code> module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.</p>
</div>
</section>
<section id="set-operations">
<h6>Set operations<a class="headerlink" href="#set-operations" title="Permalink to this heading">¶</a></h6>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from <a class="reference external" href="https://schema.org/Product">https://schema.org/Product</a>)
with groups of itemscopes and corresponding itemprops:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;img src=&quot;kenmore-microwave-17in.jpg&quot; alt=&#39;Kenmore 17&quot; Microwave&#39; /&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;aggregateRating&quot;</span>
<span class="gp">... </span><span class="s2">    itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">   Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</span>
<span class="gp">... </span><span class="s2">   based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  Product description:</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</span>
<span class="gp">... </span><span class="s2">  Has six preset cooking categories and convenience features like</span>
<span class="gp">... </span><span class="s2">  Add-A-Minute and Child Lock.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  Customer reviews:</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</span>
<span class="gp">... </span><span class="s2">    it. &lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</span>
<span class="gp">... </span><span class="s2">    fits in my apartment.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  ...</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//div[@itemscope]&quot;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;current scope:&quot;</span><span class="p">,</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;@itemtype&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">())</span>
<span class="gp">... </span>    <span class="n">props</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
<span class="gp">... </span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="sd">                set:difference(./descendant::*/@itemprop,</span>
<span class="gp">... </span><span class="sd">                               .//*[@itemscope]/*/@itemprop)&quot;&quot;&quot;</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    properties: </span><span class="si">{</span><span class="n">props</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="gp">...</span>

<span class="go">current scope: [&#39;http://schema.org/Product&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;aggregateRating&#39;, &#39;offers&#39;, &#39;description&#39;, &#39;review&#39;, &#39;review&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/AggregateRating&#39;]</span>
<span class="go">    properties: [&#39;ratingValue&#39;, &#39;reviewCount&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Offer&#39;]</span>
<span class="go">    properties: [&#39;price&#39;, &#39;availability&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;author&#39;, &#39;datePublished&#39;, &#39;reviewRating&#39;, &#39;description&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [&#39;worstRating&#39;, &#39;ratingValue&#39;, &#39;bestRating&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;author&#39;, &#39;datePublished&#39;, &#39;reviewRating&#39;, &#39;description&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [&#39;worstRating&#39;, &#39;ratingValue&#39;, &#39;bestRating&#39;]</span>
</pre></div>
</div>
<p>Here we first iterate over <code class="docutils literal notranslate"><span class="pre">itemscope</span></code> elements, and for each one,
we look for all <code class="docutils literal notranslate"><span class="pre">itemprops</span></code> elements and exclude those that are themselves
inside another <code class="docutils literal notranslate"><span class="pre">itemscope</span></code>.</p>
</section>
</section>
<section id="other-xpath-extensions">
<h5>Other XPath extensions<a class="headerlink" href="#other-xpath-extensions" title="Permalink to this heading">¶</a></h5>
<p>Scrapy selectors also provide a sorely missed XPath extension function
<code class="docutils literal notranslate"><span class="pre">has-class</span></code> that returns <code class="docutils literal notranslate"><span class="pre">True</span></code> for nodes that have all of the specified
HTML classes.</p>
<p>For the following HTML:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://example.com&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">body</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;html&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;body&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;p class=&quot;foo bar-baz&quot;&gt;First&lt;/p&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;p class=&quot;foo&quot;&gt;Second&lt;/p&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;p class=&quot;bar&quot;&gt;Third&lt;/p&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;p&gt;Fourth&lt;/p&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/body&gt;</span>
<span class="gp">... </span><span class="s2">&lt;/html&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>You can use it like this:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;//p[has-class(&quot;foo&quot;)]&#39; data=&#39;&lt;p class=&quot;foo bar-baz&quot;&gt;First&lt;/p&gt;&#39;&gt;,</span>
<span class="go">&lt;Selector query=&#39;//p[has-class(&quot;foo&quot;)]&#39; data=&#39;&lt;p class=&quot;foo&quot;&gt;Second&lt;/p&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;, &quot;bar-baz&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector query=&#39;//p[has-class(&quot;foo&quot;, &quot;bar-baz&quot;)]&#39; data=&#39;&lt;p class=&quot;foo bar-baz&quot;&gt;First&lt;/p&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;, &quot;bar&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>So XPath <code class="docutils literal notranslate"><span class="pre">//p[has-class(&quot;foo&quot;,</span> <span class="pre">&quot;bar-baz&quot;)]</span></code> is roughly equivalent to CSS
<code class="docutils literal notranslate"><span class="pre">p.foo.bar-baz</span></code>.  Please note, that it is slower in most of the cases,
because it’s a pure-Python function that’s invoked for every node in question
whereas the CSS lookup is translated into XPath and thus runs more efficiently,
so performance-wise its uses are limited to situations that are not easily
described with CSS selectors.</p>
<p>Parsel also simplifies adding your own XPath extensions with
<code class="xref py py-func docutils literal notranslate"><span class="pre">set_xpathfunc()</span></code>.</p>
</section>
</section>
<section id="module-scrapy.selector">
<span id="built-in-selectors-reference"></span><span id="topics-selectors-ref"></span><h4>Built-in Selectors reference<a class="headerlink" href="#module-scrapy.selector" title="Permalink to this heading">¶</a></h4>
<section id="selector-objects">
<h5>Selector objects<a class="headerlink" href="#selector-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.selector.Selector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.selector.</span></span><span class="sig-name descname"><span class="pre">Selector</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector" title="Permalink to this definition">¶</a></dt>
<dd><p>An instance of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> is a wrapper over response to select
certain parts of its content.</p>
<p><code class="docutils literal notranslate"><span class="pre">response</span></code> is an <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> or an
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> object that will be used for selecting
and extracting data.</p>
<p><code class="docutils literal notranslate"><span class="pre">text</span></code> is a unicode string or utf-8 encoded text for cases when a
<code class="docutils literal notranslate"><span class="pre">response</span></code> isn’t available. Using <code class="docutils literal notranslate"><span class="pre">text</span></code> and <code class="docutils literal notranslate"><span class="pre">response</span></code> together is
undefined behavior.</p>
<p><code class="docutils literal notranslate"><span class="pre">type</span></code> defines the selector type, it can be <code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;xml&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;json&quot;</span></code>
or <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
<p>If <code class="docutils literal notranslate"><span class="pre">type</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the selector automatically chooses the best type
based on <code class="docutils literal notranslate"><span class="pre">response</span></code> type (see below), or defaults to <code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> in case it
is used together with <code class="docutils literal notranslate"><span class="pre">text</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">type</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> and a <code class="docutils literal notranslate"><span class="pre">response</span></code> is passed, the selector type is
inferred from the response type as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> type</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;xml&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> type</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;json&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> type</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> for anything else</p></li>
</ul>
<p>Otherwise, if <code class="docutils literal notranslate"><span class="pre">type</span></code> is set, the selector type will be forced and no
detection will occur.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.xpath">
<span class="sig-name descname"><span class="pre">xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespaces</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Mapping" title="(in Python v3.13)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.Selector.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Find nodes matching the xpath <code class="docutils literal notranslate"><span class="pre">query</span></code> and return the result as a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> interface too.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is a string containing the XPATH query to apply.</p>
<p><code class="docutils literal notranslate"><span class="pre">namespaces</span></code> is an optional <code class="docutils literal notranslate"><span class="pre">prefix:</span> <span class="pre">namespace-uri</span></code> mapping (dict)
for additional prefixes to those registered with <code class="docutils literal notranslate"><span class="pre">register_namespace(prefix,</span> <span class="pre">uri)</span></code>.
Contrary to <code class="docutils literal notranslate"><span class="pre">register_namespace()</span></code>, these prefixes are not
saved for future calls.</p>
<p>Any additional named arguments can be used to pass values for XPath
variables in the XPath expression, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[href=$url]&#39;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For convenience, this method can be called as <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.css">
<span class="sig-name descname"><span class="pre">css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.Selector.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given CSS selector and return a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is a string containing the CSS selector to apply.</p>
<p>In the background, CSS queries are translated into XPath queries using
<a class="reference external" href="https://pypi.python.org/pypi/cssselect/">cssselect</a> library and run <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> method.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For convenience, this method can be called as <code class="docutils literal notranslate"><span class="pre">response.css()</span></code></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.jmespath">
<span class="sig-name descname"><span class="pre">jmespath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.Selector.jmespath" title="Permalink to this definition">¶</a></dt>
<dd><p>Find objects matching the JMESPath <code class="docutils literal notranslate"><span class="pre">query</span></code> and return the result as a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> interface too.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is a string containing the <a class="reference external" href="https://jmespath.org/">JMESPath</a> query to apply.</p>
<p>Any additional named arguments are passed to the underlying
<code class="docutils literal notranslate"><span class="pre">jmespath.search</span></code> call, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">jmespath</span><span class="p">(</span><span class="s1">&#39;author.name&#39;</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">jmespath</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span><span class="n">dict_cls</span><span class="o">=</span><span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For convenience, this method can be called as <code class="docutils literal notranslate"><span class="pre">response.jmespath()</span></code></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.selector.Selector.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched nodes.</p>
<p>For HTML and XML, the result is always a string, and percent-encoded
content is unquoted.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.selector.Selector.attrib">
<span class="sig-name descname"><span class="pre">attrib</span></span><a class="headerlink" href="#scrapy.selector.Selector.attrib" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the attributes dictionary for underlying element.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.re">
<span class="sig-name descname"><span class="pre">re</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.Selector.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given regex and return a list of strings with the
matches.</p>
<p><code class="docutils literal notranslate"><span class="pre">regex</span></code> can be either a compiled regular expression or a string which
will be compiled to a regular expression using <code class="docutils literal notranslate"><span class="pre">re.compile(regex)</span></code>.</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>).
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.re_first">
<span class="sig-name descname"><span class="pre">re_first</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.selector.Selector.re_first" title="Permalink to this definition">¶</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">re_first</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></span></dt>
<dd><p>Apply the given regex and return the first string which matches. If
there is no match, return the default value (<code class="docutils literal notranslate"><span class="pre">None</span></code> if the argument
is not provided).</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>).
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.register_namespace">
<span class="sig-name descname"><span class="pre">register_namespace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">uri</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.selector.Selector.register_namespace" title="Permalink to this definition">¶</a></dt>
<dd><p>Register the given namespace to be used in this <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>.
Without registering namespaces you can’t select or extract data from
non-standard namespaces. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#selector-examples-xml"><span class="std std-ref">Selector examples on XML response</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.remove_namespaces">
<span class="sig-name descname"><span class="pre">remove_namespaces</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.selector.Selector.remove_namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all namespaces, allowing to traverse the document using
namespace-less xpaths. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#removing-namespaces"><span class="std std-ref">Removing namespaces</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.__bool__">
<span class="sig-name descname"><span class="pre">__bool__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.selector.Selector.__bool__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if there is any real content selected or <code class="docutils literal notranslate"><span class="pre">False</span></code>
otherwise.  In other words, the boolean value of a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> is
given by the contents it selects.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.Selector.getall">
<span class="sig-name descname"><span class="pre">getall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.Selector.getall" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched node in a 1-element list of strings.</p>
<p>This method is added to Selector for consistency; it is more useful
with SelectorList. See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

</dd></dl>

</section>
<section id="selectorlist-objects">
<h5>SelectorList objects<a class="headerlink" href="#selectorlist-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.selector.</span></span><span class="sig-name descname"><span class="pre">SelectorList</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">iterable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">/</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> class is a subclass of the builtin <code class="docutils literal notranslate"><span class="pre">list</span></code>
class, which provides a few additional methods.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.xpath">
<span class="sig-name descname"><span class="pre">xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespaces</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Mapping" title="(in Python v3.13)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">xpath</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.xpath()</span></code></a></p>
<p><code class="docutils literal notranslate"><span class="pre">namespaces</span></code> is an optional <code class="docutils literal notranslate"><span class="pre">prefix:</span> <span class="pre">namespace-uri</span></code> mapping (dict)
for additional prefixes to those registered with <code class="docutils literal notranslate"><span class="pre">register_namespace(prefix,</span> <span class="pre">uri)</span></code>.
Contrary to <code class="docutils literal notranslate"><span class="pre">register_namespace()</span></code>, these prefixes are not
saved for future calls.</p>
<p>Any additional named arguments can be used to pass values for XPath
variables in the XPath expression, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[href=$url]&#39;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.css">
<span class="sig-name descname"><span class="pre">css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.css()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.css()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.jmespath">
<span class="sig-name descname"><span class="pre">jmespath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">SelectorList</span><span class="p"><span class="pre">[</span></span><span class="pre">_SelectorType</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.jmespath" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.jmespath()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.jmespath" title="scrapy.selector.Selector.jmespath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.jmespath()</span></code></a>.</p>
<p>Any additional named arguments are passed to the underlying
<code class="docutils literal notranslate"><span class="pre">jmespath.search</span></code> call, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">jmespath</span><span class="p">(</span><span class="s1">&#39;author.name&#39;</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="n">jmespath</span><span class="o">.</span><span class="n">Options</span><span class="p">(</span><span class="n">dict_cls</span><span class="o">=</span><span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.getall">
<span class="sig-name descname"><span class="pre">getall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.getall" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.get()</span></code> method for each element is this list and return
their results flattened, as a list of strings.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.get" title="Permalink to this definition">¶</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></span></dt>
<dd><p>Return the result of <code class="docutils literal notranslate"><span class="pre">.get()</span></code> for the first element in this list.
If the list is empty, return the default value.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.re">
<span class="sig-name descname"><span class="pre">re</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for each element in this list and return
their results flattened, as a list of strings.</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>.
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.re_first">
<span class="sig-name descname"><span class="pre">re_first</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.selector.SelectorList.re_first" title="Permalink to this definition">¶</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">re_first</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">regex</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">replace_entities</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></span></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for the first element in this list and
return the result in an string. If the list is empty or the
regex doesn’t match anything, return the default value (<code class="docutils literal notranslate"><span class="pre">None</span></code> if
the argument is not provided).</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>.
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.selector.SelectorList.attrib">
<span class="sig-name descname"><span class="pre">attrib</span></span><a class="headerlink" href="#scrapy.selector.SelectorList.attrib" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the attributes dictionary for the first element.
If the list is empty, return an empty dict.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="examples">
<span id="selector-examples"></span><h4>Examples<a class="headerlink" href="#examples" title="Permalink to this heading">¶</a></h4>
<section id="selector-examples-on-html-response">
<span id="selector-examples-html"></span><h5>Selector examples on HTML response<a class="headerlink" href="#selector-examples-on-html-response" title="Permalink to this heading">¶</a></h5>
<p>Here are some <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> examples to illustrate several concepts.
In all cases, we assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> instantiated with
a <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> object like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">html_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p>Select all <code class="docutils literal notranslate"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body, returning a list of
<a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects (i.e. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Extract the text of all <code class="docutils literal notranslate"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body,
returning a list of strings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># this includes the h1 tag</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># this excludes the h1 tag</span>
</pre></div>
</div>
</li>
<li><p>Iterate over all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> tags and print their class attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//p&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">])</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="selector-examples-on-xml-response">
<span id="selector-examples-xml"></span><h5>Selector examples on XML response<a class="headerlink" href="#selector-examples-on-xml-response" title="Permalink to this heading">¶</a></h5>
<p>Here are some examples to illustrate concepts for <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects
instantiated with an <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">xml_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p>Select all <code class="docutils literal notranslate"><span class="pre">&lt;product&gt;</span></code> elements from an XML response body, returning a list
of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects (i.e. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//product&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Extract all prices from a <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> which requires registering
a namespace:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">register_namespace</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;http://base.google.com/ns/1.0&quot;</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//g:price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<span id="document-topics/items"></span><section id="module-scrapy.item">
<span id="items"></span><span id="topics-items"></span><h3>Items<a class="headerlink" href="#module-scrapy.item" title="Permalink to this heading">¶</a></h3>
<p>The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a> may return the
extracted data as <cite>items</cite>, Python objects that define key-value pairs.</p>
<p>Scrapy supports <a class="hxr-hoverxref hxr-tooltip reference internal" href="#item-types"><span class="std std-ref">multiple types of items</span></a>. When you create an
item, you may use whichever type of item you want. When you write code that
receives an item, your code should <a class="hxr-hoverxref hxr-tooltip reference internal" href="#supporting-item-types"><span class="std std-ref">work for any item type</span></a>.</p>
<section id="item-types">
<span id="id1"></span><h4>Item Types<a class="headerlink" href="#item-types" title="Permalink to this heading">¶</a></h4>
<p>Scrapy supports the following types of items, via the <a class="reference external" href="https://github.com/scrapy/itemadapter">itemadapter</a> library:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#dict-items"><span class="std std-ref">dictionaries</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="#item-objects"><span class="std std-ref">Item objects</span></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#dataclass-items"><span class="std std-ref">dataclass objects</span></a>, and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#attrs-items"><span class="std std-ref">attrs objects</span></a>.</p>
<section id="dictionaries">
<span id="dict-items"></span><h5>Dictionaries<a class="headerlink" href="#dictionaries" title="Permalink to this heading">¶</a></h5>
<p>As an item type, <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> is convenient and familiar.</p>
</section>
<section id="item-objects">
<span id="id2"></span><h5>Item objects<a class="headerlink" href="#item-objects" title="Permalink to this heading">¶</a></h5>
<p><a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> provides a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>-like API plus additional features that
make it the most feature-complete item type:</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.Item">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.</span></span><span class="sig-name descname"><span class="pre">Item</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.Item" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for scraped items.</p>
<p>In Scrapy, an object is considered an <code class="docutils literal notranslate"><span class="pre">item</span></code> if it’s supported by the
<a class="reference external" href="https://github.com/scrapy/itemadapter">itemadapter</a> library. For example, when the output of a spider callback
is evaluated, only such objects are passed to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a>. <a class="reference internal" href="#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> is one of the classes supported by
<a class="reference external" href="https://github.com/scrapy/itemadapter">itemadapter</a> by default.</p>
<p>Items must declare <a class="reference internal" href="#scrapy.Field" title="scrapy.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> attributes, which are processed and stored
in the <code class="docutils literal notranslate"><span class="pre">fields</span></code> attribute. This restricts the set of allowed field names
and prevents typos, raising <code class="docutils literal notranslate"><span class="pre">KeyError</span></code> when referring to undefined fields.
Additionally, fields can be used to define metadata and control the way
data is processed internally. Please refer to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-items-fields"><span class="std std-ref">documentation
about fields</span></a> for additional information.</p>
<p>Unlike instances of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>, instances of <a class="reference internal" href="#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> may be
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">tracked</span></a> to debug memory leaks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Item.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.Item.copy" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.Item.deepcopy">
<span class="sig-name descname"><span class="pre">deepcopy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.Item.deepcopy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference external" href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">deepcopy()</span></code></a> of this item.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.Item.fields">
<span class="sig-name descname"><span class="pre">fields</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.Field" title="scrapy.item.Field"><span class="pre">scrapy.item.Field</span></a><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">{}</span></em><a class="headerlink" href="#scrapy.Item.fields" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing <em>all declared fields</em> for this Item, not only
those populated. The keys are the field names and the values are the
<a class="reference internal" href="#scrapy.Field" title="scrapy.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects used in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-items-declaring"><span class="std std-ref">Item declaration</span></a>.</p>
</dd></dl>

</dd></dl>

<p><a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects replicate the standard <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> API, including
its <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p><a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> allows the defining of field names, so that:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">KeyError</span></code></a> is raised when using undefined field names (i.e.
prevents typos going unnoticed)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">Item exporters</span></a> can export all fields by
default even if the first scraped object does not have values for all
of them</p></li>
</ul>
<p><a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> also allows the defining of field metadata, which can be used to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters-field-serialization"><span class="std std-ref">customize serialization</span></a>.</p>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code> tracks <a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects to help find memory leaks
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a>).</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span>


<span class="k">class</span> <span class="nc">CustomItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">one_field</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">another_field</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="dataclass-objects">
<span id="dataclass-items"></span><h5>Dataclass objects<a class="headerlink" href="#dataclass-objects" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<p><a class="reference external" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">dataclass()</span></code></a> allows the defining of item classes with field names,
so that <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporters</span></a> can export all fields by
default even if the first scraped object does not have values for all of them.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">dataclass</span></code> items also allow you to:</p>
<ul class="simple">
<li><p>define the type and default value of each defined field.</p></li>
<li><p>define custom field metadata through <a class="reference external" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">dataclasses.field()</span></code></a>, which can be used to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters-field-serialization"><span class="std std-ref">customize serialization</span></a>.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CustomItem</span><span class="p">:</span>
    <span class="n">one_field</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">another_field</span><span class="p">:</span> <span class="nb">int</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Field types are not enforced at run time.</p>
</div>
</section>
<section id="attr-s-objects">
<span id="attrs-items"></span><h5>attr.s objects<a class="headerlink" href="#attr-s-objects" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<p><a class="reference external" href="https://www.attrs.org/en/stable/api-attr.html#attr.s" title="(in attrs v24.2)"><code class="xref py py-func docutils literal notranslate"><span class="pre">attr.s()</span></code></a> allows the defining of item classes with field names,
so that <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporters</span></a> can export all fields by
default even if the first scraped object does not have values for all of them.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">attr.s</span></code> items also allow to:</p>
<ul class="simple">
<li><p>define the type and default value of each defined field.</p></li>
<li><p>define custom field <a class="reference external" href="https://www.attrs.org/en/stable/examples.html#metadata" title="(in attrs v24.2)"><span class="xref std std-ref">metadata</span></a>, which can be used to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters-field-serialization"><span class="std std-ref">customize serialization</span></a>.</p></li>
</ul>
<p>In order to use this type, the <a class="reference external" href="https://www.attrs.org/en/stable/index.html" title="(in attrs v24.2)"><span class="xref std std-doc">attrs package</span></a> needs to be installed.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">attr</span>


<span class="nd">@attr</span><span class="o">.</span><span class="n">s</span>
<span class="k">class</span> <span class="nc">CustomItem</span><span class="p">:</span>
    <span class="n">one_field</span> <span class="o">=</span> <span class="n">attr</span><span class="o">.</span><span class="n">ib</span><span class="p">()</span>
    <span class="n">another_field</span> <span class="o">=</span> <span class="n">attr</span><span class="o">.</span><span class="n">ib</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="working-with-item-objects">
<h4>Working with Item objects<a class="headerlink" href="#working-with-item-objects" title="Permalink to this heading">¶</a></h4>
<section id="declaring-item-subclasses">
<span id="topics-items-declaring"></span><h5>Declaring Item subclasses<a class="headerlink" href="#declaring-item-subclasses" title="Permalink to this heading">¶</a></h5>
<p>Item subclasses are declared using a simple class definition syntax and
<a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects. Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">tags</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">last_updated</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Those familiar with <a class="reference external" href="https://www.djangoproject.com/">Django</a> will notice that Scrapy Items are
declared similar to <a class="reference external" href="https://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a>, except that Scrapy Items are much
simpler as there is no concept of different field types.</p>
</div>
</section>
<section id="declaring-fields">
<span id="topics-items-fields"></span><h5>Declaring fields<a class="headerlink" href="#declaring-fields" title="Permalink to this heading">¶</a></h5>
<p><a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects are used to specify metadata for each field. For
example, the serializer function for the <code class="docutils literal notranslate"><span class="pre">last_updated</span></code> field illustrated in
the example above.</p>
<p>You can specify any kind of metadata for each field. There is no restriction on
the values accepted by <a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects. For this same
reason, there is no reference list of all available metadata keys. Each key
defined in <a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects could be used by a different component, and
only those components know about it. You can also define and use any other
<a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> key in your project too, for your own needs. The main goal of
<a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.</p>
<p>It’s important to note that the <a class="reference internal" href="#scrapy.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the <a class="reference internal" href="#scrapy.Item.fields" title="scrapy.Item.fields"><code class="xref py py-attr docutils literal notranslate"><span class="pre">fields</span></code></a> attribute.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.Field">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.</span></span><span class="sig-name descname"><span class="pre">Field</span></span><a class="headerlink" href="#scrapy.Field" title="Permalink to this definition">¶</a></dt>
<dd><p>Container of field metadata</p>
<p>The <a class="reference internal" href="#scrapy.Field" title="scrapy.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> class is just an alias to the built-in <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> class and
doesn’t provide any extra functionality or attributes. In other words,
<a class="reference internal" href="#scrapy.Field" title="scrapy.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects are plain-old Python dicts. A separate class is used
to support the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-items-declaring"><span class="std std-ref">item declaration syntax</span></a>
based on class attributes.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Field metadata can also be declared for <code class="docutils literal notranslate"><span class="pre">dataclass</span></code> and <code class="docutils literal notranslate"><span class="pre">attrs</span></code>
items. Please refer to the documentation for <a class="reference external" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field">dataclasses.field</a> and
<a class="reference external" href="https://www.attrs.org/en/stable/api-attr.html#attr.ib">attr.ib</a> for additional information.</p>
</div>
</section>
<section id="id4">
<h5>Working with Item objects<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<p>Here are some examples of common tasks performed with items, using the
<code class="docutils literal notranslate"><span class="pre">Product</span></code> item <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-items-declaring"><span class="std std-ref">declared above</span></a>. You will
notice the API is very similar to the <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> API.</p>
<section id="creating-items">
<h6>Creating items<a class="headerlink" href="#creating-items" title="Permalink to this heading">¶</a></h6>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Desktop PC&quot;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
</section>
<section id="getting-field-values">
<h6>Getting field values<a class="headerlink" href="#getting-field-values" title="Permalink to this heading">¶</a></h6>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span>
<span class="go">Desktop PC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span>
<span class="go">Desktop PC</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span>
<span class="go">1000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;last_updated&quot;</span><span class="p">]</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="w">    </span><span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;last_updated&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;last_updated&quot;</span><span class="p">,</span> <span class="s2">&quot;not set&quot;</span><span class="p">)</span>
<span class="go">not set</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;lala&quot;</span><span class="p">]</span>  <span class="c1"># getting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="w">    </span><span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;lala&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lala&quot;</span><span class="p">,</span> <span class="s2">&quot;unknown field&quot;</span><span class="p">)</span>
<span class="go">&#39;unknown field&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;name&quot;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is name field populated?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;last_updated&quot;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is last_updated populated?</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;last_updated&quot;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is last_updated a declared field?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;lala&quot;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is lala a declared field?</span>
<span class="go">False</span>
</pre></div>
</div>
</section>
<section id="setting-field-values">
<h6>Setting field values<a class="headerlink" href="#setting-field-values" title="Permalink to this heading">¶</a></h6>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;last_updated&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;today&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;last_updated&quot;</span><span class="p">]</span>
<span class="go">today</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s2">&quot;lala&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span>  <span class="c1"># setting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="w">    </span><span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</section>
<section id="accessing-all-populated-values">
<h6>Accessing all populated values<a class="headerlink" href="#accessing-all-populated-values" title="Permalink to this heading">¶</a></h6>
<p>To access all populated values, just use the typical <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> API:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;price&#39;, &#39;name&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="go">[(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;Desktop PC&#39;)]</span>
</pre></div>
</div>
</section>
<section id="copying-items">
<span id="id5"></span><h6>Copying items<a class="headerlink" href="#copying-items" title="Permalink to this heading">¶</a></h6>
<p>To copy an item, you must first decide whether you want a shallow copy or a
deep copy.</p>
<p>If your item contains <a class="reference external" href="https://docs.python.org/3/glossary.html#term-mutable" title="(in Python v3.13)"><span class="xref std std-term">mutable</span></a> values like lists or dictionaries,
a shallow copy will keep references to the same mutable values across all
different copies.</p>
<p>For example, if you have an item with a list of tags, and you create a shallow
copy of that item, both the original item and the copy have the same list of
tags. Adding a tag to the list of one of the items will add the tag to the
other item as well.</p>
<p>If that is not the desired behavior, use a deep copy instead.</p>
<p>See <a class="reference external" href="https://docs.python.org/3/library/copy.html#module-copy" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">copy</span></code></a> for more information.</p>
<p>To create a shallow copy of an item, you can either call
<a class="reference internal" href="#scrapy.Item.copy" title="scrapy.Item.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code></a> on an existing item
(<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">product.copy()</span></code>) or instantiate your item class from an existing
item (<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">Product(product)</span></code>).</p>
<p>To create a deep copy, call <a class="reference internal" href="#scrapy.Item.deepcopy" title="scrapy.Item.deepcopy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">deepcopy()</span></code></a> instead
(<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">product.deepcopy()</span></code>).</p>
</section>
<section id="other-common-tasks">
<h6>Other common tasks<a class="headerlink" href="#other-common-tasks" title="Permalink to this heading">¶</a></h6>
<p>Creating dicts from items:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">dict</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>  <span class="c1"># create a dict from all populated values</span>
<span class="go">{&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;}</span>

<span class="go">Creating items from dicts:</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Laptop PC&quot;</span><span class="p">,</span> <span class="s2">&quot;price&quot;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>
<span class="go">Product(price=1500, name=&#39;Laptop PC&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Laptop PC&quot;</span><span class="p">,</span> <span class="s2">&quot;lala&quot;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>  <span class="c1"># warning: unknown field in dict</span>
<span class="gt">Traceback (most recent call last):</span>
<span class="w">    </span><span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</section>
</section>
<section id="extending-item-subclasses">
<h5>Extending Item subclasses<a class="headerlink" href="#extending-item-subclasses" title="Permalink to this heading">¶</a></h5>
<p>You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DiscountedProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">discount_percent</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">discount_expiration_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SpecificProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">Product</span><span class="o">.</span><span class="n">fields</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">],</span> <span class="n">serializer</span><span class="o">=</span><span class="n">my_serializer</span><span class="p">)</span>
</pre></div>
</div>
<p>That adds (or replaces) the <code class="docutils literal notranslate"><span class="pre">serializer</span></code> metadata key for the <code class="docutils literal notranslate"><span class="pre">name</span></code> field,
keeping all the previously existing metadata values.</p>
</section>
</section>
<section id="supporting-all-item-types">
<span id="supporting-item-types"></span><h4>Supporting All Item Types<a class="headerlink" href="#supporting-all-item-types" title="Permalink to this heading">¶</a></h4>
<p>In code that receives an item, such as methods of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a>, it is a good practice to use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ItemAdapter</span></code> class and the
<code class="xref py py-func docutils literal notranslate"><span class="pre">is_item()</span></code> function to write code that works for
any supported item type.</p>
</section>
<section id="other-classes-related-to-items">
<h4>Other classes related to items<a class="headerlink" href="#other-classes-related-to-items" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.item.ItemMeta">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.item.</span></span><span class="sig-name descname"><span class="pre">ItemMeta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">class_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">bases</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attrs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.item.ItemMeta" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://realpython.com/python-metaclasses">Metaclass</a> of <a class="reference internal" href="#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> that handles field definitions.</p>
</dd></dl>

</section>
</section>
<span id="document-topics/loaders"></span><section id="module-scrapy.loader">
<span id="item-loaders"></span><span id="topics-loaders"></span><h3>Item Loaders<a class="headerlink" href="#module-scrapy.loader" title="Permalink to this heading">¶</a></h3>
<p>Item Loaders provide a convenient mechanism for populating scraped <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">items</span></a>. Even though items can be populated directly, Item Loaders provide a
much more convenient API for populating them from a scraping process, by automating
some common tasks like parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">items</span></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Item Loaders are an extension of the <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/">itemloaders</a> library that make it
easier to work with Scrapy by adding support for
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-response"><span class="std std-ref">responses</span></a>.</p>
</div>
<section id="using-item-loaders-to-populate-items">
<h4>Using Item Loaders to populate items<a class="headerlink" href="#using-item-loaders-to-populate-items" title="Permalink to this heading">¶</a></h4>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a> or without one, in which
case an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a> is automatically created in the
Item Loader <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a> class
specified in the <a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_item_class</span></code></a> attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a>. You can add more than one value to
the same item field; the Item Loader will know how to “join” those values later
using a proper processing function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Collected data is internally stored as lists,
allowing to add several values to the same field.
If an <code class="docutils literal notranslate"><span class="pre">item</span></code> argument is passed when creating a loader,
each of the item’s values will be stored as-is if it’s already
an iterable, or wrapped with a list if it’s a single value.</p>
</div>
<p>Here is a typical Item Loader usage in a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spider</span></a>, using
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items-declaring"><span class="std std-ref">Product item</span></a> declared in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">Items
chapter</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">Product</span>


<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_name&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_title&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s2">&quot;stock&quot;</span><span class="p">,</span> <span class="s2">&quot;p#stock&quot;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s2">&quot;last_updated&quot;</span><span class="p">,</span> <span class="s2">&quot;today&quot;</span><span class="p">)</span>  <span class="c1"># you can also use literal values</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>By quickly looking at that code, we can see the <code class="docutils literal notranslate"><span class="pre">name</span></code> field is being
extracted from two different XPath locations in the page:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">//div[&#64;class=&quot;product_name&quot;]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">//div[&#64;class=&quot;product_title&quot;]</span></code></p></li>
</ol>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a> method. This is the
data that will be assigned to the <code class="docutils literal notranslate"><span class="pre">name</span></code> field later.</p>
<p>Afterwards, similar calls are used for <code class="docutils literal notranslate"><span class="pre">price</span></code> and <code class="docutils literal notranslate"><span class="pre">stock</span></code> fields
(the latter using a CSS selector with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> method),
and finally the <code class="docutils literal notranslate"><span class="pre">last_update</span></code> field is populated directly with a literal value
(<code class="docutils literal notranslate"><span class="pre">today</span></code>) using a different method: <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>.</p>
<p>Finally, when all data is collected, the <a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> method is
called which actually returns the item populated with the data
previously extracted and collected with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>, and <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> calls.</p>
</section>
<section id="working-with-dataclass-items">
<span id="topics-loaders-dataclass"></span><h4>Working with dataclass items<a class="headerlink" href="#working-with-dataclass-items" title="Permalink to this heading">¶</a></h4>
<p>By default, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#dataclass-items"><span class="std std-ref">dataclass items</span></a> require all fields to be
passed when created. This could be an issue when using dataclass items with
item loaders: unless a pre-populated item is passed to the loader, fields
will be populated incrementally using the loader’s <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> and <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> methods.</p>
<p>One approach to overcome this is to define items using the
<a class="reference external" href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">field()</span></code></a> function, with a <code class="docutils literal notranslate"><span class="pre">default</span></code> argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">InventoryItem</span><span class="p">:</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">price</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">stock</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="input-and-output-processors">
<span id="topics-loaders-processors"></span><h4>Input and Output processors<a class="headerlink" href="#input-and-output-processors" title="Permalink to this heading">¶</a></h4>
<p>An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it’s
received (through the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> or
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
<a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> method is called to populate and get the populated
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a>.  That’s when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.</p>
<p>Let’s see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span>  <span class="c1"># (1)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span>  <span class="c1"># (2)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span>  <span class="c1"># (3)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>  <span class="c1"># (4)</span>
<span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>  <span class="c1"># (5)</span>
</pre></div>
</div>
<p>So what happens is:</p>
<ol class="arabic simple">
<li><p>Data from <code class="docutils literal notranslate"><span class="pre">xpath1</span></code> is extracted, and passed through the <em>input processor</em> of
the <code class="docutils literal notranslate"><span class="pre">name</span></code> field. The result of the input processor is collected and kept in
the Item Loader (but not yet assigned to the item).</p></li>
<li><p>Data from <code class="docutils literal notranslate"><span class="pre">xpath2</span></code> is extracted, and passed through the same <em>input
processor</em> used in (1). The result of the input processor is appended to the
data collected in (1) (if any).</p></li>
<li><p>This case is similar to the previous ones, except that the data is extracted
from the <code class="docutils literal notranslate"><span class="pre">css</span></code> CSS selector, and passed through the same <em>input
processor</em> used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</p></li>
<li><p>This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.</p></li>
<li><p>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the <code class="docutils literal notranslate"><span class="pre">name</span></code> field.
The result of the output processor is the value assigned to the <code class="docutils literal notranslate"><span class="pre">name</span></code>
field in the item.</p></li>
</ol>
<p>It’s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterable.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span>Processors no longer need to be methods.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Both input and output processors must receive an iterable as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
</div>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/">itemloaders</a> comes with some <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors" title="(in itemloaders)"><span class="xref std std-ref">commonly used
processors</span></a> built-in for convenience.</p>
</section>
<section id="declaring-item-loaders">
<h4>Declaring Item Loaders<a class="headerlink" href="#declaring-item-loaders" title="Permalink to this heading">¶</a></h4>
<p>Item Loaders are declared using a class definition syntax. Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>
<span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>


<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c1"># ...</span>
</pre></div>
</div>
<p>As you can see, input processors are declared using the <code class="docutils literal notranslate"><span class="pre">_in</span></code> suffix while
output processors are declared using the <code class="docutils literal notranslate"><span class="pre">_out</span></code> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_input_processor</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_output_processor</span></code></a> attributes.</p>
</section>
<section id="declaring-input-and-output-processors">
<span id="topics-loaders-processors-declaring"></span><h4>Declaring Input and Output Processors<a class="headerlink" href="#declaring-input-and-output-processors" title="Permalink to this heading">¶</a></h4>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it’s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items-fields"><span class="std std-ref">Item Field</span></a>
metadata. Here is an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">TakeFirst</span>
<span class="kn">from</span> <span class="nn">w3lib.html</span> <span class="kn">import</span> <span class="n">remove_tags</span>


<span class="k">def</span> <span class="nf">filter_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">value</span>


<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">Join</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">,</span> <span class="n">filter_price</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">TakeFirst</span><span class="p">(),</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Welcome to my&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;strong&gt;website&lt;/strong&gt;&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&amp;euro;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;span&gt;1000&lt;/span&gt;&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
<span class="go">{&#39;name&#39;: &#39;Welcome to my website&#39;, &#39;price&#39;: &#39;1000&#39;}</span>
</pre></div>
</div>
<p>The precedence order, for both input and output processors, is as follows:</p>
<ol class="arabic simple">
<li><p>Item Loader field-specific attributes: <code class="docutils literal notranslate"><span class="pre">field_in</span></code> and <code class="docutils literal notranslate"><span class="pre">field_out</span></code> (most
precedence)</p></li>
<li><p>Field metadata (<code class="docutils literal notranslate"><span class="pre">input_processor</span></code> and <code class="docutils literal notranslate"><span class="pre">output_processor</span></code> key)</p></li>
<li><p>Item Loader defaults: <a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.default_input_processor()</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.default_output_processor()</span></code></a> (least precedence)</p></li>
</ol>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-loaders-extending"><span class="std std-ref">Reusing and extending Item Loaders</span></a>.</p>
</section>
<section id="item-loader-context">
<span id="topics-loaders-context"></span><h4>Item Loader Context<a class="headerlink" href="#item-loader-context" title="Permalink to this heading">¶</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <code class="docutils literal notranslate"><span class="pre">parse_length</span></code> which receives a text
value and extracts a length from it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_length</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">loader_context</span><span class="p">):</span>
    <span class="n">unit</span> <span class="o">=</span> <span class="n">loader_context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unit&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">)</span>
    <span class="c1"># ... length parsing code goes here ...</span>
    <span class="k">return</span> <span class="n">parsed_length</span>
</pre></div>
</div>
<p>By accepting a <code class="docutils literal notranslate"><span class="pre">loader_context</span></code> argument the function is explicitly telling
the Item Loader that it’s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<code class="docutils literal notranslate"><span class="pre">parse_length</span></code> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol class="arabic">
<li><p>By modifying the currently active Item Loader context
(<a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal notranslate"><span class="pre">context</span></code></a> attribute):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="s2">&quot;unit&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cm&quot;</span>
</pre></div>
</div>
</li>
<li><p>On Item Loader instantiation (the keyword arguments of Item Loader
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method are stored in the Item Loader context):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;cm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <code class="xref py py-class docutils literal notranslate"><span class="pre">MapCompose</span></code> is one of
them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">length_out</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">parse_length</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s2">&quot;cm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="itemloader-objects">
<h4>ItemLoader objects<a class="headerlink" href="#itemloader-objects" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.loader.</span></span><span class="sig-name descname"><span class="pre">ItemLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selector</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><span class="pre">Selector</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><span class="pre">TextResponse</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parent</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/api-reference.html#itemloaders.ItemLoader" title="(in itemloaders)"><span class="pre">itemloaders.ItemLoader</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>A user-friendly abstraction to populate an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a> with data
by applying <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-loaders-processors"><span class="std std-ref">field processors</span></a> to scraped data.
When instantiated with a <code class="docutils literal notranslate"><span class="pre">selector</span></code> or a <code class="docutils literal notranslate"><span class="pre">response</span></code> it supports
data extraction from web pages using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.Item" title="scrapy.item.Item"><em>scrapy.item.Item</em></a>) – The item instance to populate using subsequent calls to
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>,
or <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>.</p></li>
<li><p><strong>selector</strong> (<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object) – The selector to extract data from, when using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_xpath" title="scrapy.loader.ItemLoader.replace_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_xpath()</span></code></a>, or
<a class="reference internal" href="#scrapy.loader.ItemLoader.replace_css" title="scrapy.loader.ItemLoader.replace_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_css()</span></code></a> method.</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – The response used to construct the selector using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_selector_class</span></code></a>, unless the selector argument is given,
in which case this argument is ignored.</p></li>
</ul>
</dd>
</dl>
<p>If no item is given, one is instantiated automatically using the class in
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_item_class</span></code></a>.</p>
<p>The item, selector, response and remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal notranslate"><span class="pre">context</span></code></a> attribute).</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.item">
<span class="sig-name descname"><span class="pre">item</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.item" title="Permalink to this definition">¶</a></dt>
<dd><p>The item object being parsed by this Item Loader.
This is mostly used as a property so, when attempting to override this
value, you may want to check out <a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_item_class</span></code></a> first.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.context">
<span class="sig-name descname"><span class="pre">context</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.context" title="Permalink to this definition">¶</a></dt>
<dd><p>The currently active <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context" title="(in itemloaders)"><span class="xref std std-ref">Context</span></a> of this Item Loader.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.default_item_class">
<span class="sig-name descname"><span class="pre">default_item_class</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.default_item_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a> class (or factory), used to instantiate
items when not given in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.default_input_processor">
<span class="sig-name descname"><span class="pre">default_input_processor</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.default_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default input processor to use for those fields which don’t specify
one.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.default_output_processor">
<span class="sig-name descname"><span class="pre">default_output_processor</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.default_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default output processor to use for those fields which don’t specify
one.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.default_selector_class">
<span class="sig-name descname"><span class="pre">default_selector_class</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.default_selector_class" title="Permalink to this definition">¶</a></dt>
<dd><p>The class used to construct the <a class="reference internal" href="#scrapy.loader.ItemLoader.selector" title="scrapy.loader.ItemLoader.selector"><code class="xref py py-attr docutils literal notranslate"><span class="pre">selector</span></code></a> of this
<a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>, if only a response is given in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.
If a selector is given in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.selector">
<span class="sig-name descname"><span class="pre">selector</span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object to extract data from.
It’s either the selector given in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method or one created from
the response given in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_selector_class</span></code></a>. This attribute is meant to be
read-only.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.add_css">
<span class="sig-name descname"><span class="pre">add_css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">css</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.add_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_css" title="scrapy.loader.ItemLoader.get_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_css()</span></code></a> for <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the CSS selector to extract data from</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.add_jmes">
<span class="sig-name descname"><span class="pre">add_jmes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">jmes</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_jmes" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.add_value()</span></code></a> but receives a JMESPath selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_jmes" title="scrapy.loader.ItemLoader.get_jmes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_jmes()</span></code></a> for <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>jmes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the JMESPath selector to extract data from</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: {&quot;name&quot;: &quot;Color TV&quot;}</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_jmes</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: {&quot;price&quot;: the price is $1200&quot;}</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_jmes</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.add_value">
<span class="sig-name descname"><span class="pre">add_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process and then add the given <code class="docutils literal notranslate"><span class="pre">value</span></code> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_value()</span></code></a> by giving the
<code class="docutils literal notranslate"><span class="pre">processors</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>, and then passed through the
<a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/processors.html#processors" title="(in itemloaders)"><span class="xref std std-ref">field input processor</span></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <code class="docutils literal notranslate"><span class="pre">field_name</span></code> can be <code class="docutils literal notranslate"><span class="pre">None</span></code>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;Color TV&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;colours&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">:</span> <span class="s1">&#39;male&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.add_xpath">
<span class="sig-name descname"><span class="pre">add_xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.add_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_xpath" title="scrapy.loader.ItemLoader.get_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_xpath()</span></code></a> for <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>xpath</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the XPath to extract data from</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_collected_values">
<span class="sig-name descname"><span class="pre">get_collected_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_collected_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values for the given field.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_css">
<span class="sig-name descname"><span class="pre">get_css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">css</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.13)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the CSS selector to extract data from</p></li>
<li><p><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><em>Pattern</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em>) – a regular expression to use for extracting data from the
selected CSS region</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_jmes">
<span class="sig-name descname"><span class="pre">get_jmes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jmes</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.13)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_jmes" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_value()</span></code></a> but receives a JMESPath selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>jmes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the JMESPath selector to extract data from</p></li>
<li><p><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><em>Pattern</em></a>) – a regular expression to use for extracting data from the
selected JMESPath</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: {&quot;name&quot;: &quot;Color TV&quot;}</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_jmes</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: {&quot;price&quot;: the price is $1200&quot;}</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_jmes</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_output_value">
<span class="sig-name descname"><span class="pre">get_output_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_output_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values parsed using the output processor, for the
given field. This method doesn’t populate or modify the item at all.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_value">
<span class="sig-name descname"><span class="pre">get_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process the given <code class="docutils literal notranslate"><span class="pre">value</span></code> by the given <code class="docutils literal notranslate"><span class="pre">processors</span></code> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><em>Pattern</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em>) – a regular expression to use for extracting data from the
given value using <code class="xref py py-func docutils literal notranslate"><span class="pre">extract_regex()</span></code> method,
applied before processors</p>
</dd>
</dl>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itemloaders</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s1">&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="go">&#39;FOO&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.get_xpath">
<span class="sig-name descname"><span class="pre">get_xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.13)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><span class="pre">Pattern</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>xpath</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the XPath to extract data from</p></li>
<li><p><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Pattern" title="(in Python v3.13)"><em>Pattern</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>]</em>) – a regular expression to use for extracting data from the
selected XPath region</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.load_item">
<span class="sig-name descname"><span class="pre">load_item</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.load_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/processors.html#processors" title="(in itemloaders)"><span class="xref std std-ref">output processors</span></a> to get the final value to assign to each item field.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.nested_css">
<span class="sig-name descname"><span class="pre">nested_css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">css</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with a css selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the item
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.nested_xpath">
<span class="sig-name descname"><span class="pre">nested_xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">context</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with an xpath selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the item
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.replace_css">
<span class="sig-name descname"><span class="pre">replace_css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">css</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> but replaces collected data instead of adding it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.replace_jmes">
<span class="sig-name descname"><span class="pre">replace_jmes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">jmes</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_jmes" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_jmes" title="scrapy.loader.ItemLoader.add_jmes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_jmes()</span></code></a> but replaces collected data instead of adding it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.replace_value">
<span class="sig-name descname"><span class="pre">replace_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> but replaces the collected data with the
new value instead of adding it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.loader.ItemLoader.replace_xpath">
<span class="sig-name descname"><span class="pre">replace_xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field_name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">processors</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">re</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Pattern</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kw</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a> but replaces collected data instead of adding it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current ItemLoader instance for method chaining.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">ItemLoader</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="nested-loaders">
<span id="topics-loaders-nested"></span><h4>Nested Loaders<a class="headerlink" href="#nested-loaders" title="Permalink to this heading">¶</a></h4>
<p>When parsing related values from a subsection of a document, it can be
useful to create nested loaders.  Imagine you’re extracting details from
a footer of a page that looks something like:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">footer</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;social&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;https://facebook.com/whatever&quot;</span><span class="o">&gt;</span><span class="n">Like</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;social&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;https://twitter.com/whatever&quot;</span><span class="o">&gt;</span><span class="n">Follow</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;email&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;mailto:whatever@example.com&quot;</span><span class="o">&gt;</span><span class="n">Email</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">footer</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Without nested loaders, you need to specify the full xpath (or css) for each value
that you wish to extract.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;social&quot;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;email&quot;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>Instead, you can create a nested loader with the footer selector and add values
relative to the footer.  The functionality is the same but you avoid repeating
the footer selector.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">footer_loader</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">nested_xpath</span><span class="p">(</span><span class="s2">&quot;//footer&quot;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;social&quot;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s2">&quot;email&quot;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="c1"># no need to call footer_loader.load_item()</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>You can nest loaders arbitrarily and they work with either xpath or css selectors.
As a general guideline, use nested loaders when they make your code simpler but do
not go overboard with nesting or your parser can become difficult to read.</p>
</section>
<section id="reusing-and-extending-item-loaders">
<span id="topics-loaders-extending"></span><h4>Reusing and extending Item Loaders<a class="headerlink" href="#reusing-and-extending-item-loaders" title="Permalink to this heading">¶</a></h4>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <code class="docutils literal notranslate"><span class="pre">---Plasma</span> <span class="pre">TV---</span></code>) and you don’t want to end up scraping
those dashes in the final product names.</p>
<p>Here’s how you can remove those dashes by reusing and extending the default
Product Item Loader (<code class="docutils literal notranslate"><span class="pre">ProductLoader</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>


<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <code class="docutils literal notranslate"><span class="pre">CDATA</span></code> occurrences. Here’s an example of how to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>
<span class="kn">from</span> <span class="nn">myproject.utils.xml</span> <span class="kn">import</span> <span class="n">remove_cdata</span>


<span class="k">class</span> <span class="nc">XmlProductLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_cdata</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-loaders-processors-declaring"><span class="std std-ref">Declaring Input and Output Processors</span></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn’t impose any specific
organization of your Loaders collection - that’s up to you and your project’s
needs.</p>
</section>
</section>
<span id="document-topics/shell"></span><section id="scrapy-shell">
<span id="topics-shell"></span><h3>Scrapy shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this heading">¶</a></h3>
<p>The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It’s meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you’re trying to scrape. It
allows you to interactively test your expressions while you’re writing your
spider, without having to run the spider to test every change.</p>
<p>Once you get familiarized with the Scrapy shell, you’ll see that it’s an
invaluable tool for developing and debugging your spiders.</p>
<section id="configuring-the-shell">
<h4>Configuring the shell<a class="headerlink" href="#configuring-the-shell" title="Permalink to this heading">¶</a></h4>
<p>If you have <a class="reference external" href="https://ipython.org/">IPython</a> installed, the Scrapy shell will use it (instead of the
standard Python console). The <a class="reference external" href="https://ipython.org/">IPython</a> console is much more powerful and
provides smart auto-completion and colorized output, among other things.</p>
<p>We highly recommend you install <a class="reference external" href="https://ipython.org/">IPython</a>, specially if you’re working on
Unix systems (where <a class="reference external" href="https://ipython.org/">IPython</a> excels). See the <a class="reference external" href="https://ipython.org/install.html">IPython installation guide</a>
for more info.</p>
<p>Scrapy also has support for <a class="reference external" href="https://bpython-interpreter.org/">bpython</a>, and will try to use it where <a class="reference external" href="https://ipython.org/">IPython</a>
is unavailable.</p>
<p>Through Scrapy’s settings you can configure it to use any one of
<code class="docutils literal notranslate"><span class="pre">ipython</span></code>, <code class="docutils literal notranslate"><span class="pre">bpython</span></code> or the standard <code class="docutils literal notranslate"><span class="pre">python</span></code> shell, regardless of which
are installed. This is done by setting the <code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> environment
variable; or by defining it in your <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-config-settings"><span class="std std-ref">scrapy.cfg</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">shell</span> <span class="o">=</span> <span class="n">bpython</span>
</pre></div>
</div>
</section>
<section id="launch-the-shell">
<h4>Launch the shell<a class="headerlink" href="#launch-the-shell" title="Permalink to this heading">¶</a></h4>
<p>To launch the Scrapy shell you can use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> command like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">&lt;url&gt;</span></code> is the URL you want to scrape.</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> also works for local files. This can be handy if you want
to play around with a local copy of a web page. <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> understands
the following syntaxes for local files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNIX-style</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">./</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">../</span><span class="n">other</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">/</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>

<span class="c1"># File URI</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="n">file</span><span class="p">:</span><span class="o">///</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using relative file paths, be explicit and prepend them
with <code class="docutils literal notranslate"><span class="pre">./</span></code> (or <code class="docutils literal notranslate"><span class="pre">../</span></code> when relevant).
<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> will not work as one might expect (and
this is by design, not a bug).</p>
<p>Because <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> favors HTTP URLs over File URIs,
and <code class="docutils literal notranslate"><span class="pre">index.html</span></code> being syntactically similar to <code class="docutils literal notranslate"><span class="pre">example.com</span></code>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> will treat <code class="docutils literal notranslate"><span class="pre">index.html</span></code> as a domain name and trigger
a DNS lookup error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell index.html
[ ... scrapy shell starts ... ]
[ ... traceback ... ]
twisted.internet.error.DNSLookupError: DNS lookup failed:
address &#39;index.html&#39; not found: [Errno -5] No address associated with hostname.
</pre></div>
</div>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> will not test beforehand if a file called <code class="docutils literal notranslate"><span class="pre">index.html</span></code>
exists in the current directory. Again, be explicit.</p>
</div>
</section>
<section id="using-the-shell">
<h4>Using the shell<a class="headerlink" href="#using-the-shell" title="Permalink to this heading">¶</a></h4>
<p>The Scrapy shell is just a regular Python console (or <a class="reference external" href="https://ipython.org/">IPython</a> console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<section id="available-shortcuts">
<h5>Available Shortcuts<a class="headerlink" href="#available-shortcuts" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">shelp()</span></code> - print a help with the list of available objects and
shortcuts</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fetch(url[,</span> <span class="pre">redirect=True])</span></code> - fetch a new response from the given URL
and update all related objects accordingly. You can optionally ask for HTTP
3xx redirections to not be followed by passing <code class="docutils literal notranslate"><span class="pre">redirect=False</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fetch(request)</span></code> - fetch a new response from the given request and update
all related objects accordingly.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">view(response)</span></code> - open the given response in your local web browser, for
inspection. This will add a <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt; tag</a> to the response body in order
for external links (such as images and style sheets) to display properly.
Note, however, that this will create a temporary file in your computer,
which won’t be removed automatically.</p></li>
</ul>
</section>
<section id="available-scrapy-objects">
<h5>Available Scrapy objects<a class="headerlink" href="#available-scrapy-objects" title="Permalink to this heading">¶</a></h5>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object and the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> objects (for both HTML and XML
content).</p>
<p>Those objects are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">crawler</span></code> - the current <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spider</span></code> - the Spider which is known to handle the URL, or a
<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object if there is no spider found for the
current URL</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">request</span></code> - a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object of the last fetched
page. You can modify this request using
<code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code> or fetch a new request (without
leaving the shell) using the <code class="docutils literal notranslate"><span class="pre">fetch</span></code> shortcut.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response</span></code> - a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object containing the last
fetched page</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">settings</span></code> - the current <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Scrapy settings</span></a></p></li>
</ul>
</section>
</section>
<section id="example-of-shell-session">
<h4>Example of shell session<a class="headerlink" href="#example-of-shell-session" title="Permalink to this heading">¶</a></h4>
<p>Here’s an example of a typical shell session where we start by scraping the
<a class="reference external" href="https://scrapy.org">https://scrapy.org</a> page, and then proceed to scrape the <a class="reference external" href="https://old.reddit.com/">https://old.reddit.com/</a>
page. Finally, we modify the (Reddit) request method to POST and re-fetch it
getting an error. We end the session by typing Ctrl-D (in Unix systems) or
Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.</p>
<p>First, we launch the shell:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s1">&#39;https://scrapy.org&#39;</span> <span class="o">--</span><span class="n">nolog</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember to always enclose URLs in quotes when running the Scrapy shell from
the command line, otherwise URLs containing arguments (i.e. the <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> character)
will not work.</p>
<p>On Windows, use double quotes instead:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s2">&quot;https://scrapy.org&quot;</span> <span class="o">--</span><span class="n">nolog</span>
</pre></div>
</div>
</div>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you’ll notice that these lines
all start with the <code class="docutils literal notranslate"><span class="pre">[s]</span></code> prefix):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">scrapy</span>     <span class="n">scrapy</span> <span class="n">module</span> <span class="p">(</span><span class="n">contains</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">,</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">,</span> <span class="n">etc</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f07395dd690</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">item</span>       <span class="p">{}</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">request</span>    <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">response</span>   <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">settings</span>   <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">Settings</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f07395dd710</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">spider</span>     <span class="o">&lt;</span><span class="n">DefaultSpider</span> <span class="s1">&#39;default&#39;</span> <span class="n">at</span> <span class="mh">0x7f0735891690</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Useful</span> <span class="n">shortcuts</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">url</span><span class="p">[,</span> <span class="n">redirect</span><span class="o">=</span><span class="kc">True</span><span class="p">])</span> <span class="n">Fetch</span> <span class="n">URL</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span> <span class="p">(</span><span class="n">by</span> <span class="n">default</span><span class="p">,</span> <span class="n">redirects</span> <span class="n">are</span> <span class="n">followed</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>                  <span class="n">Fetch</span> <span class="n">a</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">shelp</span><span class="p">()</span>           <span class="n">Shell</span> <span class="n">help</span> <span class="p">(</span><span class="nb">print</span> <span class="n">this</span> <span class="n">help</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>    <span class="n">View</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">browser</span>

<span class="o">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>After that, we can start playing with the objects:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Scrapy | A Fast and Powerful Scraping and Web Crawling Framework&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="s2">&quot;https://old.reddit.com/&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//title/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;reddit: the front page of the internet&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;POST&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">status</span>
<span class="go">404</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">)</span>
<span class="go">{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],</span>
<span class="go">&#39;Cache-Control&#39;: [&#39;max-age=0, must-revalidate&#39;],</span>
<span class="go">&#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],</span>
<span class="go">&#39;Date&#39;: [&#39;Thu, 08 Dec 2016 16:21:19 GMT&#39;],</span>
<span class="go">&#39;Server&#39;: [&#39;snooserv&#39;],</span>
<span class="go">&#39;Set-Cookie&#39;: [&#39;loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;],</span>
<span class="go">&#39;Vary&#39;: [&#39;accept-encoding&#39;],</span>
<span class="go">&#39;Via&#39;: [&#39;1.1 varnish&#39;],</span>
<span class="go">&#39;X-Cache&#39;: [&#39;MISS&#39;],</span>
<span class="go">&#39;X-Cache-Hits&#39;: [&#39;0&#39;],</span>
<span class="go">&#39;X-Content-Type-Options&#39;: [&#39;nosniff&#39;],</span>
<span class="go">&#39;X-Frame-Options&#39;: [&#39;SAMEORIGIN&#39;],</span>
<span class="go">&#39;X-Moose&#39;: [&#39;majestic&#39;],</span>
<span class="go">&#39;X-Served-By&#39;: [&#39;cache-cdg8730-CDG&#39;],</span>
<span class="go">&#39;X-Timer&#39;: [&#39;S1481214079.394283,VS0,VE159&#39;],</span>
<span class="go">&#39;X-Ua-Compatible&#39;: [&#39;IE=edge&#39;],</span>
<span class="go">&#39;X-Xss-Protection&#39;: [&#39;1; mode=block&#39;]}</span>
</pre></div>
</div>
</section>
<section id="invoking-the-shell-from-spiders-to-inspect-responses">
<span id="topics-shell-inspect-response"></span><h4>Invoking the shell from spiders to inspect responses<a class="headerlink" href="#invoking-the-shell-from-spiders-to-inspect-responses" title="Permalink to this heading">¶</a></h4>
<p>Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.</p>
<p>This can be achieved by using the <code class="docutils literal notranslate"><span class="pre">scrapy.shell.inspect_response</span></code> function.</p>
<p>Here’s an example of how you would call it from your spider:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://example.com&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.org&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.net&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># We want to inspect one specific response.</span>
        <span class="k">if</span> <span class="s2">&quot;.org&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="kn">import</span> <span class="n">inspect_response</span>

            <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Rest of parsing code.</span>
</pre></div>
</div>
<p>When you run the spider, you will get something similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2014</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">23</span> <span class="mi">17</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mi">31</span><span class="o">-</span><span class="mi">0400</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2014</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">23</span> <span class="mi">17</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mi">31</span><span class="o">-</span><span class="mi">0400</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">org</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x1e16b50</span><span class="o">&gt;</span>
<span class="o">...</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
<span class="s1">&#39;http://example.org&#39;</span>
</pre></div>
</div>
<p>Then, you can check if the extraction code is working:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h1[@class=&quot;fn&quot;]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>Nope, it doesn’t. So you can open the response in your web browser and see if
it’s the response you were expecting:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the
crawling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="o">^</span><span class="n">D</span>
<span class="go">2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)</span>
<span class="go">...</span>
</pre></div>
</div>
<p>Note that you can’t use the <code class="docutils literal notranslate"><span class="pre">fetch</span></code> shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.</p>
</section>
</section>
<span id="document-topics/item-pipeline"></span><section id="item-pipeline">
<span id="topics-item-pipeline"></span><h3>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this heading">¶</a></h3>
<p>After an item has been scraped by a spider, it is sent to the Item Pipeline
which processes it through several components that are executed sequentially.</p>
<p>Each item pipeline component (sometimes referred as just “Item Pipeline”) is a
Python class that implements a simple method. They receive an item and perform
an action over it, also deciding if the item should continue through the
pipeline or be dropped and no longer processed.</p>
<p>Typical uses of item pipelines are:</p>
<ul class="simple">
<li><p>cleansing HTML data</p></li>
<li><p>validating scraped data (checking that the items contain certain fields)</p></li>
<li><p>checking for duplicates (and dropping them)</p></li>
<li><p>storing the scraped item in a database</p></li>
</ul>
<section id="writing-your-own-item-pipeline">
<h4>Writing your own item pipeline<a class="headerlink" href="#writing-your-own-item-pipeline" title="Permalink to this heading">¶</a></h4>
<p>Each item pipeline component is a Python class that must implement the following method:</p>
<dl class="py method">
<dt class="sig sig-object py" id="process_item">
<span class="sig-name descname"><span class="pre">process_item</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#process_item" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for every item pipeline component.</p>
<p><cite>item</cite> is an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>, see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#supporting-item-types"><span class="std std-ref">Supporting All Item Types</span></a>.</p>
<p><a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_item()</span></code></a> must either: return an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>,
return a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> or raise a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception.</p>
<p>Dropped items are no longer processed by further pipeline components.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item</strong> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>) – the scraped item</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>Additionally, they may also implement the following methods:</p>
<dl class="py method">
<dt class="sig sig-object py" id="open_spider">
<span class="sig-name descname"><span class="pre">open_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is opened.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which was opened</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="close_spider">
<span class="sig-name descname"><span class="pre">close_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which was closed</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="from_crawler">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this class method is called to create a pipeline instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the pipeline. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for pipeline to
access them and hook its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this pipeline</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="item-pipeline-example">
<h4>Item pipeline example<a class="headerlink" href="#item-pipeline-example" title="Permalink to this heading">¶</a></h4>
<section id="price-validation-and-dropping-items-with-no-prices">
<h5>Price validation and dropping items with no prices<a class="headerlink" href="#price-validation-and-dropping-items-with-no-prices" title="Permalink to this heading">¶</a></h5>
<p>Let’s take a look at the following hypothetical pipeline that adjusts the
<code class="docutils literal notranslate"><span class="pre">price</span></code> attribute for those items that do not include VAT
(<code class="docutils literal notranslate"><span class="pre">price_excludes_vat</span></code> attribute), and drops those items which don’t
contain a price:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>


<span class="k">class</span> <span class="nc">PricePipeline</span><span class="p">:</span>
    <span class="n">vat_factor</span> <span class="o">=</span> <span class="mf">1.15</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">adapter</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;price&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">adapter</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;price_excludes_vat&quot;</span><span class="p">):</span>
                <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat_factor</span>
            <span class="k">return</span> <span class="n">item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Missing price&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="write-items-to-a-json-lines-file">
<h5>Write items to a JSON lines file<a class="headerlink" href="#write-items-to-a-json-lines-file" title="Permalink to this heading">¶</a></h5>
<p>The following pipeline stores all scraped items (from all spiders) into a
single <code class="docutils literal notranslate"><span class="pre">items.jsonl</span></code> file, containing one item per line serialized in JSON
format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>


<span class="k">class</span> <span class="nc">JsonWriterPipeline</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;items.jsonl&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">asdict</span><span class="p">())</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p>
</div>
</section>
<section id="write-items-to-mongodb">
<h5>Write items to MongoDB<a class="headerlink" href="#write-items-to-mongodb" title="Permalink to this heading">¶</a></h5>
<p>In this example we’ll write items to <a class="reference external" href="https://www.mongodb.com/">MongoDB</a> using <a class="reference external" href="https://pymongo.readthedocs.io/en/stable/">pymongo</a>.
MongoDB address and database name are specified in Scrapy settings;
MongoDB collection is named after item class.</p>
<p>The main point of this example is to show how to use <a class="reference internal" href="#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a>
method and how to clean up the resources properly.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymongo</span>
<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>


<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">:</span>
    <span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;scrapy_items&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;MONGO_URI&quot;</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;MONGO_DATABASE&quot;</span><span class="p">,</span> <span class="s2">&quot;items&quot;</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert_one</span><span class="p">(</span><span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">asdict</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</section>
<section id="take-screenshot-of-item">
<span id="screenshotpipeline"></span><h5>Take screenshot of item<a class="headerlink" href="#take-screenshot-of-item" title="Permalink to this heading">¶</a></h5>
<p>This example demonstrates how to use <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">coroutine syntax</span></a> in
the <a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_item()</span></code></a> method.</p>
<p>This item pipeline makes a request to a locally-running instance of <a class="reference external" href="https://splash.readthedocs.io/en/stable/">Splash</a> to
render a screenshot of the item URL. After the request response is downloaded,
the item pipeline saves the screenshot to a file and adds the filename to the
item.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">quote</span>

<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.http.request</span> <span class="kn">import</span> <span class="n">NO_CALLBACK</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.defer</span> <span class="kn">import</span> <span class="n">maybe_deferred_to_future</span>


<span class="k">class</span> <span class="nc">ScreenshotPipeline</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span>
<span class="sd">    every Scrapy item.&quot;&quot;&quot;</span>

    <span class="n">SPLASH_URL</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8050/render.png?url=</span><span class="si">{}</span><span class="s2">&quot;</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">encoded_item_url</span> <span class="o">=</span> <span class="n">quote</span><span class="p">(</span><span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;url&quot;</span><span class="p">])</span>
        <span class="n">screenshot_url</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPLASH_URL</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoded_item_url</span><span class="p">)</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">screenshot_url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">NO_CALLBACK</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">maybe_deferred_to_future</span><span class="p">(</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
            <span class="c1"># Error happened, return item.</span>
            <span class="k">return</span> <span class="n">item</span>

        <span class="c1"># Save screenshot to file, filename will be hash of url.</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;url&quot;</span><span class="p">]</span>
        <span class="n">url_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">url</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf8&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">url_hash</span><span class="si">}</span><span class="s2">.png&quot;</span>
        <span class="n">Path</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">write_bytes</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>

        <span class="c1"># Store filename in item.</span>
        <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;screenshot_filename&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">filename</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</section>
<section id="duplicates-filter">
<h5>Duplicates filter<a class="headerlink" href="#duplicates-filter" title="Permalink to this heading">¶</a></h5>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let’s say that our items have a unique id, but our spider
returns multiples items with the same id:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>


<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Item ID already seen: </span><span class="si">{</span><span class="n">adapter</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</section>
</section>
<section id="activating-an-item-pipeline-component">
<h4>Activating an Item Pipeline component<a class="headerlink" href="#activating-an-item-pipeline-component" title="Permalink to this heading">¶</a></h4>
<p>To activate an Item Pipeline component you must add its class to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting, like in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;myproject.pipelines.PricePipeline&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s2">&quot;myproject.pipelines.JsonWriterPipeline&quot;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It’s customary to define these numbers in the 0-1000 range.</p>
</section>
</section>
<span id="document-topics/feed-exports"></span><section id="feed-exports">
<span id="topics-feed-exports"></span><h3>Feed exports<a class="headerlink" href="#feed-exports" title="Permalink to this heading">¶</a></h3>
<p>One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating an “export file” with the scraped data (commonly called “export
feed”) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate feeds with the scraped items, using multiple
serialization formats and storage backends.</p>
<p>This page provides detailed documentation for all feed export features. If you
are looking for a step-by-step guide, check out <a class="reference external" href="https://docs.zyte.com/web-scraping/guides/export/index.html#exporting-scraped-data">Zyte’s export guides</a>.</p>
<section id="serialization-formats">
<span id="topics-feed-format"></span><h4>Serialization formats<a class="headerlink" href="#serialization-formats" title="Permalink to this heading">¶</a></h4>
<p>For serializing the scraped data, the feed exports use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">Item exporters</span></a>. These formats are supported out of the box:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-format-json"><span class="std std-ref">JSON</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-format-jsonlines"><span class="std std-ref">JSON lines</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-format-csv"><span class="std std-ref">CSV</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-format-xml"><span class="std std-ref">XML</span></a></p></li>
</ul>
<p>But you can also extend the supported format through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a> setting.</p>
<section id="json">
<span id="topics-feed-format-json"></span><h5>JSON<a class="headerlink" href="#json" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">json</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a></p></li>
<li><p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#json-with-large-data"><span class="std std-ref">this warning</span></a> if you’re using JSON with
large feeds.</p></li>
</ul>
</section>
<section id="json-lines">
<span id="topics-feed-format-jsonlines"></span><h5>JSON lines<a class="headerlink" href="#json-lines" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">jsonlines</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonLinesItemExporter</span></code></a></p></li>
</ul>
</section>
<section id="csv">
<span id="topics-feed-format-csv"></span><h5>CSV<a class="headerlink" href="#csv" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">csv</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a></p></li>
<li><p>To specify columns to export, their order and their column names, use
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a>. Other feed exporters can also use this
option, but it is important for CSV because unlike many other export
formats CSV uses a fixed header.</p></li>
</ul>
</section>
<section id="xml">
<span id="topics-feed-format-xml"></span><h5>XML<a class="headerlink" href="#xml" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">xml</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a></p></li>
</ul>
</section>
<section id="pickle">
<span id="topics-feed-format-pickle"></span><h5>Pickle<a class="headerlink" href="#pickle" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">pickle</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.PickleItemExporter" title="scrapy.exporters.PickleItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PickleItemExporter</span></code></a></p></li>
</ul>
</section>
<section id="marshal">
<span id="topics-feed-format-marshal"></span><h5>Marshal<a class="headerlink" href="#marshal" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Value for the <code class="docutils literal notranslate"><span class="pre">format</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting: <code class="docutils literal notranslate"><span class="pre">marshal</span></code></p></li>
<li><p>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.MarshalItemExporter" title="scrapy.exporters.MarshalItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalItemExporter</span></code></a></p></li>
</ul>
</section>
</section>
<section id="storages">
<span id="topics-feed-storage"></span><h4>Storages<a class="headerlink" href="#storages" title="Permalink to this heading">¶</a></h4>
<p>When using the feed exports you define where to store the feed using one or multiple <a class="reference external" href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URIs</a>
(through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-fs"><span class="std std-ref">Local filesystem</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-ftp"><span class="std std-ref">FTP</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-s3"><span class="std std-ref">S3</span></a> (requires <a class="reference external" href="https://github.com/boto/boto3">boto3</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-gcs"><span class="std std-ref">Google Cloud Storage (GCS)</span></a> (requires <a class="reference external" href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-stdout"><span class="std std-ref">Standard output</span></a></p></li>
</ul>
<p>Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the <a class="reference external" href="https://github.com/boto/boto3">boto3</a>
library is installed.</p>
</section>
<section id="storage-uri-parameters">
<span id="topics-feed-uri-params"></span><h4>Storage URI parameters<a class="headerlink" href="#storage-uri-parameters" title="Permalink to this heading">¶</a></h4>
<p>The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">%(time)s</span></code> - gets replaced by a timestamp when the feed is being created</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">%(name)s</span></code> - gets replaced by the spider name</p></li>
</ul>
<p>Any other named parameter gets replaced by the spider attribute of the same
name. For example, <code class="docutils literal notranslate"><span class="pre">%(site_id)s</span></code> would get replaced by the <code class="docutils literal notranslate"><span class="pre">spider.site_id</span></code>
attribute the moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<ul class="simple">
<li><p>Store in FTP using one directory per spider:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ftp://user:password&#64;ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</span></code></p></li>
</ul>
</li>
<li><p>Store in S3 using one directory per spider:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</span></code></p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">Spider arguments</span></a> become spider attributes, hence
they can also be used as storage URI parameters.</p>
</div>
</section>
<section id="storage-backends">
<span id="topics-feed-storage-backends"></span><h4>Storage backends<a class="headerlink" href="#storage-backends" title="Permalink to this heading">¶</a></h4>
<section id="local-filesystem">
<span id="topics-feed-storage-fs"></span><h5>Local filesystem<a class="headerlink" href="#local-filesystem" title="Permalink to this heading">¶</a></h5>
<p>The feeds are stored in the local filesystem.</p>
<ul class="simple">
<li><p>URI scheme: <code class="docutils literal notranslate"><span class="pre">file</span></code></p></li>
<li><p>Example URI: <code class="docutils literal notranslate"><span class="pre">file:///tmp/export.csv</span></code></p></li>
<li><p>Required external libraries: none</p></li>
</ul>
<p>Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like <code class="docutils literal notranslate"><span class="pre">/tmp/export.csv</span></code> (Unix systems only).
Alternatively you can also use a <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> object.</p>
</section>
<section id="ftp">
<span id="topics-feed-storage-ftp"></span><h5>FTP<a class="headerlink" href="#ftp" title="Permalink to this heading">¶</a></h5>
<p>The feeds are stored in a FTP server.</p>
<ul class="simple">
<li><p>URI scheme: <code class="docutils literal notranslate"><span class="pre">ftp</span></code></p></li>
<li><p>Example URI: <code class="docutils literal notranslate"><span class="pre">ftp://user:pass&#64;ftp.example.com/path/to/export.csv</span></code></p></li>
<li><p>Required external libraries: none</p></li>
</ul>
<p>FTP supports two different connection modes: <a class="reference external" href="https://stackoverflow.com/a/1699163">active or passive</a>. Scrapy uses the passive connection
mode by default. To use the active connection mode instead, set the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>The default value for the <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> for this
storage backend is: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> will cause you to lose the
previous version of your data.</p>
</div>
<p>This storage backend uses <a class="hxr-hoverxref hxr-tooltip reference internal" href="#delayed-file-delivery"><span class="std std-ref">delayed file delivery</span></a>.</p>
</section>
<section id="s3">
<span id="topics-feed-storage-s3"></span><h5>S3<a class="headerlink" href="#s3" title="Permalink to this heading">¶</a></h5>
<p>The feeds are stored on <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<ul class="simple">
<li><p>URI scheme: <code class="docutils literal notranslate"><span class="pre">s3</span></code></p></li>
<li><p>Example URIs:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">s3://mybucket/path/to/export.csv</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">s3://aws_key:aws_secret&#64;mybucket/path/to/export.csv</span></code></p></li>
</ul>
</li>
<li><p>Required external libraries: <a class="reference external" href="https://github.com/boto/boto3">boto3</a> &gt;= 1.20.0</p></li>
</ul>
<p>The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_ACCESS_KEY_ID"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ACCESS_KEY_ID</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_SECRET_ACCESS_KEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_SECRET_ACCESS_KEY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_SESSION_TOKEN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_SESSION_TOKEN</span></code></a> (only needed for <a class="reference external" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html">temporary security credentials</a>)</p></li>
</ul>
<p>You can also define a custom ACL, custom endpoint, and region name for exported
feeds using these settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a></p></li>
</ul>
<p>The default value for the <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> for this
storage backend is: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> will cause you to lose the
previous version of your data.</p>
</div>
<p>This storage backend uses <a class="hxr-hoverxref hxr-tooltip reference internal" href="#delayed-file-delivery"><span class="std std-ref">delayed file delivery</span></a>.</p>
</section>
<section id="google-cloud-storage-gcs">
<span id="topics-feed-storage-gcs"></span><h5>Google Cloud Storage (GCS)<a class="headerlink" href="#google-cloud-storage-gcs" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.</span></p>
</div>
<p>The feeds are stored on <a class="reference external" href="https://cloud.google.com/storage/">Google Cloud Storage</a>.</p>
<ul class="simple">
<li><p>URI scheme: <code class="docutils literal notranslate"><span class="pre">gs</span></code></p></li>
<li><p>Example URIs:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gs://mybucket/path/to/export.csv</span></code></p></li>
</ul>
</li>
<li><p>Required external libraries: <a class="reference external" href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a>.</p></li>
</ul>
<p>For more information about authentication, please refer to <a class="reference external" href="https://cloud.google.com/docs/authentication">Google Cloud documentation</a>.</p>
<p>You can set a <em>Project ID</em> and <em>Access Control List (ACL)</em> through the following settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORAGE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_GCS_ACL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-GCS_PROJECT_ID"><code class="xref std std-setting docutils literal notranslate"><span class="pre">GCS_PROJECT_ID</span></code></a></p></li>
</ul>
<p>The default value for the <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> for this
storage backend is: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> will cause you to lose the
previous version of your data.</p>
</div>
<p>This storage backend uses <a class="hxr-hoverxref hxr-tooltip reference internal" href="#delayed-file-delivery"><span class="std std-ref">delayed file delivery</span></a>.</p>
</section>
<section id="standard-output">
<span id="topics-feed-storage-stdout"></span><h5>Standard output<a class="headerlink" href="#standard-output" title="Permalink to this heading">¶</a></h5>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<ul class="simple">
<li><p>URI scheme: <code class="docutils literal notranslate"><span class="pre">stdout</span></code></p></li>
<li><p>Example URI: <code class="docutils literal notranslate"><span class="pre">stdout:</span></code></p></li>
<li><p>Required external libraries: none</p></li>
</ul>
</section>
<section id="delayed-file-delivery">
<span id="id1"></span><h5>Delayed file delivery<a class="headerlink" href="#delayed-file-delivery" title="Permalink to this heading">¶</a></h5>
<p>As indicated above, some of the described storage backends use delayed file
delivery.</p>
<p>These storage backends do not upload items to the feed URI as those items are
scraped. Instead, Scrapy writes items into a temporary local file, and only
once all the file contents have been written (i.e. at the end of the crawl) is
that file uploaded to the feed URI.</p>
<p>If you want item delivery to start earlier when using one of these storage
backends, use <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> to split the output items
in multiple files, with the specified maximum item count per file. That way, as
soon as a file reaches the maximum item count, that file is delivered to the
feed URI, allowing item delivery to start way before the end of the crawl.</p>
</section>
</section>
<section id="item-filtering">
<span id="item-filter"></span><h4>Item filtering<a class="headerlink" href="#item-filtering" title="Permalink to this heading">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
<p>You can filter items that you want to allow for a particular feed by using the
<code class="docutils literal notranslate"><span class="pre">item_classes</span></code> option in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">feeds options</span></a>. Only items of
the specified types will be added to the feed.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">item_classes</span></code> option is implemented by the <a class="reference internal" href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemFilter</span></code></a>
class, which is the default value of the <code class="docutils literal notranslate"><span class="pre">item_filter</span></code> <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">feed option</span></a>.</p>
<p>You can create your own custom filtering class by implementing <a class="reference internal" href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemFilter</span></code></a>’s
method <code class="docutils literal notranslate"><span class="pre">accepts</span></code> and taking <code class="docutils literal notranslate"><span class="pre">feed_options</span></code> as an argument.</p>
<p>For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyCustomFilter</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feed_options</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_options</span> <span class="o">=</span> <span class="n">feed_options</span>

    <span class="k">def</span> <span class="nf">accepts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;field1&quot;</span> <span class="ow">in</span> <span class="n">item</span> <span class="ow">and</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;field1&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;expected_data&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="kc">False</span>
</pre></div>
</div>
<p>You can assign your custom filtering class to the <code class="docutils literal notranslate"><span class="pre">item_filter</span></code> <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">option of a feed</span></a>.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> for examples.</p>
<section id="itemfilter">
<h5>ItemFilter<a class="headerlink" href="#itemfilter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.feedexport.ItemFilter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.feedexport.</span></span><span class="sig-name descname"><span class="pre">ItemFilter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">feed_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.feedexport.ItemFilter" title="Permalink to this definition">¶</a></dt>
<dd><p>This will be used by FeedExporter to decide if an item should be allowed
to be exported to a particular feed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>feed_options</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – feed specific options passed from FeedExporter</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.feedexport.ItemFilter.accepts">
<span class="sig-name descname"><span class="pre">accepts</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.extensions.feedexport.ItemFilter.accepts" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if <cite>item</cite> should be exported or <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>item</strong> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">Scrapy items</span></a>) – scraped item which user wants to check if is acceptable</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><cite>True</cite> if accepted, <cite>False</cite> otherwise</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">bool</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="post-processing">
<span id="id2"></span><h4>Post-Processing<a class="headerlink" href="#post-processing" title="Permalink to this heading">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
<p>Scrapy provides an option to activate plugins to post-process feeds before they are exported
to feed storages. In addition to using <a class="hxr-hoverxref hxr-tooltip reference internal" href="#builtin-plugins"><span class="std std-ref">builtin plugins</span></a>, you
can create your own <a class="hxr-hoverxref hxr-tooltip reference internal" href="#custom-plugins"><span class="std std-ref">plugins</span></a>.</p>
<p>These plugins can be activated through the <code class="docutils literal notranslate"><span class="pre">postprocessing</span></code> option of a feed.
The option must be passed a list of post-processing plugins in the order you want
the feed to be processed. These plugins can be declared either as an import string
or with the imported class of the plugin. Parameters to plugins can be passed
through the feed options. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">feed options</span></a> for examples.</p>
<section id="built-in-plugins">
<span id="builtin-plugins"></span><h5>Built-in Plugins<a class="headerlink" href="#built-in-plugins" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.postprocessing.GzipPlugin">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.postprocessing.</span></span><span class="sig-name descname"><span class="pre">GzipPlugin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.13)"><span class="pre">BinaryIO</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.postprocessing.GzipPlugin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compresses received data using <a class="reference external" href="https://en.wikipedia.org/wiki/Gzip">gzip</a>.</p>
<p>Accepted <code class="docutils literal notranslate"><span class="pre">feed_options</span></code> parameters:</p>
<ul class="simple">
<li><p><cite>gzip_compresslevel</cite></p></li>
<li><p><cite>gzip_mtime</cite></p></li>
<li><p><cite>gzip_filename</cite></p></li>
</ul>
<p>See <a class="reference external" href="https://docs.python.org/3/library/gzip.html#gzip.GzipFile" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">gzip.GzipFile</span></code></a> for more info about parameters.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.postprocessing.LZMAPlugin">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.postprocessing.</span></span><span class="sig-name descname"><span class="pre">LZMAPlugin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.13)"><span class="pre">BinaryIO</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.postprocessing.LZMAPlugin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compresses received data using <a class="reference external" href="https://en.wikipedia.org/wiki/Lempel–Ziv–Markov_chain_algorithm">lzma</a>.</p>
<p>Accepted <code class="docutils literal notranslate"><span class="pre">feed_options</span></code> parameters:</p>
<ul class="simple">
<li><p><cite>lzma_format</cite></p></li>
<li><p><cite>lzma_check</cite></p></li>
<li><p><cite>lzma_preset</cite></p></li>
<li><p><cite>lzma_filters</cite></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">lzma_filters</span></code> cannot be used in pypy version 7.3.1 and older.</p>
</div>
<p>See <a class="reference external" href="https://docs.python.org/3/library/lzma.html#lzma.LZMAFile" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">lzma.LZMAFile</span></code></a> for more info about parameters.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.postprocessing.Bz2Plugin">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.postprocessing.</span></span><span class="sig-name descname"><span class="pre">Bz2Plugin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.13)"><span class="pre">BinaryIO</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.postprocessing.Bz2Plugin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compresses received data using <a class="reference external" href="https://en.wikipedia.org/wiki/Bzip2">bz2</a>.</p>
<p>Accepted <code class="docutils literal notranslate"><span class="pre">feed_options</span></code> parameters:</p>
<ul class="simple">
<li><p><cite>bz2_compresslevel</cite></p></li>
</ul>
<p>See <a class="reference external" href="https://docs.python.org/3/library/bz2.html#bz2.BZ2File" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bz2.BZ2File</span></code></a> for more info about parameters.</p>
</dd></dl>

</section>
<section id="custom-plugins">
<span id="id3"></span><h5>Custom Plugins<a class="headerlink" href="#custom-plugins" title="Permalink to this heading">¶</a></h5>
<p>Each plugin is a class that must implement the following methods:</p>
<dl class="py method">
<dt class="sig sig-object py" id="init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_options</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the plugin.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file</strong> – file-like object having at least the <cite>write</cite>, <cite>tell</cite> and <cite>close</cite> methods implemented</p></li>
<li><p><strong>feed_options</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>) – feed-specific <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">options</span></a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="write">
<span class="sig-name descname"><span class="pre">write</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#write" title="Permalink to this definition">¶</a></dt>
<dd><p>Process and write <cite>data</cite> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes</span></code></a> or <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#memoryview" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">memoryview</span></code></a>) into the plugin’s target file.
It must return number of bytes written.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="close">
<span class="sig-name descname"><span class="pre">close</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#close" title="Permalink to this definition">¶</a></dt>
<dd><p>Clean up the plugin.</p>
<p>For example, you might want to close a file wrapper that you might have
used to compress data written into the file received in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code>
method.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not close the file from the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</div>
</dd></dl>

<p>To pass a parameter to your plugin, use <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">feed options</span></a>. You
can then access those parameters from the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of your plugin.</p>
</section>
</section>
<section id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this heading">¶</a></h4>
<p>These are the settings used for configuring the feed exports:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> (mandatory)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a></p></li>
</ul>
<section id="feeds">
<span id="std-setting-FEEDS"></span><h5>FEEDS<a class="headerlink" href="#feeds" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dictionary in which every key is a feed URI (or a <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a>
object) and each value is a nested dictionary containing configuration
parameters for the specific feed.</p>
<p>This setting is required for enabling the feed export feature.</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-backends"><span class="std std-ref">Storage backends</span></a> for supported URI schemes.</p>
<p>For instance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;items.json&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;format&#39;</span><span class="p">:</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span>
        <span class="s1">&#39;encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;utf8&#39;</span><span class="p">,</span>
        <span class="s1">&#39;store_empty&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s1">&#39;item_classes&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">MyItemClass1</span><span class="p">,</span> <span class="s1">&#39;myproject.items.MyItemClass2&#39;</span><span class="p">],</span>
        <span class="s1">&#39;fields&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s1">&#39;indent&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="s1">&#39;item_export_kwargs&#39;</span><span class="p">:</span> <span class="p">{</span>
           <span class="s1">&#39;export_empty_fields&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s1">&#39;/home/user/documents/items.xml&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;format&#39;</span><span class="p">:</span> <span class="s1">&#39;xml&#39;</span><span class="p">,</span>
        <span class="s1">&#39;fields&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">],</span>
        <span class="s1">&#39;item_filter&#39;</span><span class="p">:</span> <span class="n">MyCustomFilter1</span><span class="p">,</span>
        <span class="s1">&#39;encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;latin1&#39;</span><span class="p">,</span>
        <span class="s1">&#39;indent&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s1">&#39;items.csv.gz&#39;</span><span class="p">):</span> <span class="p">{</span>
        <span class="s1">&#39;format&#39;</span><span class="p">:</span> <span class="s1">&#39;csv&#39;</span><span class="p">,</span>
        <span class="s1">&#39;fields&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">],</span>
        <span class="s1">&#39;item_filter&#39;</span><span class="p">:</span> <span class="s1">&#39;myproject.filters.MyCustomFilter2&#39;</span><span class="p">,</span>
        <span class="s1">&#39;postprocessing&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">MyPlugin1</span><span class="p">,</span> <span class="s1">&#39;scrapy.extensions.postprocessing.GzipPlugin&#39;</span><span class="p">],</span>
        <span class="s1">&#39;gzip_compresslevel&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<p id="feed-options">The following is a list of the accepted keys and the setting that is used
as a fallback value if that key is not provided for a specific feed definition:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">format</span></code>: the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-format"><span class="std std-ref">serialization format</span></a>.</p>
<p>This setting is mandatory, there is no fallback value.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_item_count</span></code>: falls back to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">encoding</span></code>: falls back to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fields</span></code>: falls back to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">item_classes</span></code>: list of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item classes</span></a> to export.</p>
<p>If undefined or empty, all items are exported.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">item_filter</span></code>: a <a class="hxr-hoverxref hxr-tooltip reference internal" href="#item-filter"><span class="std std-ref">filter class</span></a> to filter items to export.</p>
<p><a class="reference internal" href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemFilter</span></code></a> is used be default.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">indent</span></code>: falls back to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">item_export_kwargs</span></code>: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> with keyword arguments for the corresponding <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporter class</span></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">overwrite</span></code>: whether to overwrite the file if it already exists
(<code class="docutils literal notranslate"><span class="pre">True</span></code>) or append to its content (<code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
<p>The default value depends on the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-backends"><span class="std std-ref">storage backend</span></a>:</p>
<ul>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-fs"><span class="std std-ref">Local filesystem</span></a>: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-ftp"><span class="std std-ref">FTP</span></a>: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some FTP servers may not support appending to files (the
<code class="docutils literal notranslate"><span class="pre">APPE</span></code> FTP command).</p>
</div>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-s3"><span class="std std-ref">S3</span></a>: <code class="docutils literal notranslate"><span class="pre">True</span></code> (appending is not supported)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-gcs"><span class="std std-ref">Google Cloud Storage (GCS)</span></a>: <code class="docutils literal notranslate"><span class="pre">True</span></code> (appending is not supported)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-feed-storage-stdout"><span class="std std-ref">Standard output</span></a>: <code class="docutils literal notranslate"><span class="pre">False</span></code> (overwriting is not supported)</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">store_empty</span></code>: falls back to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uri_params</span></code>: falls back to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">postprocessing</span></code>: list of <a class="hxr-hoverxref hxr-tooltip reference internal" href="#post-processing"><span class="std std-ref">plugins</span></a> to use for post-processing.</p>
<p>The plugins will be used in the order of the list passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
</li>
</ul>
</section>
<section id="feed-export-encoding">
<span id="std-setting-FEED_EXPORT_ENCODING"></span><h5>FEED_EXPORT_ENCODING<a class="headerlink" href="#feed-export-encoding" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The encoding to be used for the feed.</p>
<p>If unset or set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (default) it uses UTF-8 for everything except JSON output,
which uses safe numeric encoding (<code class="docutils literal notranslate"><span class="pre">\uXXXX</span></code> sequences) for historic reasons.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">utf-8</span></code> if you want UTF-8 for JSON too.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.8: </span>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command now sets this setting to
<code class="docutils literal notranslate"><span class="pre">utf-8</span></code> in the generated <code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file.</p>
</div>
</section>
<section id="feed-export-fields">
<span id="std-setting-FEED_EXPORT_FIELDS"></span><h5>FEED_EXPORT_FIELDS<a class="headerlink" href="#feed-export-fields" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code> setting to define the fields to export, their
order and their output names. See <a class="reference internal" href="index.html#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BaseItemExporter.fields_to_export</span></code></a> for more information.</p>
</section>
<section id="feed-export-indent">
<span id="std-setting-FEED_EXPORT_INDENT"></span><h5>FEED_EXPORT_INDENT<a class="headerlink" href="#feed-export-indent" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Amount of spaces used to indent the output on each level. If <code class="docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code>
is a non-negative integer, then array elements and object members will be pretty-printed
with that indent level. An indent level of <code class="docutils literal notranslate"><span class="pre">0</span></code> (the default), or negative,
will put each item on a new line. <code class="docutils literal notranslate"><span class="pre">None</span></code> selects the most compact representation.</p>
<p>Currently implemented only by <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>
and <a class="reference internal" href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>, i.e. when you are exporting
to <code class="docutils literal notranslate"><span class="pre">.json</span></code> or <code class="docutils literal notranslate"><span class="pre">.xml</span></code>.</p>
</section>
<section id="feed-store-empty">
<span id="std-setting-FEED_STORE_EMPTY"></span><h5>FEED_STORE_EMPTY<a class="headerlink" href="#feed-store-empty" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to export empty feeds (i.e. feeds with no items).
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, and there are no items to export, no new files are created and
existing files are not modified, even if the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#feed-options"><span class="std std-ref">overwrite feed option</span></a> is enabled.</p>
</section>
<section id="feed-storages">
<span id="std-setting-FEED_STORAGES"></span><h5>FEED_STORAGES<a class="headerlink" href="#feed-storages" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.</p>
</section>
<section id="feed-storage-ftp-active">
<span id="std-setting-FEED_STORAGE_FTP_ACTIVE"></span><h5>FEED_STORAGE_FTP_ACTIVE<a class="headerlink" href="#feed-storage-ftp-active" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether to use the active connection mode when exporting feeds to an FTP server
(<code class="docutils literal notranslate"><span class="pre">True</span></code>) or use the passive connection mode instead (<code class="docutils literal notranslate"><span class="pre">False</span></code>, default).</p>
<p>For information about FTP connection modes, see <a class="reference external" href="https://stackoverflow.com/a/1699163">What is the difference between
active and passive FTP?</a>.</p>
</section>
<section id="feed-storage-s3-acl">
<span id="std-setting-FEED_STORAGE_S3_ACL"></span><h5>FEED_STORAGE_S3_ACL<a class="headerlink" href="#feed-storage-s3-acl" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string)</p>
<p>A string containing a custom ACL for feeds exported to Amazon S3 by your project.</p>
<p>For a complete list of available values, access the <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl">Canned ACL</a> section on Amazon S3 docs.</p>
</section>
<section id="feed-storages-base">
<span id="std-setting-FEED_STORAGES_BASE"></span><h5>FEED_STORAGES_BASE<a class="headerlink" href="#feed-storages-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.extensions.feedexport.FileFeedStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;file&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.extensions.feedexport.FileFeedStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;stdout&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.extensions.feedexport.StdoutFeedStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;s3&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.extensions.feedexport.S3FeedStorage&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ftp&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.extensions.feedexport.FTPFeedStorage&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed storage backends supported by Scrapy. You
can disable any of these backends by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their URI scheme in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_STORAGES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGES</span></code></a>. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FEED_STORAGES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;ftp&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="feed-exporters">
<span id="std-setting-FEED_EXPORTERS"></span><h5>FEED_EXPORTERS<a class="headerlink" href="#feed-exporters" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing additional exporters supported by your project. The keys are
serialization formats and the values are paths to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">Item exporter</span></a> classes.</p>
</section>
<section id="feed-exporters-base">
<span id="std-setting-FEED_EXPORTERS_BASE"></span><h5>FEED_EXPORTERS_BASE<a class="headerlink" href="#feed-exporters-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;json&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.JsonItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;jsonlines&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.JsonLinesItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;jsonl&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.JsonLinesItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;jl&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.JsonLinesItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;csv&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.CsvItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;xml&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.XmlItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;marshal&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.MarshalItemExporter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;pickle&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.exporters.PickleItemExporter&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed exporters supported by Scrapy. You can
disable any of these exporters by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their serialization
format in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a>. E.g., to disable the built-in CSV exporter
(without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FEED_EXPORTERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;csv&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="feed-export-batch-item-count">
<span id="std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"></span><h5>FEED_EXPORT_BATCH_ITEM_COUNT<a class="headerlink" href="#feed-export-batch-item-count" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>If assigned an integer number higher than <code class="docutils literal notranslate"><span class="pre">0</span></code>, Scrapy generates multiple output files
storing up to the specified number of items in each output file.</p>
<p>When generating multiple output files, you must use at least one of the following
placeholders in the feed URI to indicate how the different output file names are
generated:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">%(batch_time)s</span></code> - gets replaced by a timestamp when the feed is being created
(e.g. <code class="docutils literal notranslate"><span class="pre">2020-03-28T14-45-08.237134</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">%(batch_id)d</span></code> - gets replaced by the 1-based sequence number of the batch.</p>
<p>Use <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.13)"><span class="xref std std-ref">printf-style string formatting</span></a> to
alter the number format. For example, to make the batch ID a 5-digit
number by introducing leading zeroes as needed, use <code class="docutils literal notranslate"><span class="pre">%(batch_id)05d</span></code>
(e.g. <code class="docutils literal notranslate"><span class="pre">3</span></code> becomes <code class="docutils literal notranslate"><span class="pre">00003</span></code>, <code class="docutils literal notranslate"><span class="pre">123</span></code> becomes <code class="docutils literal notranslate"><span class="pre">00123</span></code>).</p>
</li>
</ul>
<p>For instance, if your settings include:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FEED_EXPORT_BATCH_ITEM_COUNT</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>And your <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> command line is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">spidername</span> <span class="o">-</span><span class="n">o</span> <span class="s2">&quot;dirname/</span><span class="si">%(batch_id)d</span><span class="s2">-filename</span><span class="si">%(batch_time)s</span><span class="s2">.json&quot;</span>
</pre></div>
</div>
<p>The command line above can generate a directory tree like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-&gt;</span><span class="n">projectname</span>
<span class="o">--&gt;</span><span class="n">dirname</span>
<span class="o">---&gt;</span><span class="mi">1</span><span class="o">-</span><span class="n">filename2020</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">28</span><span class="n">T14</span><span class="o">-</span><span class="mi">45</span><span class="o">-</span><span class="mf">08.237134</span><span class="o">.</span><span class="n">json</span>
<span class="o">---&gt;</span><span class="mi">2</span><span class="o">-</span><span class="n">filename2020</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">28</span><span class="n">T14</span><span class="o">-</span><span class="mi">45</span><span class="o">-</span><span class="mf">09.148903</span><span class="o">.</span><span class="n">json</span>
<span class="o">---&gt;</span><span class="mi">3</span><span class="o">-</span><span class="n">filename2020</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">28</span><span class="n">T14</span><span class="o">-</span><span class="mi">45</span><span class="o">-</span><span class="mf">10.046092</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Where the first and second files contain exactly 100 items. The last one contains
100 items or fewer.</p>
</section>
<section id="feed-uri-params">
<span id="std-setting-FEED_URI_PARAMS"></span><h5>FEED_URI_PARAMS<a class="headerlink" href="#feed-uri-params" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>A string with the import path of a function to set the parameters to apply with
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.13)"><span class="xref std std-ref">printf-style string formatting</span></a> to the
feed URI.</p>
<p>The function signature should be as follows:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.extensions.feedexport.uri_params">
<span class="sig-prename descclassname"><span class="pre">scrapy.extensions.feedexport.</span></span><span class="sig-name descname"><span class="pre">uri_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.feedexport.uri_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> of key-value pairs to apply to the feed URI using
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.13)"><span class="xref std std-ref">printf-style string formatting</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – <p>default key-value pairs</p>
<p>Specifically:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_id</span></code>: ID of the file batch. See
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a>.</p>
<p>If <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> is <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">batch_id</span></code>
is always <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_time</span></code>: UTC date and time, in ISO format with <code class="docutils literal notranslate"><span class="pre">:</span></code>
replaced with <code class="docutils literal notranslate"><span class="pre">-</span></code>.</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.3.0.</span></p>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">time</span></code>: <code class="docutils literal notranslate"><span class="pre">batch_time</span></code>, with microseconds set to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
</ul>
</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><em>scrapy.Spider</em></a>) – source spider of the feed items</p></li>
</ul>
</dd>
</dl>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The function should return a new dictionary, modifying
the received <code class="docutils literal notranslate"><span class="pre">params</span></code> in-place is deprecated.</p>
</div>
</dd></dl>

<p>For example, to include the <a class="reference internal" href="index.html#scrapy.Spider.name" title="scrapy.Spider.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code></a> of the
source spider in the feed URI:</p>
<ol class="arabic">
<li><p>Define the following function somewhere in your project:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># myproject/utils.py</span>
<span class="k">def</span> <span class="nf">uri_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="o">**</span><span class="n">params</span><span class="p">,</span> <span class="s2">&quot;spider_name&quot;</span><span class="p">:</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Point <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> to that function in your settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># myproject/settings.py</span>
<span class="n">FEED_URI_PARAMS</span> <span class="o">=</span> <span class="s2">&quot;myproject.utils.uri_params&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">%(spider_name)s</span></code> in your feed URI:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="o">&lt;</span><span class="n">spider_name</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">o</span> <span class="s2">&quot;</span><span class="si">%(spider_name)s</span><span class="s2">.jsonl&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
<span id="document-topics/request-response"></span><section id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h3>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="Permalink to this heading">¶</a></h3>
<p>Scrapy uses <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-request-subclasses"><span class="std std-ref">Request subclasses</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-response-subclasses"><span class="std std-ref">Response subclasses</span></a>.</p>
<section id="request-objects">
<h4>Request objects<a class="headerlink" href="#request-objects" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.Request">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">Request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request" title="Permalink to this definition">¶</a></dt>
<dd><p>Represents an HTTP request, which is usually generated in a Spider and
executed by the Downloader, thus generating a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – <p>the URL of this request</p>
<p>If the URL is invalid, a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code></a> exception is raised.</p>
</p></li>
<li><p><strong>callback</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>collections.abc.Callable</em></a>) – <p>the function that will be called with the response of this
request (once it’s downloaded) as its first parameter.</p>
<p>In addition to a function, the following values are supported:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (default), which indicates that the spider’s
<a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method must be used.</p></li>
<li><p><a class="reference internal" href="#scrapy.http.request.NO_CALLBACK" title="scrapy.http.request.NO_CALLBACK"><code class="xref py py-func docutils literal notranslate"><span class="pre">NO_CALLBACK()</span></code></a></p></li>
</ul>
<p>For more information, see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If exceptions are raised during processing, <code class="docutils literal notranslate"><span class="pre">errback</span></code> is
called instead.</p>
</div>
</p></li>
<li><p><strong>method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the HTTP method of this request. Defaults to <code class="docutils literal notranslate"><span class="pre">'GET'</span></code>.</p></li>
<li><p><strong>meta</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – the initial values for the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</p></li>
<li><p><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the request body. If a string is passed, then it’s encoded as
bytes using the <code class="docutils literal notranslate"><span class="pre">encoding</span></code> passed (which defaults to <code class="docutils literal notranslate"><span class="pre">utf-8</span></code>). If
<code class="docutils literal notranslate"><span class="pre">body</span></code> is not given, an empty bytes object is stored. Regardless of the
type of this argument, the final value stored will be a bytes object
(never a string or <code class="docutils literal notranslate"><span class="pre">None</span></code>).</p></li>
<li><p><strong>headers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – <p>the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<code class="docutils literal notranslate"><span class="pre">None</span></code> is passed as value, the HTTP header will not be sent at all.</p>
<blockquote>
<div><div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Cookies set via the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header are not considered by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code> parameter. This is a known
current limitation that is being worked on.</p>
</div>
</div></blockquote>
</p></li>
<li><p><strong>cookies</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic simple">
<li><p>Using a dict:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
    <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;currency&quot;</span><span class="p">:</span> <span class="s2">&quot;USD&quot;</span><span class="p">,</span> <span class="s2">&quot;country&quot;</span><span class="p">:</span> <span class="s2">&quot;UY&quot;</span><span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Using a list of dicts:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://www.example.com&quot;</span><span class="p">,</span>
    <span class="n">cookies</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;currency&quot;</span><span class="p">,</span>
            <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;USD&quot;</span><span class="p">,</span>
            <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="s2">&quot;example.com&quot;</span><span class="p">,</span>
            <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="s2">&quot;/currency&quot;</span><span class="p">,</span>
            <span class="s2">&quot;secure&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The latter form allows for customizing the <code class="docutils literal notranslate"><span class="pre">domain</span></code> and <code class="docutils literal notranslate"><span class="pre">path</span></code>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<span class="target" id="std-reqmeta-dont_merge_cookies"></span><p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests.
That’s the typical behaviour of any regular web browser.</p>
<p>Note that setting the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_merge_cookies</span></code></a> key to <code class="docutils literal notranslate"><span class="pre">True</span></code> in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">request.meta</span></code> causes custom cookies to be
ignored.</p>
<p>For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Cookies set via the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header are not considered by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code> parameter. This is a known
current limitation that is being worked on.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0: </span>Cookie values that are <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a> or <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>
are casted to <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>.</p>
</div>
</p></li>
<li><p><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the encoding of this request (defaults to <code class="docutils literal notranslate"><span class="pre">'utf-8'</span></code>).
This encoding will be used to percent-encode the URL and to convert the
body to bytes (if given as a string).</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the priority of this request (defaults to <code class="docutils literal notranslate"><span class="pre">0</span></code>).
The priority is used by the scheduler to define the order used to process
requests.  Requests with a higher priority value will execute earlier.
Negative values are allowed in order to indicate relatively low-priority.</p></li>
<li><p><strong>dont_filter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>errback</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>collections.abc.Callable</em></a>) – <p>a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Failure</span></code></a> as first parameter.
For more information,
see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-errbacks"><span class="std std-ref">Using errbacks to catch exceptions in request processing</span></a> below.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span>The <em>callback</em> parameter is no longer required when the <em>errback</em>
parameter is specified.</p>
</div>
</p></li>
<li><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – Flags sent to the request, can be used for logging or similar purposes.</p></li>
<li><p><strong>cb_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – A dict with arbitrary data that will be passed as keyword arguments to the Request’s callback.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.url">
<span class="sig-name descname"><span class="pre">url</span></span><a class="headerlink" href="#scrapy.http.Request.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.method">
<span class="sig-name descname"><span class="pre">method</span></span><a class="headerlink" href="#scrapy.http.Request.method" title="Permalink to this definition">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <code class="docutils literal notranslate"><span class="pre">&quot;GET&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;POST&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;PUT&quot;</span></code>, etc</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.headers">
<span class="sig-name descname"><span class="pre">headers</span></span><a class="headerlink" href="#scrapy.http.Request.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.body">
<span class="sig-name descname"><span class="pre">body</span></span><a class="headerlink" href="#scrapy.http.Request.body" title="Permalink to this definition">¶</a></dt>
<dd><p>The request body as bytes.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.meta">
<span class="sig-name descname"><span class="pre">meta</span></span><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">{}</span></em><a class="headerlink" href="#scrapy.http.Request.meta" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>A dictionary of arbitrary metadata for the request.</p>
<p>You may extend request metadata as you see fit.</p>
<p>Request metadata can also be accessed through the
<a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code></a> attribute of a response.</p>
<p>To pass data from one spider callback to another, consider using
<a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code></a> instead. However, request metadata may be the right
choice in certain scenarios, such as to maintain some debugging data
across all follow-up requests (e.g. the source URL).</p>
<p>A common use of request metadata is to define request-specific
parameters for Scrapy components (extensions, middlewares, etc.). For
example, if you set <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> will never
retry that request, even if it fails. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-meta"><span class="std std-ref">Request.meta special keys</span></a>.</p>
<p>You may also use request metadata in your custom Scrapy components, for
example, to keep request state information relevant to your component.
For example,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> uses the
<code class="docutils literal notranslate"><span class="pre">retry_times</span></code> metadata key to keep track of how many times a request
has been retried so far.</p>
<p>Copying all the metadata of a previous request into a new, follow-up
request in a spider callback is a bad practice, because request
metadata may include metadata set by Scrapy components that is not
meant to be copied into other requests. For example, copying the
<code class="docutils literal notranslate"><span class="pre">retry_times</span></code> metadata key into follow-up requests can lower the
amount of retries allowed for those follow-up requests.</p>
<p>You should only copy all request metadata from one request to another
if the new request is meant to replace the old request, as is often the
case when returning a request from a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middleware</span></a> method.</p>
<p>Also mind that the <a class="reference internal" href="#scrapy.http.Request.copy" title="scrapy.http.Request.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code></a> and <a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a> request methods
<a class="reference external" href="https://docs.python.org/3/library/copy.html" title="(in Python v3.13)"><span class="xref std std-doc">shallow-copy</span></a> request metadata.</p>
</div></blockquote>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.cb_kwargs">
<span class="sig-name descname"><span class="pre">cb_kwargs</span></span><a class="headerlink" href="#scrapy.http.Request.cb_kwargs" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary that contains arbitrary metadata for this request. Its contents
will be passed to the Request’s callback as keyword arguments. It is empty
for new Requests, which means by default callbacks only get a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object as argument.</p>
<p>This dict is <a class="reference external" href="https://docs.python.org/3/library/copy.html" title="(in Python v3.13)"><span class="xref std std-doc">shallow copied</span></a> when the request is
cloned using the <code class="docutils literal notranslate"><span class="pre">copy()</span></code> or <code class="docutils literal notranslate"><span class="pre">replace()</span></code> methods, and can also be
accessed, in your spider, from the <code class="docutils literal notranslate"><span class="pre">response.cb_kwargs</span></code> attribute.</p>
<p>In case of a failure to process the request, this dict can be accessed as
<code class="docutils literal notranslate"><span class="pre">failure.request.cb_kwargs</span></code> in the request’s errback. For more information,
see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#errback-cb-kwargs"><span class="std std-ref">Accessing additional data in errback functions</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Request.attributes">
<span class="sig-name descname"><span class="pre">attributes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">('url',</span> <span class="pre">'callback',</span> <span class="pre">'method',</span> <span class="pre">'headers',</span> <span class="pre">'body',</span> <span class="pre">'cookies',</span> <span class="pre">'meta',</span> <span class="pre">'encoding',</span> <span class="pre">'priority',</span> <span class="pre">'dont_filter',</span> <span class="pre">'errback',</span> <span class="pre">'flags',</span> <span class="pre">'cb_kwargs')</span></em><a class="headerlink" href="#scrapy.http.Request.attributes" title="Permalink to this definition">¶</a></dt>
<dd><p>A tuple of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> objects containing the name of all public
attributes of the class that are also keyword parameters of the
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>Currently used by <a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.replace()</span></code></a>, <a class="reference internal" href="#scrapy.http.Request.to_dict" title="scrapy.http.Request.to_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.to_dict()</span></code></a> and
<a class="reference internal" href="#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">request_from_dict()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Request.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Request.replace">
<span class="sig-name descname"><span class="pre">replace</span></span><span class="sig-paren">(</span><span class="optional">[</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cookies</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_filter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
<a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> and <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attributes are shallow
copied by default (unless new values are given as arguments). See also
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Request.from_curl">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_curl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">curl_command</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_unknown_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.http.Request.from_curl" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a Request object from a string containing a <a class="reference external" href="https://curl.se/">cURL</a> command. It populates the HTTP method, the
URL, the headers, the cookies and the body. It accepts the same
arguments as the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class, taking preference and
overriding the values of the same arguments contained in the cURL
command.</p>
<p>Unrecognized options are ignored by default. To raise an error when
finding unknown options call this method by passing
<code class="docutils literal notranslate"><span class="pre">ignore_unknown_options=False</span></code>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Using <a class="reference internal" href="#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_curl()</span></code></a> from <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
subclasses, such as <a class="reference internal" href="#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonRequest</span></code></a>, or
<code class="xref py py-class docutils literal notranslate"><span class="pre">XmlRpcRequest</span></code>, as well as having
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middlewares</span></a>
and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a>
enabled, such as
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultHeadersMiddleware</span></code></a>,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UserAgentMiddleware</span></code></a>,
or
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a>,
may modify the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
</div>
<p>To translate a cURL command into a Scrapy request,
you may use <a class="reference external" href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Request.to_dict">
<span class="sig-name descname"><span class="pre">to_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.http.Request.to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary containing the Request’s data.</p>
<p>Use <a class="reference internal" href="#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">request_from_dict()</span></code></a> to convert back into a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object.</p>
<p>If a spider is given, this method will try to find out the name of the spider methods used as callback
and errback and include them in the output dict, raising an exception if they cannot be found.</p>
</dd></dl>

</dd></dl>

<section id="other-functions-related-to-requests">
<h5>Other functions related to requests<a class="headerlink" href="#other-functions-related-to-requests" title="Permalink to this heading">¶</a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.http.request.NO_CALLBACK">
<span class="sig-prename descclassname"><span class="pre">scrapy.http.request.</span></span><span class="sig-name descname"><span class="pre">NO_CALLBACK</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.NoReturn" title="(in Python v3.13)"><span class="pre">NoReturn</span></a></span></span><a class="headerlink" href="#scrapy.http.request.NO_CALLBACK" title="Permalink to this definition">¶</a></dt>
<dd><p>When assigned to the <code class="docutils literal notranslate"><span class="pre">callback</span></code> parameter of
<a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, it indicates that the request is not meant
to have a spider callback at all.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;https://example.com&quot;</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">NO_CALLBACK</span><span class="p">)</span>
</pre></div>
</div>
<p>This value should be used by <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-components"><span class="std std-ref">components</span></a> that
create and handle their own requests, e.g. through
<code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.core.engine.ExecutionEngine.download()</span></code>, so that downloader
middlewares handling such requests can treat them differently from requests
intended for the <a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> callback.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.request.request_from_dict">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.request.</span></span><span class="sig-name descname"><span class="pre">request_from_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.request.Request"><span class="pre">Request</span></a></span></span><a class="headerlink" href="#scrapy.utils.request.request_from_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object from a dict.</p>
<p>If a spider is given, it will try to resolve the callbacks looking at the
spider for methods with the same name.</p>
</dd></dl>

</section>
<section id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h5>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="Permalink to this heading">¶</a></h5>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
        <span class="s2">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Visited </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback.
The following example shows how to achieve this by using the
<a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
        <span class="s2">&quot;http://www.example.com/index.html&quot;</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">,</span>
        <span class="n">cb_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">main_url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">request</span><span class="o">.</span><span class="n">cb_kwargs</span><span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;bar&quot;</span>  <span class="c1"># add more arguments for the callback</span>
    <span class="k">yield</span> <span class="n">request</span>


<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">main_url</span><span class="p">,</span> <span class="n">foo</span><span class="p">):</span>
    <span class="k">yield</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">main_url</span><span class="o">=</span><span class="n">main_url</span><span class="p">,</span>
        <span class="n">other_url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span>
        <span class="n">foo</span><span class="o">=</span><span class="n">foo</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> was introduced in version <code class="docutils literal notranslate"><span class="pre">1.7</span></code>.
Prior to that, using <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> was recommended for passing
information around callbacks. After <code class="docutils literal notranslate"><span class="pre">1.7</span></code>, <a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a>
became the preferred way for handling user information, leaving <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a>
for communication with components like middlewares and extensions.</p>
</div>
</section>
<section id="using-errbacks-to-catch-exceptions-in-request-processing">
<span id="topics-request-response-ref-errbacks"></span><h5>Using errbacks to catch exceptions in request processing<a class="headerlink" href="#using-errbacks-to-catch-exceptions-in-request-processing" title="Permalink to this heading">¶</a></h5>
<p>The errback of a request is a function that will be called when an exception
is raise while processing it.</p>
<p>It receives a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Failure</span></code></a> as first parameter and can
be used to track connection establishment timeouts, DNS errors etc.</p>
<p>Here’s an example spider logging all errors and catching some specific
errors if needed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">scrapy.spidermiddlewares.httperror</span> <span class="kn">import</span> <span class="n">HttpError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="kn">import</span> <span class="n">DNSLookupError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="kn">import</span> <span class="ne">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span>


<span class="k">class</span> <span class="nc">ErrbackSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;errback_example&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.httpbin.org/&quot;</span><span class="p">,</span>  <span class="c1"># HTTP 200 expected</span>
        <span class="s2">&quot;http://www.httpbin.org/status/404&quot;</span><span class="p">,</span>  <span class="c1"># Not found error</span>
        <span class="s2">&quot;http://www.httpbin.org/status/500&quot;</span><span class="p">,</span>  <span class="c1"># server issue</span>
        <span class="s2">&quot;http://www.httpbin.org:12345/&quot;</span><span class="p">,</span>  <span class="c1"># non-responding host, timeout expected</span>
        <span class="s2">&quot;https://example.invalid/&quot;</span><span class="p">,</span>  <span class="c1"># DNS error expected</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
                <span class="n">u</span><span class="p">,</span>
                <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_httpbin</span><span class="p">,</span>
                <span class="n">errback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">errback_httpbin</span><span class="p">,</span>
                <span class="n">dont_filter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Got successful response from </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
        <span class="c1"># do something useful here...</span>

    <span class="k">def</span> <span class="nf">errback_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">failure</span><span class="p">):</span>
        <span class="c1"># log all failures</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">failure</span><span class="p">))</span>

        <span class="c1"># in case you want to do something special for some errors,</span>
        <span class="c1"># you may need the failure&#39;s type:</span>

        <span class="k">if</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">HttpError</span><span class="p">):</span>
            <span class="c1"># these exceptions come from HttpError spider middleware</span>
            <span class="c1"># you can get the non-200 response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">response</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;HttpError on </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">DNSLookupError</span><span class="p">):</span>
            <span class="c1"># this is the original request</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;DNSLookupError on </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="ne">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span><span class="p">):</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;TimeoutError on </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="accessing-additional-data-in-errback-functions">
<span id="errback-cb-kwargs"></span><h5>Accessing additional data in errback functions<a class="headerlink" href="#accessing-additional-data-in-errback-functions" title="Permalink to this heading">¶</a></h5>
<p>In case of a failure to process the request, you may be interested in
accessing arguments to the callback functions so you can process further
based on the arguments in the errback. The following example shows how to
achieve this by using <code class="docutils literal notranslate"><span class="pre">Failure.request.cb_kwargs</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
        <span class="s2">&quot;http://www.example.com/index.html&quot;</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">,</span>
        <span class="n">errback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">errback_page2</span><span class="p">,</span>
        <span class="n">cb_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">main_url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">yield</span> <span class="n">request</span>


<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">main_url</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">def</span> <span class="nf">errback_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">failure</span><span class="p">):</span>
    <span class="k">yield</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">main_url</span><span class="o">=</span><span class="n">failure</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">cb_kwargs</span><span class="p">[</span><span class="s2">&quot;main_url&quot;</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="request-fingerprints">
<span id="id1"></span><h5>Request fingerprints<a class="headerlink" href="#request-fingerprints" title="Permalink to this heading">¶</a></h5>
<p>There are some aspects of scraping, such as filtering out duplicate requests
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a>) or caching responses (see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a>), where you need the ability to generate a short,
unique identifier from a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object: a request
fingerprint.</p>
<p>You often do not need to worry about request fingerprints, the default request
fingerprinter works for most projects.</p>
<p>However, there is no universal way to generate a unique identifier from a
request, because different situations require comparing requests differently.
For example, sometimes you may need to compare URLs case-insensitively, include
URL fragments, exclude certain URL query parameters, include some or all
headers, etc.</p>
<p>To change how request fingerprints are built for your requests, use the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-REQUEST_FINGERPRINTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_CLASS</span></code></a> setting.</p>
<section id="request-fingerprinter-class">
<span id="std-setting-REQUEST_FINGERPRINTER_CLASS"></span><h6>REQUEST_FINGERPRINTER_CLASS<a class="headerlink" href="#request-fingerprinter-class" title="Permalink to this heading">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.7.</span></p>
</div>
<p>Default: <a class="reference internal" href="#scrapy.utils.request.RequestFingerprinter" title="scrapy.utils.request.RequestFingerprinter"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.request.RequestFingerprinter</span></code></a></p>
<p>A <a class="hxr-hoverxref hxr-tooltip reference internal" href="#custom-request-fingerprinter"><span class="std std-ref">request fingerprinter class</span></a> or its
import path.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.utils.request.RequestFingerprinter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.utils.request.</span></span><span class="sig-name descname"><span class="pre">RequestFingerprinter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.request.RequestFingerprinter" title="Permalink to this definition">¶</a></dt>
<dd><p>Default fingerprinter.</p>
<p>It takes into account a canonical version
(<a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url" title="(in w3lib v2.2)"><code class="xref py py-func docutils literal notranslate"><span class="pre">w3lib.url.canonicalize_url()</span></code></a>) of <a class="reference internal" href="#scrapy.http.Request.url" title="scrapy.http.Request.url"><code class="xref py py-attr docutils literal notranslate"><span class="pre">request.url</span></code></a> and the values of <a class="reference internal" href="#scrapy.http.Request.method" title="scrapy.http.Request.method"><code class="xref py py-attr docutils literal notranslate"><span class="pre">request.method</span></code></a> and <a class="reference internal" href="#scrapy.http.Request.body" title="scrapy.http.Request.body"><code class="xref py py-attr docutils literal notranslate"><span class="pre">request.body</span></code></a>. It then generates an <a class="reference external" href="https://en.wikipedia.org/wiki/SHA-1">SHA1</a> hash.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref std std-setting docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_IMPLEMENTATION</span></code>.</p>
</div>
</dd></dl>

</section>
<section id="writing-your-own-request-fingerprinter">
<span id="custom-request-fingerprinter"></span><h6>Writing your own request fingerprinter<a class="headerlink" href="#writing-your-own-request-fingerprinter" title="Permalink to this heading">¶</a></h6>
<p>A request fingerprinter is a class that must implement the following method:</p>
<dl class="py method">
<dt class="sig sig-object py" id="fingerprint">
<span class="sig-name descname"><span class="pre">fingerprint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#fingerprint" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes</span></code></a> object that uniquely identifies <em>request</em>.</p>
<p>See also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#request-fingerprint-restrictions"><span class="std std-ref">Request fingerprint restrictions</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><em>scrapy.http.Request</em></a>) – request to fingerprint</p>
</dd>
</dl>
</dd></dl>

<p>Additionally, it may also implement the following method:</p>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>If present, this class method is called to create a request fingerprinter
instance from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object. It must return a
new instance of the request fingerprinter.</p>
<p><em>crawler</em> provides access to all Scrapy core components like settings and
signals; it is a way for the request fingerprinter to access them and hook
its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this request fingerprinter</p>
</dd>
</dl>
</dd></dl>

<p>The <a class="reference internal" href="#fingerprint" title="fingerprint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fingerprint()</span></code></a> method of the default request fingerprinter,
<a class="reference internal" href="#scrapy.utils.request.RequestFingerprinter" title="scrapy.utils.request.RequestFingerprinter"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.request.RequestFingerprinter</span></code></a>, uses
<a class="reference internal" href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint()</span></code></a> with its default parameters. For some
common use cases you can use <a class="reference internal" href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint()</span></code></a> as well
in your <a class="reference internal" href="#fingerprint" title="fingerprint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fingerprint()</span></code></a> method implementation:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.request.fingerprint">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.request.</span></span><span class="sig-name descname"><span class="pre">fingerprint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_fragments</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></span><a class="headerlink" href="#scrapy.utils.request.fingerprint" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the request fingerprint.</p>
<p>The request fingerprint is a hash that uniquely identifies the resource the
request points to. For example, take the following two urls:
<code class="docutils literal notranslate"><span class="pre">http://www.example.com/query?id=111&amp;cat=222</span></code>,
<code class="docutils literal notranslate"><span class="pre">http://www.example.com/query?cat=222&amp;id=111</span></code>.</p>
<p>Even though those are two different URLs both point to the same resource
and are equivalent (i.e. they should return the same response).</p>
<p>Another example are cookies used to store session ids. Suppose the
following page is only accessible to authenticated users:
<code class="docutils literal notranslate"><span class="pre">http://www.example.com/members/offers.html</span></code>.</p>
<p>Lots of sites use a cookie to store the session id, which adds a random
component to the HTTP Request and thus should be ignored when calculating
the fingerprint.</p>
<p>For this reason, request headers are ignored by default when calculating
the fingerprint. If you want to include specific headers use the
include_headers argument, which is a list of Request headers to include.</p>
<p>Also, servers usually ignore fragments in urls when handling requests,
so they are also ignored by default when calculating the fingerprint.
If you want to include them, set the keep_fragments argument to True
(for instance when handling requests with a headless browser).</p>
</dd></dl>

<p>For example, to take the value of a request header named <code class="docutils literal notranslate"><span class="pre">X-ID</span></code> into
account:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># my_project/settings.py</span>
<span class="n">REQUEST_FINGERPRINTER_CLASS</span> <span class="o">=</span> <span class="s2">&quot;my_project.utils.RequestFingerprinter&quot;</span>

<span class="c1"># my_project/utils.py</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.request</span> <span class="kn">import</span> <span class="n">fingerprint</span>


<span class="k">class</span> <span class="nc">RequestFingerprinter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">fingerprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">fingerprint</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">include_headers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X-ID&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>You can also write your own fingerprinting logic from scratch.</p>
<p>However, if you do not use <a class="reference internal" href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint()</span></code></a>, make sure
you use <a class="reference external" href="https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">WeakKeyDictionary</span></code></a> to cache request fingerprints:</p>
<ul class="simple">
<li><p>Caching saves CPU by ensuring that fingerprints are calculated only once
per request, and not once per Scrapy component that needs the fingerprint
of a request.</p></li>
<li><p>Using <a class="reference external" href="https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">WeakKeyDictionary</span></code></a> saves memory by ensuring that
request objects do not stay in memory forever just because you have
references to them in your cache dictionary.</p></li>
</ul>
<p>For example, to take into account only the URL of a request, without any prior
URL canonicalization or taking the request method or body into account:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hashlib</span> <span class="kn">import</span> <span class="n">sha1</span>
<span class="kn">from</span> <span class="nn">weakref</span> <span class="kn">import</span> <span class="n">WeakKeyDictionary</span>

<span class="kn">from</span> <span class="nn">scrapy.utils.python</span> <span class="kn">import</span> <span class="n">to_bytes</span>


<span class="k">class</span> <span class="nc">RequestFingerprinter</span><span class="p">:</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="n">WeakKeyDictionary</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fingerprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">request</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="n">fp</span> <span class="o">=</span> <span class="n">sha1</span><span class="p">()</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_bytes</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">request</span><span class="p">]</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">digest</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">request</span><span class="p">]</span>
</pre></div>
</div>
<p>If you need to be able to override the request fingerprinting for arbitrary
requests from your spider callbacks, you may implement a request fingerprinter
that reads fingerprints from <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">request.meta</span></code></a>
when available, and then falls back to
<a class="reference internal" href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint()</span></code></a>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.utils.request</span> <span class="kn">import</span> <span class="n">fingerprint</span>


<span class="k">class</span> <span class="nc">RequestFingerprinter</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">fingerprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;fingerprint&quot;</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;fingerprint&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">fingerprint</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
</pre></div>
</div>
<p>If you need to reproduce the same fingerprinting algorithm as Scrapy 2.6
without using the deprecated <code class="docutils literal notranslate"><span class="pre">'2.6'</span></code> value of the
<code class="xref std std-setting docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_IMPLEMENTATION</span></code> setting, use the following
request fingerprinter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hashlib</span> <span class="kn">import</span> <span class="n">sha1</span>
<span class="kn">from</span> <span class="nn">weakref</span> <span class="kn">import</span> <span class="n">WeakKeyDictionary</span>

<span class="kn">from</span> <span class="nn">scrapy.utils.python</span> <span class="kn">import</span> <span class="n">to_bytes</span>
<span class="kn">from</span> <span class="nn">w3lib.url</span> <span class="kn">import</span> <span class="n">canonicalize_url</span>


<span class="k">class</span> <span class="nc">RequestFingerprinter</span><span class="p">:</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="n">WeakKeyDictionary</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">fingerprint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">request</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">:</span>
            <span class="n">fp</span> <span class="o">=</span> <span class="n">sha1</span><span class="p">()</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_bytes</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">method</span><span class="p">))</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">to_bytes</span><span class="p">(</span><span class="n">canonicalize_url</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)))</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">body</span> <span class="ow">or</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">request</span><span class="p">]</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">digest</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">[</span><span class="n">request</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="request-fingerprint-restrictions">
<span id="id2"></span><h6>Request fingerprint restrictions<a class="headerlink" href="#request-fingerprint-restrictions" title="Permalink to this heading">¶</a></h6>
<p>Scrapy components that use request fingerprints may impose additional
restrictions on the format of the fingerprints that your <a class="hxr-hoverxref hxr-tooltip reference internal" href="#custom-request-fingerprinter"><span class="std std-ref">request
fingerprinter</span></a> generates.</p>
<p>The following built-in Scrapy components have such restrictions:</p>
<ul>
<li><p><a class="reference internal" href="index.html#scrapy.extensions.httpcache.FilesystemCacheStorage" title="scrapy.extensions.httpcache.FilesystemCacheStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.FilesystemCacheStorage</span></code></a> (default
value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a>)</p>
<p>Request fingerprints must be at least 1 byte long.</p>
<p>Path and filename length limits of the file system of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DIR</span></code></a> also apply. Inside <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DIR</span></code></a>,
the following directory structure is created:</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">Spider.name</span></code></p>
<ul>
<li><p>first byte of a request fingerprint as hexadecimal</p>
<ul>
<li><p>fingerprint as hexadecimal</p>
<ul>
<li><p>filenames up to 16 characters long</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>For example, if a request fingerprint is made of 20 bytes (default),
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DIR</span></code></a> is <code class="docutils literal notranslate"><span class="pre">'/home/user/project/.scrapy/httpcache'</span></code>,
and the name of your spider is <code class="docutils literal notranslate"><span class="pre">'my_spider'</span></code> your file system must
support a file path like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">project</span><span class="o">/.</span><span class="n">scrapy</span><span class="o">/</span><span class="n">httpcache</span><span class="o">/</span><span class="n">my_spider</span><span class="o">/</span><span class="mi">01</span><span class="o">/</span><span class="mi">0123456789</span><span class="n">abcdef0123456789abcdef01234567</span><span class="o">/</span><span class="n">response_headers</span>
</pre></div>
</div>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.extensions.httpcache.DbmCacheStorage" title="scrapy.extensions.httpcache.DbmCacheStorage"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.DbmCacheStorage</span></code></a></p>
<p>The underlying DBM implementation must support keys as long as twice
the number of bytes of a request fingerprint, plus 5. For example,
if a request fingerprint is made of 20 bytes (default),
45-character-long keys must be supported.</p>
</li>
</ul>
</section>
</section>
</section>
<section id="request-meta-special-keys">
<span id="topics-request-meta"></span><h4>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="Permalink to this heading">¶</a></h4>
<p>The <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-autothrottle_dont_adjust_delay"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">autothrottle_dont_adjust_delay</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-bindaddress"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">bindaddress</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_cache</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_merge_cookies</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_obey_robotstxt"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_obey_robotstxt</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_redirect</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_retry"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_retry</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-download_fail_on_dataloss"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_fail_on_dataloss</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-download_latency"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_latency</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_maxsize</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-download_warnsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_warnsize</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ftp_password</span></code> (See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_PASSWORD</span></code></a> for more info)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ftp_user</span></code> (See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_USER</span></code></a> for more info)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-handle_httpstatus_all"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-handle_httpstatus_list"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-referrer_policy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">referrer_policy</span></code></a></p></li>
</ul>
<section id="bindaddress">
<span id="std-reqmeta-bindaddress"></span><h5>bindaddress<a class="headerlink" href="#bindaddress" title="Permalink to this heading">¶</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</section>
<section id="download-timeout">
<span id="std-reqmeta-download_timeout"></span><h5>download_timeout<a class="headerlink" href="#download-timeout" title="Permalink to this heading">¶</a></h5>
<p>The amount of time (in secs) that the downloader will wait before timing out.
See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a>.</p>
</section>
<section id="download-latency">
<span id="std-reqmeta-download_latency"></span><h5>download_latency<a class="headerlink" href="#download-latency" title="Permalink to this heading">¶</a></h5>
<p>The amount of time spent to fetch the response, since the request has been
started, i.e. HTTP message sent over the network. This meta key only becomes
available when the response has been downloaded. While most other meta keys are
used to control Scrapy behavior, this one is supposed to be read-only.</p>
</section>
<section id="download-fail-on-dataloss">
<span id="std-reqmeta-download_fail_on_dataloss"></span><h5>download_fail_on_dataloss<a class="headerlink" href="#download-fail-on-dataloss" title="Permalink to this heading">¶</a></h5>
<p>Whether or not to fail on broken responses. See:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a>.</p>
</section>
<section id="max-retry-times">
<span id="std-reqmeta-max_retry_times"></span><h5>max_retry_times<a class="headerlink" href="#max-retry-times" title="Permalink to this heading">¶</a></h5>
<p>The meta key is used set retry times per request. When initialized, the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key takes higher precedence over the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
</section>
</section>
<section id="stopping-the-download-of-a-response">
<span id="topics-stop-response-download"></span><h4>Stopping the download of a Response<a class="headerlink" href="#stopping-the-download-of-a-response" title="Permalink to this heading">¶</a></h4>
<p>Raising a <a class="reference internal" href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload"><code class="xref py py-exc docutils literal notranslate"><span class="pre">StopDownload</span></code></a> exception from a handler for the
<a class="reference internal" href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes_received</span></code></a> or <a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a>
signals will stop the download of a given response. See the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">StopSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;stop&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://docs.scrapy.org/en/latest/&quot;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">spider</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">on_bytes_received</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">scrapy</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">bytes_received</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">spider</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># &#39;last_chars&#39; show that the full response was not downloaded</span>
        <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;len&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">),</span> <span class="s2">&quot;last_chars&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="o">-</span><span class="mi">40</span><span class="p">:]}</span>

    <span class="k">def</span> <span class="nf">on_bytes_received</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">StopDownload</span><span class="p">(</span><span class="n">fail</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>which produces the following output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">12</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">12</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">0</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">13</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">downloader</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">http11</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Download</span> <span class="n">stopped</span> <span class="k">for</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">en</span><span class="o">/</span><span class="n">latest</span><span class="o">/&gt;</span> <span class="kn">from</span> <span class="nn">signal</span> <span class="n">handler</span> <span class="n">StopSpider</span><span class="o">.</span><span class="n">on_bytes_received</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">13</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">en</span><span class="o">/</span><span class="n">latest</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span> <span class="p">[</span><span class="s1">&#39;download_stopped&#39;</span><span class="p">]</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">13</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scraper</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Scraped</span> <span class="kn">from</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">docs</span><span class="o">.</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">en</span><span class="o">/</span><span class="n">latest</span><span class="o">/&gt;</span>
<span class="p">{</span><span class="s1">&#39;len&#39;</span><span class="p">:</span> <span class="mi">279</span><span class="p">,</span> <span class="s1">&#39;last_chars&#39;</span><span class="p">:</span> <span class="s1">&#39;dth, initial-scale=1.0&quot;&gt;</span><span class="se">\n</span><span class="s1">  </span><span class="se">\n</span><span class="s1">  &lt;title&gt;Scr&#39;</span><span class="p">}</span>
<span class="mi">2020</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">19</span> <span class="mi">17</span><span class="p">:</span><span class="mi">26</span><span class="p">:</span><span class="mi">13</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Closing</span> <span class="n">spider</span> <span class="p">(</span><span class="n">finished</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, resulting responses are handled by their corresponding errbacks. To
call their callback instead, like in this example, pass <code class="docutils literal notranslate"><span class="pre">fail=False</span></code> to the
<a class="reference internal" href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload"><code class="xref py py-exc docutils literal notranslate"><span class="pre">StopDownload</span></code></a> exception.</p>
</section>
<section id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h4>Request subclasses<a class="headerlink" href="#request-subclasses" title="Permalink to this heading">¶</a></h4>
<p>Here is the list of built-in <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<section id="formrequest-objects">
<h5>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="Permalink to this heading">¶</a></h5>
<p>The FormRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="https://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.scrapy.http.request.form.FormRequest">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.request.form.</span></span><span class="sig-name descname"><span class="pre">FormRequest</span></span><a class="headerlink" href="#scrapy.http.scrapy.http.request.form.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.scrapy.http.FormRequest">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">FormRequest</span></span><a class="headerlink" href="#scrapy.http.scrapy.http.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.scrapy.FormRequest">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.</span></span><span class="sig-name descname"><span class="pre">FormRequest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">formdata</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.scrapy.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> class adds a new keyword parameter to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>formdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.13)"><em>collections.abc.Iterable</em></a>) – is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</p>
</dd>
</dl>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> objects support the following class method in
addition to the standard <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> methods:</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.scrapy.FormRequest.FormRequest.from_response">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">FormRequest.</span></span><span class="sig-name descname"><span class="pre">from_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">formname=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formid=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formnumber=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formdata=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formxpath=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formcss=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clickdata=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_click=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.scrapy.FormRequest.FormRequest.from_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> object with its form field values
pre-populated with those found in the HTML <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element contained
in the given response. For an example see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <code class="docutils literal notranslate"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></code>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_response()</span></code> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<code class="docutils literal notranslate"><span class="pre">dont_click</span></code> argument to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<code class="docutils literal notranslate"><span class="pre">clickdata</span></code> argument.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Using this method with select elements which have leading
or trailing whitespace in the option values will not work due to a
<a class="reference external" href="https://bugs.launchpad.net/lxml/+bug/1665241">bug in lxml</a>, which should be fixed in lxml 3.8 and above.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response containing a HTML form which will be used
to pre-populate the form fields</p></li>
<li><p><strong>formname</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – if given, the form with name attribute set to this value will be used.</p></li>
<li><p><strong>formid</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – if given, the form with id attribute set to this value will be used.</p></li>
<li><p><strong>formxpath</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – if given, the first form that matches the xpath will be used.</p></li>
<li><p><strong>formcss</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – if given, the first form that matches the css selector will be used.</p></li>
<li><p><strong>formnumber</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>formdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – fields to override in the form data. If a field was
already present in the response <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element, its value is
overridden by the one passed in this parameter. If a value passed in
this parameter is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the field will not be included in the
request, even if it was present in the response <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element.</p></li>
<li><p><strong>clickdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – attributes to lookup the control clicked. If it’s not
given, the form data will be submitted simulating a click on the
first clickable element. In addition to html attributes, the control
can be identified by its zero-based index relative to other
submittable inputs inside the form, via the <code class="docutils literal notranslate"><span class="pre">nr</span></code> attribute.</p></li>
<li><p><strong>dont_click</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, the form data will be submitted without
clicking in any element.</p></li>
</ul>
</dd>
</dl>
<p>The other parameters of this class method are passed directly to the
<code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</dd></dl>

</dd></dl>

</section>
<section id="request-usage-examples">
<h5>Request usage examples<a class="headerlink" href="#request-usage-examples" title="Permalink to this heading">¶</a></h5>
<section id="using-formrequest-to-send-data-via-http-post">
<h6>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this heading">¶</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> object (from your
spider) like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="p">[</span>
    <span class="n">FormRequest</span><span class="p">(</span>
        <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
        <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;John Doe&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">:</span> <span class="s2">&quot;27&quot;</span><span class="p">},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
<section id="using-formrequest-from-response-to-simulate-a-user-login">
<span id="topics-request-response-ref-request-userlogin"></span><h6>Using FormRequest.from_response() to simulate a user login<a class="headerlink" href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this heading">¶</a></h6>
<p>It is usual for web sites to provide pre-populated form fields through <code class="docutils literal notranslate"><span class="pre">&lt;input</span>
<span class="pre">type=&quot;hidden&quot;&gt;</span></code> elements, such as session related data or authentication
tokens (for login pages). When scraping, you’ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code>
method for this job. Here’s an example spider which uses it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">def</span> <span class="nf">authentication_failed</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="c1"># TODO: Check the contents of the response and return True if it failed</span>
    <span class="c1"># or False if it succeeded.</span>
    <span class="k">pass</span>


<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example.com&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://www.example.com/users/login.php&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;john&quot;</span><span class="p">,</span> <span class="s2">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;secret&quot;</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">authentication_failed</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Login failed&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</section>
</section>
<section id="jsonrequest">
<h5>JsonRequest<a class="headerlink" href="#jsonrequest" title="Permalink to this heading">¶</a></h5>
<p>The JsonRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class with functionality for
dealing with JSON requests.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.JsonRequest">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">JsonRequest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">...</span> <span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dumps_kwargs</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.JsonRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonRequest</span></code></a> class adds two new keyword parameters to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<p>Using the <a class="reference internal" href="#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonRequest</span></code></a> will set the <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> header to <code class="docutils literal notranslate"><span class="pre">application/json</span></code>
and <code class="docutils literal notranslate"><span class="pre">Accept</span></code> header to <code class="docutils literal notranslate"><span class="pre">application/json,</span> <span class="pre">text/javascript,</span> <span class="pre">*/*;</span> <span class="pre">q=0.01</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – is any JSON serializable object that needs to be JSON encoded and assigned to body.
if <a class="reference internal" href="#scrapy.http.Request.body" title="scrapy.http.Request.body"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.body</span></code></a> argument is provided this parameter will be ignored.
if <a class="reference internal" href="#scrapy.http.Request.body" title="scrapy.http.Request.body"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.body</span></code></a> argument is not provided and data argument is provided <a class="reference internal" href="#scrapy.http.Request.method" title="scrapy.http.Request.method"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.method</span></code></a> will be
set to <code class="docutils literal notranslate"><span class="pre">'POST'</span></code> automatically.</p></li>
<li><p><strong>dumps_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Parameters that will be passed to underlying <a class="reference external" href="https://docs.python.org/3/library/json.html#json.dumps" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">json.dumps()</span></code></a> method which is used to serialize
data into JSON format.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.JsonRequest.attributes">
<span class="sig-name descname"><span class="pre">attributes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">('url',</span> <span class="pre">'callback',</span> <span class="pre">'method',</span> <span class="pre">'headers',</span> <span class="pre">'body',</span> <span class="pre">'cookies',</span> <span class="pre">'meta',</span> <span class="pre">'encoding',</span> <span class="pre">'priority',</span> <span class="pre">'dont_filter',</span> <span class="pre">'errback',</span> <span class="pre">'flags',</span> <span class="pre">'cb_kwargs',</span> <span class="pre">'dumps_kwargs')</span></em><a class="headerlink" href="#scrapy.http.JsonRequest.attributes" title="Permalink to this definition">¶</a></dt>
<dd><p>A tuple of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> objects containing the name of all public
attributes of the class that are also keyword parameters of the
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>Currently used by <a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.replace()</span></code></a>, <a class="reference internal" href="#scrapy.http.Request.to_dict" title="scrapy.http.Request.to_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.to_dict()</span></code></a> and
<a class="reference internal" href="#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">request_from_dict()</span></code></a>.</p>
</dd></dl>

</dd></dl>

</section>
<section id="jsonrequest-usage-example">
<h5>JsonRequest usage example<a class="headerlink" href="#jsonrequest-usage-example" title="Permalink to this heading">¶</a></h5>
<p>Sending a JSON POST request with a JSON payload:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;name1&quot;</span><span class="p">:</span> <span class="s2">&quot;value1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;name2&quot;</span><span class="p">:</span> <span class="s2">&quot;value2&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">yield</span> <span class="n">JsonRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="response-objects">
<h4>Response objects<a class="headerlink" href="#response-objects" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.Response">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">Response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response" title="Permalink to this definition">¶</a></dt>
<dd><p>An object that represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the URL of this response</p></li>
<li><p><strong>status</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the HTTP status of the response. Defaults to <code class="docutils literal notranslate"><span class="pre">200</span></code>.</p></li>
<li><p><strong>headers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</p></li>
<li><p><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – the response body. To access the decoded text as a string, use
<code class="docutils literal notranslate"><span class="pre">response.text</span></code> from an encoding-aware
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-request-response-ref-response-subclasses"><span class="std std-ref">Response subclass</span></a>,
such as <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>.</p></li>
<li><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – is a list containing the initial values for the
<a class="reference internal" href="#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.flags</span></code></a> attribute. If given, the list will be shallow
copied.</p></li>
<li><p><strong>request</strong> (<em>scrapy.Request</em>) – the initial value of the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> attribute.
This represents the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> that generated this response.</p></li>
<li><p><strong>certificate</strong> (<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html" title="(in Twisted)"><em>twisted.internet.ssl.Certificate</em></a>) – an object representing the server’s SSL certificate.</p></li>
<li><p><strong>ip_address</strong> (<a class="reference external" href="https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv4Address" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ipaddress.IPv4Address</span></code></a> or <a class="reference external" href="https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv6Address" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ipaddress.IPv6Address</span></code></a>) – The IP address of the server from which the Response originated.</p></li>
<li><p><strong>protocol</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) – The protocol that was used to download the response.
For instance: “HTTP/1.0”, “HTTP/1.1”, “h2”</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.0: </span>The <code class="docutils literal notranslate"><span class="pre">certificate</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.0: </span>The <code class="docutils literal notranslate"><span class="pre">ip_address</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.5.0: </span>The <code class="docutils literal notranslate"><span class="pre">protocol</span></code> parameter.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.url">
<span class="sig-name descname"><span class="pre">url</span></span><a class="headerlink" href="#scrapy.http.Response.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.status">
<span class="sig-name descname"><span class="pre">status</span></span><a class="headerlink" href="#scrapy.http.Response.status" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <code class="docutils literal notranslate"><span class="pre">200</span></code>,
<code class="docutils literal notranslate"><span class="pre">404</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.headers">
<span class="sig-name descname"><span class="pre">headers</span></span><a class="headerlink" href="#scrapy.http.Response.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers. Values can
be accessed using <code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code> to return the first header value with the
specified name or <code class="xref py py-meth docutils literal notranslate"><span class="pre">getlist()</span></code> to return all header values with the
specified name. For example, this call will give you all cookies in the
headers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">getlist</span><span class="p">(</span><span class="s1">&#39;Set-Cookie&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.body">
<span class="sig-name descname"><span class="pre">body</span></span><a class="headerlink" href="#scrapy.http.Response.body" title="Permalink to this definition">¶</a></dt>
<dd><p>The response body as bytes.</p>
<p>If you want the body as a string, use <a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.text</span></code></a> (only
available in <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.request">
<span class="sig-name descname"><span class="pre">request</span></span><a class="headerlink" href="#scrapy.http.Response.request" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a>.
In particular, this means that:</p>
<ul class="simple">
<li><p>HTTP redirections will create a new request from the request before
redirection. It has the majority of the same metadata and original
request attributes and gets assigned to the redirected response
instead of the propagation of the original request.</p></li>
<li><p>Response.request.url doesn’t always equal Response.url</p></li>
<li><p>This attribute is only available in the spider code, and in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">Spider Middlewares</span></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-response_downloaded"><code class="xref std std-signal docutils literal notranslate"><span class="pre">response_downloaded</span></code></a> signal.</p></li>
</ul>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.meta">
<span class="sig-name descname"><span class="pre">meta</span></span><a class="headerlink" href="#scrapy.http.Response.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> object (i.e. <code class="docutils literal notranslate"><span class="pre">self.request.meta</span></code>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> attribute, the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.meta</span></code></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.cb_kwargs">
<span class="sig-name descname"><span class="pre">cb_kwargs</span></span><a class="headerlink" href="#scrapy.http.Response.cb_kwargs" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> object (i.e. <code class="docutils literal notranslate"><span class="pre">self.request.cb_kwargs</span></code>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> attribute, the
<a class="reference internal" href="#scrapy.http.Response.cb_kwargs" title="scrapy.http.Response.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.cb_kwargs</span></code></a> attribute is propagated along redirects and
retries, so you will get the original <a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> sent
from your spider.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.flags">
<span class="sig-name descname"><span class="pre">flags</span></span><a class="headerlink" href="#scrapy.http.Response.flags" title="Permalink to this definition">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <code class="docutils literal notranslate"><span class="pre">'cached'</span></code>, <code class="docutils literal notranslate"><span class="pre">'redirected</span></code>’, etc. And
they’re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.certificate">
<span class="sig-name descname"><span class="pre">certificate</span></span><a class="headerlink" href="#scrapy.http.Response.certificate" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.0.</span></p>
</div>
<p>A <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">twisted.internet.ssl.Certificate</span></code></a> object representing
the server’s SSL certificate.</p>
<p>Only populated for <code class="docutils literal notranslate"><span class="pre">https</span></code> responses, <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.ip_address">
<span class="sig-name descname"><span class="pre">ip_address</span></span><a class="headerlink" href="#scrapy.http.Response.ip_address" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.1.0.</span></p>
</div>
<p>The IP address of the server from which the Response originated.</p>
<p>This attribute is currently only populated by the HTTP 1.1 download
handler, i.e. for <code class="docutils literal notranslate"><span class="pre">http(s)</span></code> responses. For other handlers,
<a class="reference internal" href="#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ip_address</span></code></a> is always <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.protocol">
<span class="sig-name descname"><span class="pre">protocol</span></span><a class="headerlink" href="#scrapy.http.Response.protocol" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.5.0.</span></p>
</div>
<p>The protocol that was used to download the response.
For instance: “HTTP/1.0”, “HTTP/1.1”</p>
<p>This attribute is currently only populated by the HTTP download
handlers, i.e. for <code class="docutils literal notranslate"><span class="pre">http(s)</span></code> responses. For other handlers,
<a class="reference internal" href="#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol"><code class="xref py py-attr docutils literal notranslate"><span class="pre">protocol</span></code></a> is always <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.Response.attributes">
<span class="sig-name descname"><span class="pre">attributes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">('url',</span> <span class="pre">'status',</span> <span class="pre">'headers',</span> <span class="pre">'body',</span> <span class="pre">'flags',</span> <span class="pre">'request',</span> <span class="pre">'certificate',</span> <span class="pre">'ip_address',</span> <span class="pre">'protocol')</span></em><a class="headerlink" href="#scrapy.http.Response.attributes" title="Permalink to this definition">¶</a></dt>
<dd><p>A tuple of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> objects containing the name of all public
attributes of the class that are also keyword parameters of the
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>Currently used by <a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.replace()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Response.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Response.replace">
<span class="sig-name descname"><span class="pre">replace</span></span><span class="sig-paren">(</span><span class="optional">[</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">status</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cls</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.meta</span></code></a> is copied by default.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Response.urljoin">
<span class="sig-name descname"><span class="pre">urljoin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.urljoin" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs an absolute url by combining the Response’s <a class="reference internal" href="#scrapy.http.Response.url" title="scrapy.http.Response.url"><code class="xref py py-attr docutils literal notranslate"><span class="pre">url</span></code></a> with
a possible relative url.</p>
<p>This is a wrapper over <a class="reference external" href="https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">urljoin()</span></code></a>, it’s merely an alias for
making this call:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Response.follow">
<span class="sig-name descname"><span class="pre">follow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><span class="pre">Link</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CallbackT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'GET'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cookies</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CookiesT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'utf-8'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Failure</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a></span></span><a class="headerlink" href="#scrapy.http.Response.follow" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal notranslate"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal notranslate"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal notranslate"><span class="pre">url</span></code> can be a relative URL or a <code class="docutils literal notranslate"><span class="pre">scrapy.link.Link</span></code> object,
not only an absolute URL.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> provides a <a class="reference internal" href="#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">follow()</span></code></a>
method which supports selectors in addition to absolute/relative URLs
and Link objects.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0: </span>The <em>flags</em> parameter.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.Response.follow_all">
<span class="sig-name descname"><span class="pre">follow_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">urls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><span class="pre">Link</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CallbackT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'GET'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cookies</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CookiesT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'utf-8'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Failure</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.http.Response.follow_all" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Return an iterable of <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instances to follow all links
in <code class="docutils literal notranslate"><span class="pre">urls</span></code>. It accepts the same arguments as <code class="docutils literal notranslate"><span class="pre">Request.__init__</span></code> method,
but elements of <code class="docutils literal notranslate"><span class="pre">urls</span></code> can be relative URLs or <a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> objects,
not only absolute URLs.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> provides a <a class="reference internal" href="#scrapy.http.TextResponse.follow_all" title="scrapy.http.TextResponse.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">follow_all()</span></code></a>
method which supports selectors in addition to absolute/relative URLs
and Link objects.</p>
</dd></dl>

</dd></dl>

</section>
<section id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h4>Response subclasses<a class="headerlink" href="#response-subclasses" title="Permalink to this heading">¶</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<section id="textresponse-objects">
<h5>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.TextResponse">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">TextResponse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects adds encoding capabilities to the base
<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support a new <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method argument, in
addition to the base <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> class and is not documented here.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> object with a string as
body, it will be converted to bytes encoded using this encoding. If
<em>encoding</em> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), the encoding will be looked up in the
response headers and body instead.</p>
</dd>
</dl>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> ones:</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.text">
<span class="sig-name descname"><span class="pre">text</span></span><a class="headerlink" href="#scrapy.http.TextResponse.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Response body, as a string.</p>
<p>The same as <code class="docutils literal notranslate"><span class="pre">response.body.decode(response.encoding)</span></code>, but the
result is cached after the first call, so you can access
<code class="docutils literal notranslate"><span class="pre">response.text</span></code> multiple times without extra overhead.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">str(response.body)</span></code> is not a correct way to convert the response
body into a string:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">str</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;body&quot;</span><span class="p">)</span>
<span class="go">&quot;b&#39;body&#39;&quot;</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.encoding">
<span class="sig-name descname"><span class="pre">encoding</span></span><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li><p>the encoding passed in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method <code class="docutils literal notranslate"><span class="pre">encoding</span></code> argument</p></li>
<li><p>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (i.e. unknown), it is ignored and the next
resolution mechanism is tried.</p></li>
<li><p>the encoding declared in the response body. The TextResponse class
doesn’t provide any special functionality for this. However, the
<a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> and <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> classes do.</p></li>
<li><p>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</p></li>
</ol>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.selector">
<span class="sig-name descname"><span class="pre">selector</span></span><a class="headerlink" href="#scrapy.http.TextResponse.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> instance using the response as
target. The selector is lazily instantiated on first access.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.attributes">
<span class="sig-name descname"><span class="pre">attributes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w">  </span><span class="p"><span class="pre">=</span></span><span class="w">  </span><span class="pre">('url',</span> <span class="pre">'status',</span> <span class="pre">'headers',</span> <span class="pre">'body',</span> <span class="pre">'flags',</span> <span class="pre">'request',</span> <span class="pre">'certificate',</span> <span class="pre">'ip_address',</span> <span class="pre">'protocol',</span> <span class="pre">'encoding')</span></em><a class="headerlink" href="#scrapy.http.TextResponse.attributes" title="Permalink to this definition">¶</a></dt>
<dd><p>A tuple of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> objects containing the name of all public
attributes of the class that are also keyword parameters of the
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>Currently used by <a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.replace()</span></code></a>.</p>
</dd></dl>

<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support the following methods in addition to
the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> ones:</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.jmespath">
<span class="sig-name descname"><span class="pre">jmespath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.jmespath" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal notranslate"><span class="pre">TextResponse.selector.jmespath(query)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">jmespath</span><span class="p">(</span><span class="s1">&#39;object.[*]&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.xpath">
<span class="sig-name descname"><span class="pre">xpath</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal notranslate"><span class="pre">TextResponse.selector.xpath(query)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.css">
<span class="sig-name descname"><span class="pre">css</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.css" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal notranslate"><span class="pre">TextResponse.selector.css(query)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.follow">
<span class="sig-name descname"><span class="pre">follow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><span class="pre">Link</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">parsel.Selector</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CallbackT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'GET'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cookies</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CookiesT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Failure</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a></span></span><a class="headerlink" href="#scrapy.http.TextResponse.follow" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal notranslate"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal notranslate"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal notranslate"><span class="pre">url</span></code> can be not only an absolute URL, but also</p>
<ul class="simple">
<li><p>a relative URL</p></li>
<li><p>a <a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> object, e.g. the result of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractors</span></a></p></li>
<li><p>a <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object for a <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> element, e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a.my_link')[0]</span></code></p></li>
<li><p>an attribute <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> (not SelectorList), e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a::attr(href)')[0]</span></code> or
<code class="docutils literal notranslate"><span class="pre">response.xpath('//img/&#64;src')[0]</span></code></p></li>
</ul>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#response-follow-example"><span class="std std-ref">A shortcut for creating Requests</span></a> for usage examples.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.follow_all">
<span class="sig-name descname"><span class="pre">follow_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">urls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><span class="pre">Link</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">parsel.SelectorList</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CallbackT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'GET'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Mapping</span><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">AnyStr</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cookies</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">CookiesT</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">meta</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_filter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errback</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Failure</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cb_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">css</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xpath</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.http.TextResponse.follow_all" title="Permalink to this definition">¶</a></dt>
<dd><p>A generator that produces <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instances to follow all
links in <code class="docutils literal notranslate"><span class="pre">urls</span></code>. It accepts the same arguments as the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>’s
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, except that each <code class="docutils literal notranslate"><span class="pre">urls</span></code> element does not need to be
an absolute URL, it can be any of the following:</p>
<ul class="simple">
<li><p>a relative URL</p></li>
<li><p>a <a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> object, e.g. the result of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractors</span></a></p></li>
<li><p>a <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object for a <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> element, e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a.my_link')[0]</span></code></p></li>
<li><p>an attribute <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> (not SelectorList), e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a::attr(href)')[0]</span></code> or
<code class="docutils literal notranslate"><span class="pre">response.xpath('//img/&#64;src')[0]</span></code></p></li>
</ul>
<p>In addition, <code class="docutils literal notranslate"><span class="pre">css</span></code> and <code class="docutils literal notranslate"><span class="pre">xpath</span></code> arguments are accepted to perform the link extraction
within the <code class="docutils literal notranslate"><span class="pre">follow_all</span></code> method (only one of <code class="docutils literal notranslate"><span class="pre">urls</span></code>, <code class="docutils literal notranslate"><span class="pre">css</span></code> and <code class="docutils literal notranslate"><span class="pre">xpath</span></code> is accepted).</p>
<p>Note that when passing a <code class="docutils literal notranslate"><span class="pre">SelectorList</span></code> as argument for the <code class="docutils literal notranslate"><span class="pre">urls</span></code> parameter or
using the <code class="docutils literal notranslate"><span class="pre">css</span></code> or <code class="docutils literal notranslate"><span class="pre">xpath</span></code> parameters, this method will not produce requests for
selectors from which links cannot be obtained (for instance, anchor tags without an
<code class="docutils literal notranslate"><span class="pre">href</span></code> attribute)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.json">
<span class="sig-name descname"><span class="pre">json</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.json" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<p>Deserialize a JSON document to a Python object.</p>
<p>Returns a Python object from deserialized JSON document.
The result is cached after the first call.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.http.TextResponse.urljoin">
<span class="sig-name descname"><span class="pre">urljoin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.urljoin" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs an absolute url by combining the Response’s base url with
a possible relative url. The base url shall be extracted from the
<code class="docutils literal notranslate"><span class="pre">&lt;base&gt;</span></code> tag, or just the Response’s <code class="xref py py-attr docutils literal notranslate"><span class="pre">url</span></code> if there is no such
tag.</p>
</dd></dl>

</dd></dl>

</section>
<section id="htmlresponse-objects">
<h5>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.HtmlResponse">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">HtmlResponse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="https://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</section>
<section id="xmlresponse-objects">
<h5>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.XmlResponse">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">XmlResponse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</section>
<section id="jsonresponse-objects">
<h5>JsonResponse objects<a class="headerlink" href="#jsonresponse-objects" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.http.JsonResponse">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.http.</span></span><span class="sig-name descname"><span class="pre">JsonResponse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">...</span></span></em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.JsonResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.JsonResponse" title="scrapy.http.JsonResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>
that is used when the response has a <a class="reference external" href="https://mimesniff.spec.whatwg.org/#json-mime-type">JSON MIME type</a> in its <cite>Content-Type</cite>
header.</p>
</dd></dl>

</section>
</section>
</section>
<span id="document-topics/link-extractors"></span><section id="link-extractors">
<span id="topics-link-extractors"></span><h3>Link Extractors<a class="headerlink" href="#link-extractors" title="Permalink to this heading">¶</a></h3>
<p>A link extractor is an object that extracts links from responses.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmlLinkExtractor</span></code></a> takes settings that
determine which links may be extracted. <a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmlLinkExtractor.extract_links</span></code></a> returns a
list of matching <a class="reference internal" href="#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> objects from a
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object.</p>
<p>Link extractors are used in <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a> spiders
through a set of <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> objects.</p>
<p>You can also use link extractors in regular spiders. For example, you can instantiate
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a> into a class
variable in your spider, and use it from your spider callbacks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">link_extractor</span><span class="o">.</span><span class="n">extract_links</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">link</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<section id="module-scrapy.linkextractors">
<span id="link-extractor-reference"></span><span id="topics-link-extractors-ref"></span><h4>Link extractor reference<a class="headerlink" href="#module-scrapy.linkextractors" title="Permalink to this heading">¶</a></h4>
<p>The link extractor class is
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor</span></code></a>. For convenience it
can also be imported as <code class="docutils literal notranslate"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="kn">import</span> <span class="n">LinkExtractor</span>
</pre></div>
</div>
<section id="module-scrapy.linkextractors.lxmlhtml">
<span id="lxmllinkextractor"></span><h5>LxmlLinkExtractor<a class="headerlink" href="#module-scrapy.linkextractors.lxmlhtml" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.linkextractors.lxmlhtml.</span></span><span class="sig-name descname"><span class="pre">LxmlLinkExtractor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">allow</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deny</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_domains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deny_domains</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deny_extensions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restrict_xpaths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">restrict_css</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tags</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('a',</span> <span class="pre">'area')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attrs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('href',)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">canonicalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unique</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml’s robust HTMLParser.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>allow</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</p></li>
<li><p><strong>deny</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (i.e. not
extracted). It has precedence over the <code class="docutils literal notranslate"><span class="pre">allow</span></code> parameter. If not
given (or empty) it won’t exclude any links.</p></li>
<li><p><strong>allow_domains</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a single value or a list of string containing
domains which will be considered for extracting the links</p></li>
<li><p><strong>deny_domains</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a single value or a list of strings containing
domains which won’t be considered for extracting the links</p></li>
<li><p><strong>deny_extensions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – <p>a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to
<code class="xref py py-data docutils literal notranslate"><span class="pre">scrapy.linkextractors.IGNORED_EXTENSIONS</span></code>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span><code class="xref py py-data docutils literal notranslate"><span class="pre">IGNORED_EXTENSIONS</span></code> now includes
<code class="docutils literal notranslate"><span class="pre">7z</span></code>, <code class="docutils literal notranslate"><span class="pre">7zip</span></code>, <code class="docutils literal notranslate"><span class="pre">apk</span></code>, <code class="docutils literal notranslate"><span class="pre">bz2</span></code>, <code class="docutils literal notranslate"><span class="pre">cdr</span></code>, <code class="docutils literal notranslate"><span class="pre">dmg</span></code>, <code class="docutils literal notranslate"><span class="pre">ico</span></code>,
<code class="docutils literal notranslate"><span class="pre">iso</span></code>, <code class="docutils literal notranslate"><span class="pre">tar</span></code>, <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code>, <code class="docutils literal notranslate"><span class="pre">webm</span></code>, and <code class="docutils literal notranslate"><span class="pre">xz</span></code>.</p>
</div>
</p></li>
<li><p><strong>restrict_xpaths</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – is an XPath (or list of XPath’s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links.</p></li>
<li><p><strong>restrict_css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a CSS selector (or list of selectors) which defines
regions inside the response where links should be extracted from.
Has the same behaviour as <code class="docutils literal notranslate"><span class="pre">restrict_xpaths</span></code>.</p></li>
<li><p><strong>restrict_text</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a single regular expression (or list of regular expressions)
that the link’s text must match in order to be extracted. If not
given (or empty), it will match all links. If a list of regular expressions is
given, the link will be extracted if it matches at least one.</p></li>
<li><p><strong>tags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – a tag or a list of tags to consider when extracting links.
Defaults to <code class="docutils literal notranslate"><span class="pre">('a',</span> <span class="pre">'area')</span></code>.</p></li>
<li><p><strong>attrs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <code class="docutils literal notranslate"><span class="pre">tags</span></code>
parameter). Defaults to <code class="docutils literal notranslate"><span class="pre">('href',)</span></code></p></li>
<li><p><strong>canonicalize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – canonicalize each extracted url (using
w3lib.url.canonicalize_url). Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note that canonicalize_url is meant for duplicate checking;
it can change the URL visible at server side, so the response can be
different for requests with canonicalized and raw URLs. If you’re
using LinkExtractor to follow links it is more robust to
keep the default <code class="docutils literal notranslate"><span class="pre">canonicalize=False</span></code>.</p></li>
<li><p><strong>unique</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether duplicate filtering should be applied to extracted
links.</p></li>
<li><p><strong>process_value</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>collections.abc.Callable</em></a>) – <p>a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return <code class="docutils literal notranslate"><span class="pre">None</span></code> to ignore the link altogether. If not
given, <code class="docutils literal notranslate"><span class="pre">process_value</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></code>.</p>
<p>For example, to extract links from this code:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;javascript:goToPage(&#39;../other/page.html&#39;); return false&quot;</span><span class="p">&gt;</span>Link text<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>You can use the following function in <code class="docutils literal notranslate"><span class="pre">process_value</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;javascript:goToPage\(&#39;(.*?)&#39;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</p></li>
<li><p><strong>strip</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether to strip whitespaces from extracted attributes.
According to HTML5 standard, leading and trailing whitespaces
must be stripped from <code class="docutils literal notranslate"><span class="pre">href</span></code> attributes of <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;area&gt;</span></code>
and many other elements, <code class="docutils literal notranslate"><span class="pre">src</span></code> attribute of <code class="docutils literal notranslate"><span class="pre">&lt;img&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;iframe&gt;</span></code>
elements, etc., so LinkExtractor strips space chars by default.
Set <code class="docutils literal notranslate"><span class="pre">strip=False</span></code> to turn it off (e.g. if you’re extracting urls
from elements or attributes which allow leading/trailing whitespaces).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">
<span class="sig-name descname"><span class="pre">extract_links</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><span class="pre">TextResponse</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><span class="pre">Link</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of <a class="reference internal" href="#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> objects from the
specified <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>Only links that match the settings passed to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of
the link extractor are returned.</p>
<p>Duplicate links are omitted if the <code class="docutils literal notranslate"><span class="pre">unique</span></code> attribute is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
otherwise they are returned.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-scrapy.link">
<span id="link"></span><h5>Link<a class="headerlink" href="#module-scrapy.link" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.link.Link">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.link.</span></span><span class="sig-name descname"><span class="pre">Link</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">text</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fragment</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nofollow</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.link.Link" title="Permalink to this definition">¶</a></dt>
<dd><p>Link objects represent an extracted link by the LinkExtractor.</p>
<p>Using the anchor tag sample below to illustrate the parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;https://example.com/nofollow.html#foo&quot;</span> <span class="n">rel</span><span class="o">=</span><span class="s2">&quot;nofollow&quot;</span><span class="o">&gt;</span><span class="n">Dont</span> <span class="n">follow</span> <span class="n">this</span> <span class="n">one</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> – the absolute url being linked to in the anchor tag.
From the sample, this is <code class="docutils literal notranslate"><span class="pre">https://example.com/nofollow.html</span></code>.</p></li>
<li><p><strong>text</strong> – the text in the anchor tag. From the sample, this is <code class="docutils literal notranslate"><span class="pre">Dont</span> <span class="pre">follow</span> <span class="pre">this</span> <span class="pre">one</span></code>.</p></li>
<li><p><strong>fragment</strong> – the part of the url after the hash symbol. From the sample, this is <code class="docutils literal notranslate"><span class="pre">foo</span></code>.</p></li>
<li><p><strong>nofollow</strong> – an indication of the presence or absence of a nofollow value in the <code class="docutils literal notranslate"><span class="pre">rel</span></code> attribute
of the anchor tag.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<span id="document-topics/settings"></span><section id="settings">
<span id="topics-settings"></span><h3>Settings<a class="headerlink" href="#settings" title="Permalink to this heading">¶</a></h3>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
<section id="designating-the-settings">
<span id="topics-settings-module-envvar"></span><h4>Designating the settings<a class="headerlink" href="#designating-the-settings" title="Permalink to this heading">¶</a></h4>
<p>When you use Scrapy, you have to tell it which settings you’re using. You can
do this by using an environment variable, <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code>.</p>
<p>The value of <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> should be in Python path syntax, e.g.
<code class="docutils literal notranslate"><span class="pre">myproject.settings</span></code>. Note that the settings module should be on the
Python <a class="reference external" href="https://docs.python.org/3/tutorial/modules.html#tut-searchpath" title="(in Python v3.13)"><span class="xref std std-ref">import search path</span></a>.</p>
</section>
<section id="populating-the-settings">
<span id="populating-settings"></span><h4>Populating the settings<a class="headerlink" href="#populating-the-settings" title="Permalink to this heading">¶</a></h4>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Command line options (most precedence)</p></li>
<li><p>Settings per-spider</p></li>
<li><p>Project settings module</p></li>
<li><p>Settings set by add-ons</p></li>
<li><p>Default settings per-command</p></li>
<li><p>Default global settings (less precedence)</p></li>
</ol>
</div></blockquote>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-settings"><span class="std std-ref">Settings API</span></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<section id="command-line-options">
<h5>1. Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this heading">¶</a></h5>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <code class="docutils literal notranslate"><span class="pre">-s</span></code> (or <code class="docutils literal notranslate"><span class="pre">--set</span></code>) command line option.</p>
<p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>scrapy<span class="w"> </span>crawl<span class="w"> </span>myspider<span class="w"> </span>-s<span class="w"> </span><span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</section>
<section id="settings-per-spider">
<h5>2. Settings per-spider<a class="headerlink" href="#settings-per-spider" title="Permalink to this heading">¶</a></h5>
<p>Spiders (See the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a> chapter for reference) can define their
own settings that will take precedence and override the project ones. One way
to do so is by setting their <a class="reference internal" href="index.html#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="n">custom_settings</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;SOME_SETTING&quot;</span><span class="p">:</span> <span class="s2">&quot;some value&quot;</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>It’s often better to implement <a class="reference internal" href="index.html#scrapy.Spider.update_settings" title="scrapy.Spider.update_settings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update_settings()</span></code></a> instead,
and settings set there should use the “spider” priority explicitly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">update_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
        <span class="n">settings</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;SOME_SETTING&quot;</span><span class="p">,</span> <span class="s2">&quot;some value&quot;</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="s2">&quot;spider&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.11.</span></p>
</div>
<p>It’s also possible to modify the settings in the
<a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> method, e.g. based on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">spider
arguments</span></a> or other logic:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">spider</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;some_argument&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">spider</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
                <span class="s2">&quot;SOME_SETTING&quot;</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;some_argument&quot;</span><span class="p">],</span> <span class="n">priority</span><span class="o">=</span><span class="s2">&quot;spider&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">spider</span>
</pre></div>
</div>
</section>
<section id="project-settings-module">
<h5>3. Project settings module<a class="headerlink" href="#project-settings-module" title="Permalink to this heading">¶</a></h5>
<p>The project settings module is the standard configuration file for your Scrapy
project, it’s where most of your custom settings will be populated. For a
standard Scrapy project, this means you’ll be adding or changing the settings
in the <code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file created for your project.</p>
</section>
<section id="settings-set-by-add-ons">
<h5>4. Settings set by add-ons<a class="headerlink" href="#settings-set-by-add-ons" title="Permalink to this heading">¶</a></h5>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-addons"><span class="std std-ref">Add-ons</span></a> can modify settings. They should do this with
this priority, though this is not enforced.</p>
</section>
<section id="default-settings-per-command">
<h5>5. Default settings per-command<a class="headerlink" href="#default-settings-per-command" title="Permalink to this heading">¶</a></h5>
<p>Each <a class="reference internal" href="index.html#document-topics/commands"><span class="doc">Scrapy tool</span></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <code class="docutils literal notranslate"><span class="pre">default_settings</span></code> attribute of the command
class.</p>
</section>
<section id="default-global-settings">
<h5>6. Default global settings<a class="headerlink" href="#default-global-settings" title="Permalink to this heading">¶</a></h5>
<p>The global defaults are located in the <code class="docutils literal notranslate"><span class="pre">scrapy.settings.default_settings</span></code>
module and documented in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> section.</p>
</section>
</section>
<section id="compatibility-with-pickle">
<h4>Compatibility with pickle<a class="headerlink" href="#compatibility-with-pickle" title="Permalink to this heading">¶</a></h4>
<p>Setting values must be <a class="reference external" href="https://docs.python.org/3/library/pickle.html#pickle-picklable" title="(in Python v3.13)"><span class="xref std std-ref">picklable</span></a>.</p>
</section>
<section id="import-paths-and-classes">
<h4>Import paths and classes<a class="headerlink" href="#import-paths-and-classes" title="Permalink to this heading">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4.0.</span></p>
</div>
<p>When a setting references a callable object to be imported by Scrapy, such as a
class or a function, there are two different ways you can specify that object:</p>
<ul class="simple">
<li><p>As a string containing the import path of that object</p></li>
<li><p>As the object itself</p></li>
</ul>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mybot.pipelines.validate</span> <span class="kn">import</span> <span class="n">ValidateMyItem</span>

<span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># passing the classname...</span>
    <span class="n">ValidateMyItem</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="c1"># ...equals passing the class path</span>
    <span class="s2">&quot;mybot.pipelines.validate.ValidateMyItem&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing non-callable objects is not supported.</p>
</div>
</section>
<section id="how-to-access-settings">
<h4>How to access settings<a class="headerlink" href="#how-to-access-settings" title="Permalink to this heading">¶</a></h4>
<p>In a spider, the settings are available through <code class="docutils literal notranslate"><span class="pre">self.settings</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;http://example.com&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Existing settings: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">attributes</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">settings</span></code> attribute is set in the base Spider class after the spider
is initialized.  If you want to use the settings before the initialization
(e.g., in your spider’s <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method), you’ll need to override the
<a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> method.</p>
</div>
<p>Settings can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler.settings</span></code></a>
attribute of the Crawler that is passed to <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> method in
extensions, middlewares and item pipelines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_is_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">log_is_enabled</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;log is enabled!&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s2">&quot;LOG_ENABLED&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>The settings object can be used like a dict (e.g.,
<code class="docutils literal notranslate"><span class="pre">settings['LOG_ENABLED']</span></code>), but it’s usually preferred to extract the setting
in the format you need it to avoid type errors, using one of the methods
provided by the <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> API.</p>
</section>
<section id="rationale-for-setting-names">
<h4>Rationale for setting names<a class="headerlink" href="#rationale-for-setting-names" title="Permalink to this heading">¶</a></h4>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_ENABLED</span></code>, <code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code>, <code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_CACHEDIR</span></code>, etc.</p>
</section>
<section id="built-in-settings-reference">
<span id="topics-settings-ref"></span><h4>Built-in settings reference<a class="headerlink" href="#built-in-settings-reference" title="Permalink to this heading">¶</a></h4>
<p>Here’s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it’s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<section id="addons">
<span id="std-setting-ADDONS"></span><h5>ADDONS<a class="headerlink" href="#addons" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing paths to the add-ons enabled in your project and their
priorities. For more information, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-addons"><span class="std std-ref">Add-ons</span></a>.</p>
</section>
<section id="aws-access-key-id">
<span id="std-setting-AWS_ACCESS_KEY_ID"></span><h5>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</section>
<section id="aws-secret-access-key">
<span id="std-setting-AWS_SECRET_ACCESS_KEY"></span><h5>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</section>
<section id="aws-session-token">
<span id="std-setting-AWS_SESSION_TOKEN"></span><h5>AWS_SESSION_TOKEN<a class="headerlink" href="#aws-session-token" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The AWS security token used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>, when using
<a class="reference external" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html">temporary security credentials</a>.</p>
</section>
<section id="aws-endpoint-url">
<span id="std-setting-AWS_ENDPOINT_URL"></span><h5>AWS_ENDPOINT_URL<a class="headerlink" href="#aws-endpoint-url" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Endpoint URL used for S3-like storage, for example Minio or s3.scality.</p>
</section>
<section id="aws-use-ssl">
<span id="std-setting-AWS_USE_SSL"></span><h5>AWS_USE_SSL<a class="headerlink" href="#aws-use-ssl" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Use this option if you want to disable SSL connection for communication with
S3 or S3-like storage. By default SSL will be used.</p>
</section>
<section id="aws-verify">
<span id="std-setting-AWS_VERIFY"></span><h5>AWS_VERIFY<a class="headerlink" href="#aws-verify" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Verify SSL connection between Scrapy and S3 or S3-like storage. By default
SSL verification will occur.</p>
</section>
<section id="aws-region-name">
<span id="std-setting-AWS_REGION_NAME"></span><h5>AWS_REGION_NAME<a class="headerlink" href="#aws-region-name" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The name of the region associated with the AWS client.</p>
</section>
<section id="asyncio-event-loop">
<span id="std-setting-ASYNCIO_EVENT_LOOP"></span><h5>ASYNCIO_EVENT_LOOP<a class="headerlink" href="#asyncio-event-loop" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Import path of a given <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> event loop class.</p>
<p>If the asyncio reactor is enabled (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a>) this setting can be used to specify the
asyncio event loop to be used with it. Set the setting to the import path of the
desired asyncio event loop class. If the setting is set to <code class="docutils literal notranslate"><span class="pre">None</span></code> the default asyncio
event loop will be used.</p>
<p>If you are installing the asyncio reactor manually using the <a class="reference internal" href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>
function, you can use the <code class="docutils literal notranslate"><span class="pre">event_loop_path</span></code> parameter to indicate the import path of the event loop
class to be used.</p>
<p>Note that the event loop class must inherit from <a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">asyncio.AbstractEventLoop</span></code></a>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Please be aware that, when using a non-default event loop
(either defined via <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-ASYNCIO_EVENT_LOOP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ASYNCIO_EVENT_LOOP</span></code></a> or installed with
<a class="reference internal" href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>), Scrapy will call
<a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.set_event_loop" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">asyncio.set_event_loop()</span></code></a>, which will set the specified event loop
as the current loop for the current OS thread.</p>
</div>
</section>
<section id="bot-name">
<span id="std-setting-BOT_NAME"></span><h5>BOT_NAME<a class="headerlink" href="#bot-name" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapybot'</span></code></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This name will be used for the logging too.</p>
<p>It’s automatically populated with your project name when you create your
project with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command.</p>
</section>
<section id="concurrent-items">
<span id="std-setting-CONCURRENT_ITEMS"></span><h5>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">100</span></code></p>
<p>Maximum number of concurrent items (per response) to process in parallel in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a>.</p>
</section>
<section id="concurrent-requests">
<span id="std-setting-CONCURRENT_REQUESTS"></span><h5>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">16</span></code></p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
</section>
<section id="concurrent-requests-per-domain">
<span id="std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h5>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">8</span></code></p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will be
performed to any single domain.</p>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a> and its
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> option.</p>
</section>
<section id="concurrent-requests-per-ip">
<span id="std-setting-CONCURRENT_REQUESTS_PER_IP"></span><h5>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a>: if <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>
is non-zero, download delay is enforced per IP, not per domain.</p>
</section>
<section id="default-item-class">
<span id="std-setting-DEFAULT_ITEM_CLASS"></span><h5>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.Item'</span></code></p>
<p>The default class that will be used for instantiating items in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">the
Scrapy shell</span></a>.</p>
</section>
<section id="default-request-headers">
<span id="std-setting-DEFAULT_REQUEST_HEADERS"></span><h5>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;Accept&quot;</span><span class="p">:</span> <span class="s2">&quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Accept-Language&quot;</span><span class="p">:</span> <span class="s2">&quot;en&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default headers used for Scrapy HTTP Requests. They’re populated in the
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultHeadersMiddleware</span></code></a>.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Cookies set via the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header are not considered by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code> parameter. This is a known
current limitation that is being worked on.</p>
</div>
</section>
<section id="depth-limit">
<span id="std-setting-DEPTH_LIMIT"></span><h5>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
</section>
<section id="depth-priority">
<span id="std-setting-DEPTH_PRIORITY"></span><h5>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>An integer that is used to adjust the <code class="xref py py-attr docutils literal notranslate"><span class="pre">priority</span></code> of
a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> based on its depth.</p>
<p>The priority of a request is adjusted as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">request</span><span class="o">.</span><span class="n">priority</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">priority</span> <span class="o">-</span> <span class="p">(</span><span class="n">depth</span> <span class="o">*</span> <span class="n">DEPTH_PRIORITY</span><span class="p">)</span>
</pre></div>
</div>
<p>As depth increases, positive values of <code class="docutils literal notranslate"><span class="pre">DEPTH_PRIORITY</span></code> decrease request
priority (BFO), while negative values increase request priority (DFO). See
also <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Does Scrapy crawl in breadth-first or depth-first order?</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This setting adjusts priority <strong>in the opposite way</strong> compared to
other priority settings <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-REDIRECT_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_PRIORITY_ADJUST</span></code></a>
and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a>.</p>
</div>
</section>
<section id="depth-stats-verbose">
<span id="std-setting-DEPTH_STATS_VERBOSE"></span><h5>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
</section>
<section id="dnscache-enabled">
<span id="std-setting-DNSCACHE_ENABLED"></span><h5>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable DNS in-memory cache.</p>
</section>
<section id="dnscache-size">
<span id="std-setting-DNSCACHE_SIZE"></span><h5>DNSCACHE_SIZE<a class="headerlink" href="#dnscache-size" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">10000</span></code></p>
<p>DNS in-memory cache size.</p>
</section>
<section id="dns-resolver">
<span id="std-setting-DNS_RESOLVER"></span><h5>DNS_RESOLVER<a class="headerlink" href="#dns-resolver" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.resolver.CachingThreadedResolver'</span></code></p>
<p>The class to be used to resolve DNS names. The default <code class="docutils literal notranslate"><span class="pre">scrapy.resolver.CachingThreadedResolver</span></code>
supports specifying a timeout for DNS requests via the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DNS_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code></a> setting,
but works only with IPv4 addresses. Scrapy provides an alternative resolver,
<code class="docutils literal notranslate"><span class="pre">scrapy.resolver.CachingHostnameResolver</span></code>, which supports IPv4/IPv6 addresses but does not
take the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DNS_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code></a> setting into account.</p>
</section>
<section id="dns-timeout">
<span id="std-setting-DNS_TIMEOUT"></span><h5>DNS_TIMEOUT<a class="headerlink" href="#dns-timeout" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60</span></code></p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.</p>
</section>
<section id="downloader">
<span id="std-setting-DOWNLOADER"></span><h5>DOWNLOADER<a class="headerlink" href="#downloader" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.Downloader'</span></code></p>
<p>The downloader to use for crawling.</p>
</section>
<section id="downloader-httpclientfactory">
<span id="std-setting-DOWNLOADER_HTTPCLIENTFACTORY"></span><h5>DOWNLOADER_HTTPCLIENTFACTORY<a class="headerlink" href="#downloader-httpclientfactory" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</span></code></p>
<p>Defines a Twisted <code class="docutils literal notranslate"><span class="pre">protocol.ClientFactory</span></code>  class to use for HTTP/1.0
connections (for <code class="docutils literal notranslate"><span class="pre">HTTP10DownloadHandler</span></code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you really want to use HTTP/1.0 and override
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> for <code class="docutils literal notranslate"><span class="pre">http(s)</span></code> scheme accordingly,
i.e. to <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</span></code>.</p>
</div>
</section>
<section id="downloader-clientcontextfactory">
<span id="std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"></span><h5>DOWNLOADER_CLIENTCONTEXTFACTORY<a class="headerlink" href="#downloader-clientcontextfactory" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</span></code></p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, “ContextFactory” is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate verification,
or even enable client-side authentication (and various other things).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p>If you do need remote server certificate verification enabled,
Scrapy also has another context factory class that you can set,
<code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</span></code>,
which uses the platform’s certificates to validate remote endpoints.</p>
</div>
<p>If you do use a custom ContextFactory, make sure its <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method
accepts a <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter (this is the <code class="docutils literal notranslate"><span class="pre">OpenSSL.SSL</span></code> method mapping
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>), a <code class="docutils literal notranslate"><span class="pre">tls_verbose_logging</span></code>
parameter (<code class="docutils literal notranslate"><span class="pre">bool</span></code>) and a <code class="docutils literal notranslate"><span class="pre">tls_ciphers</span></code> parameter (see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a>).</p>
</section>
<section id="downloader-client-tls-ciphers">
<span id="std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"></span><h5>DOWNLOADER_CLIENT_TLS_CIPHERS<a class="headerlink" href="#downloader-client-tls-ciphers" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'DEFAULT'</span></code></p>
<p>Use  this setting to customize the TLS/SSL ciphers used by the default
HTTP/1.1 downloader.</p>
<p>The setting should contain a string in the <a class="reference external" href="https://docs.openssl.org/master/man1/openssl-ciphers/#cipher-list-format">OpenSSL cipher list format</a>,
these ciphers will be used as client ciphers. Changing this setting may be
necessary to access certain HTTPS websites: for example, you may need to use
<code class="docutils literal notranslate"><span class="pre">'DEFAULT:!DH'</span></code> for a website with weak DH parameters or enable a
specific cipher that is not included in <code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code> if a website requires it.</p>
</section>
<section id="downloader-client-tls-method">
<span id="std-setting-DOWNLOADER_CLIENT_TLS_METHOD"></span><h5>DOWNLOADER_CLIENT_TLS_METHOD<a class="headerlink" href="#downloader-client-tls-method" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'TLS'</span></code></p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'TLS'</span></code>: maps to OpenSSL’s <code class="docutils literal notranslate"><span class="pre">TLS_method()</span></code> (a.k.a <code class="docutils literal notranslate"><span class="pre">SSLv23_method()</span></code>),
which allows protocol negotiation, starting from the highest supported
by the platform; <strong>default, recommended</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'TLSv1.0'</span></code>: this value forces HTTPS connections to use TLS version 1.0 ;
set this if you want the behavior of Scrapy&lt;1.1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'TLSv1.1'</span></code>: forces TLS version 1.1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'TLSv1.2'</span></code>: forces TLS version 1.2</p></li>
</ul>
</section>
<section id="downloader-client-tls-verbose-logging">
<span id="std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"></span><h5>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING<a class="headerlink" href="#downloader-client-tls-verbose-logging" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Setting this to <code class="docutils literal notranslate"><span class="pre">True</span></code> will enable DEBUG level messages about TLS connection
parameters after establishing HTTPS connections. The kind of information logged
depends on the versions of OpenSSL and pyOpenSSL.</p>
<p>This setting is only used for the default
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENTCONTEXTFACTORY</span></code></a>.</p>
</section>
<section id="downloader-middlewares">
<span id="std-setting-DOWNLOADER_MIDDLEWARES"></span><h5>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="Permalink to this heading">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</section>
<section id="downloader-middlewares-base">
<span id="std-setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h5>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.offsite.OffsiteMiddleware&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;</span><span class="p">:</span> <span class="mi">560</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. Low
orders are closer to the engine, high orders are closer to the downloader. You
should never modify this setting in your project, modify
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> instead.  For more info see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</section>
<section id="downloader-stats">
<span id="std-setting-DOWNLOADER_STATS"></span><h5>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable downloader stats collection.</p>
</section>
<section id="download-delay">
<span id="std-setting-DOWNLOAD_DELAY"></span><h5>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Minimum seconds to wait between 2 consecutive requests to the same domain.</p>
<p>Use <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> to throttle your crawling speed, to avoid hitting
servers too hard.</p>
<p>Decimal numbers are supported. For example, to send a maximum of 4 requests
every 10 seconds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">2.5</span>
</pre></div>
</div>
<p>This setting is also affected by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>
setting, which is enabled by default.</p>
<p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> is non-zero, delays are enforced
per IP address instead of per domain.</p>
<p>Note that <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> can lower the effective per-domain
concurrency below <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>. If the response
time of a domain is lower than <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>, the effective
concurrency for that domain is 1. When testing throttling configurations, it
usually makes sense to lower <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> first,
and only increase <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> once
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> is 1 but a higher throttling is
desired.</p>
<div class="admonition note" id="spider-download-delay-attribute">
<p class="admonition-title">Note</p>
<p>This delay can be set per spider using <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_delay</span></code> spider attribute.</p>
</div>
<p>It is also possible to change this setting per domain, although it requires
non-trivial code. See the implementation of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle</span></a> extension for an example.</p>
</section>
<section id="download-handlers">
<span id="std-setting-DOWNLOAD_HANDLERS"></span><h5>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for example format.</p>
</section>
<section id="download-handlers-base">
<span id="std-setting-DOWNLOAD_HANDLERS_BASE"></span><h5>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;file&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.file.FileDownloadHandler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;http&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;s3&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.s3.S3DownloadHandler&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ftp&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> instead.</p>
<p>You can disable any of these download handlers by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their
URI scheme in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>. E.g., to disable the built-in FTP
handler (without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;ftp&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p id="http2">The default HTTPS handler uses HTTP/1.1. To use HTTP/2:</p>
<ol class="arabic">
<li><p>Install <code class="docutils literal notranslate"><span class="pre">Twisted[http2]&gt;=17.9.0</span></code> to install the packages required to
enable HTTP/2 support in Twisted.</p></li>
<li><p>Update <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;https&quot;</span><span class="p">:</span> <span class="s2">&quot;scrapy.core.downloader.handlers.http2.H2DownloadHandler&quot;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>HTTP/2 support in Scrapy is experimental, and not yet recommended for
production environments. Future Scrapy versions may introduce related
changes without a deprecation period or warning.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Known limitations of the current HTTP/2 implementation of Scrapy include:</p>
<ul class="simple">
<li><p>No support for HTTP/2 Cleartext (h2c), since no major browser supports
HTTP/2 unencrypted (refer <a class="reference external" href="https://http2.github.io/faq/#does-http2-require-encryption">http2 faq</a>).</p></li>
<li><p>No setting to specify a maximum <a class="reference external" href="https://datatracker.ietf.org/doc/html/rfc7540#section-4.2">frame size</a> larger than the default
value, 16384. Connections to servers that send a larger frame will
fail.</p></li>
<li><p>No support for <a class="reference external" href="https://datatracker.ietf.org/doc/html/rfc7540#section-8.2">server pushes</a>, which are ignored.</p></li>
<li><p>No support for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-bytes_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">bytes_received</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-headers_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">headers_received</span></code></a> signals.</p></li>
</ul>
</div>
</section>
<section id="download-slots">
<span id="std-setting-DOWNLOAD_SLOTS"></span><h5>DOWNLOAD_SLOTS<a class="headerlink" href="#download-slots" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>Allows to define concurrency/delay parameters on per slot (domain) basis:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_SLOTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;quotes.toscrape.com&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;concurrency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;delay&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;randomize_delay&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
    <span class="s2">&quot;books.toscrape.com&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;delay&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;randomize_delay&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For other downloader slots default settings values will be used:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>: <code class="docutils literal notranslate"><span class="pre">delay</span></code></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>: <code class="docutils literal notranslate"><span class="pre">concurrency</span></code></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>: <code class="docutils literal notranslate"><span class="pre">randomize_delay</span></code></p></li>
</ul>
</div>
</section>
<section id="download-timeout">
<span id="std-setting-DOWNLOAD_TIMEOUT"></span><h5>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">180</span></code></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This timeout can be set per spider using <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_timeout</span></code>
spider attribute and per-request using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a>
Request.meta key.</p>
</div>
<span class="target" id="std-setting-DOWNLOAD_MAXSIZE"></span></section>
<section id="download-maxsize">
<span id="std-reqmeta-download_maxsize"></span><h5>DOWNLOAD_MAXSIZE<a class="headerlink" href="#download-maxsize" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">1073741824</span></code> (1 GiB)</p>
<p>The maximum response body size (in bytes) allowed. Bigger responses are
aborted and ignored.</p>
<p>This applies both before and after compression. If decompressing a response
body would exceed this limit, decompression is aborted and the response is
ignored.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">0</span></code> to disable this limit.</p>
<p>This limit can be set per spider using the <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_maxsize</span></code> spider
attribute and per request using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_maxsize</span></code></a> Request.meta
key.</p>
<span class="target" id="std-setting-DOWNLOAD_WARNSIZE"></span></section>
<section id="download-warnsize">
<span id="std-reqmeta-download_warnsize"></span><h5>DOWNLOAD_WARNSIZE<a class="headerlink" href="#download-warnsize" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">33554432</span></code> (32 MiB)</p>
<p>If the size of a response exceeds this value, before or after compression, a
warning will be logged about it.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">0</span></code> to disable this limit.</p>
<p>This limit can be set per spider using the <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_warnsize</span></code> spider
attribute and per request using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-download_warnsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_warnsize</span></code></a> Request.meta
key.</p>
</section>
<section id="download-fail-on-dataloss">
<span id="std-setting-DOWNLOAD_FAIL_ON_DATALOSS"></span><h5>DOWNLOAD_FAIL_ON_DATALOSS<a class="headerlink" href="#download-fail-on-dataloss" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to fail on broken responses, that is, declared
<code class="docutils literal notranslate"><span class="pre">Content-Length</span></code> does not match content sent by the server or chunked
response was not properly finish. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, these responses raise a
<code class="docutils literal notranslate"><span class="pre">ResponseFailed([_DataLoss])</span></code> error. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, these responses
are passed through and the flag <code class="docutils literal notranslate"><span class="pre">dataloss</span></code> is added to the response, i.e.:
<code class="docutils literal notranslate"><span class="pre">'dataloss'</span> <span class="pre">in</span> <span class="pre">response.flags</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>Optionally, this can be set per-request basis by using the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-download_fail_on_dataloss"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_fail_on_dataloss</span></code></a> Request.meta key to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A broken response, or data loss error, may happen under several
circumstances, from server misconfiguration to network errors to data
corruption. It is up to the user to decide if it makes sense to process
broken responses considering they may contain partial or incomplete content.
If <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code> and this setting is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the <code class="docutils literal notranslate"><span class="pre">ResponseFailed([_DataLoss])</span></code> failure will be retried as usual.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This setting is ignored by the
<code class="xref py py-class docutils literal notranslate"><span class="pre">H2DownloadHandler</span></code>
download handler (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>). In case of a data loss
error, the corresponding HTTP/2 connection may be corrupted, affecting other
requests that use the same connection; hence, a <code class="docutils literal notranslate"><span class="pre">ResponseFailed([InvalidBodyLengthError])</span></code>
failure is always raised for every request that was using that connection.</p>
</div>
</section>
<section id="dupefilter-class">
<span id="std-setting-DUPEFILTER_CLASS"></span><h5>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.dupefilters.RFPDupeFilter'</span></code></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code>) filters based on the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_CLASS</span></code></a> setting.</p>
<p>You can disable filtering of duplicate requests by setting
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> to <code class="docutils literal notranslate"><span class="pre">'scrapy.dupefilters.BaseDupeFilter'</span></code>.
Be very careful about this however, because you can get into crawling loops.
It’s usually a better idea to set the <code class="docutils literal notranslate"><span class="pre">dont_filter</span></code> parameter to
<code class="docutils literal notranslate"><span class="pre">True</span></code> on the specific <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> that should not be
filtered.</p>
</section>
<section id="dupefilter-debug">
<span id="std-setting-DUPEFILTER_DEBUG"></span><h5>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> only logs the first duplicate request.
Setting <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DUPEFILTER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_DEBUG</span></code></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code> will make it log all duplicate requests.</p>
</section>
<section id="editor">
<span id="std-setting-EDITOR"></span><h5>EDITOR<a class="headerlink" href="#editor" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">vi</span></code> (on Unix systems) or the IDLE editor (on Windows)</p>
<p>The editor to use for editing spiders with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a> command.
Additionally, if the <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment variable is set, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a>
command will prefer it over the default setting.</p>
</section>
<section id="extensions">
<span id="std-setting-EXTENSIONS"></span><h5>EXTENSIONS<a class="headerlink" href="#extensions" title="Permalink to this heading">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
</section>
<section id="extensions-base">
<span id="std-setting-EXTENSIONS_BASE"></span><h5>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;scrapy.extensions.corestats.CoreStats&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.telnet.TelnetConsole&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.memusage.MemoryUsage&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.memdebug.MemoryDebugger&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.closespider.CloseSpider&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.feedexport.FeedExporter&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.logstats.LogStats&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.spiderstate.SpiderState&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.throttle.AutoThrottle&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the extensions available by default in Scrapy, and their
orders. This setting contains all stable built-in extensions. Keep in mind that
some of them need to be enabled through a setting.</p>
<p>For more information See the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions user guide</span></a>
and the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref"><span class="std std-ref">list of available extensions</span></a>.</p>
</section>
<section id="feed-tempdir">
<span id="std-setting-FEED_TEMPDIR"></span><h5>FEED_TEMPDIR<a class="headerlink" href="#feed-tempdir" title="Permalink to this heading">¶</a></h5>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-ftp"><span class="std std-ref">FTP feed storage</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">Amazon S3</span></a>.</p>
</section>
<section id="feed-storage-gcs-acl">
<span id="std-setting-FEED_STORAGE_GCS_ACL"></span><h5>FEED_STORAGE_GCS_ACL<a class="headerlink" href="#feed-storage-gcs-acl" title="Permalink to this heading">¶</a></h5>
<p>The Access Control List (ACL) used when storing items to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-gcs"><span class="std std-ref">Google Cloud Storage</span></a>.
For more information on how to set this value, please refer to the column <em>JSON API</em> in <a class="reference external" href="https://cloud.google.com/storage/docs/access-control/lists">Google Cloud documentation</a>.</p>
</section>
<section id="ftp-passive-mode">
<span id="std-setting-FTP_PASSIVE_MODE"></span><h5>FTP_PASSIVE_MODE<a class="headerlink" href="#ftp-passive-mode" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to use passive mode when initiating FTP transfers.</p>
<span class="target" id="std-reqmeta-ftp_password"></span></section>
<section id="ftp-password">
<span id="std-setting-FTP_PASSWORD"></span><h5>FTP_PASSWORD<a class="headerlink" href="#ftp-password" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;guest&quot;</span></code></p>
<p>The password to use for FTP connections when there is no <code class="docutils literal notranslate"><span class="pre">&quot;ftp_password&quot;</span></code>
in <code class="docutils literal notranslate"><span class="pre">Request</span></code> meta.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Paraphrasing <a class="reference external" href="https://datatracker.ietf.org/doc/html/rfc1635">RFC 1635</a>, although it is common to use either the password
“guest” or one’s e-mail address for anonymous FTP,
some FTP servers explicitly ask for the user’s e-mail address
and will not allow login with the “guest” password.</p>
</div>
<span class="target" id="std-reqmeta-ftp_user"></span></section>
<section id="ftp-user">
<span id="std-setting-FTP_USER"></span><h5>FTP_USER<a class="headerlink" href="#ftp-user" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;anonymous&quot;</span></code></p>
<p>The username to use for FTP connections when there is no <code class="docutils literal notranslate"><span class="pre">&quot;ftp_user&quot;</span></code>
in <code class="docutils literal notranslate"><span class="pre">Request</span></code> meta.</p>
</section>
<section id="gcs-project-id">
<span id="std-setting-GCS_PROJECT_ID"></span><h5>GCS_PROJECT_ID<a class="headerlink" href="#gcs-project-id" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The Project ID that will be used when storing data on <a class="reference external" href="https://cloud.google.com/storage/">Google Cloud Storage</a>.</p>
</section>
<section id="item-pipelines">
<span id="std-setting-ITEM_PIPELINES"></span><h5>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the item pipelines to use, and their orders. Order values are
arbitrary, but it is customary to define them in the 0-1000 range. Lower orders
process before higher orders.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;mybot.pipelines.validate.ValidateMyItem&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s2">&quot;mybot.pipelines.validate.StoreMyItem&quot;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="item-pipelines-base">
<span id="std-setting-ITEM_PIPELINES_BASE"></span><h5>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> instead.</p>
</section>
<section id="jobdir">
<span id="std-setting-JOBDIR"></span><h5>JOBDIR<a class="headerlink" href="#jobdir" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>A string indicating the directory for storing the state of a crawl when
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">pausing and resuming crawls</span></a>.</p>
</section>
<section id="log-enabled">
<span id="std-setting-LOG_ENABLED"></span><h5>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable logging.</p>
</section>
<section id="log-encoding">
<span id="std-setting-LOG_ENCODING"></span><h5>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'utf-8'</span></code></p>
<p>The encoding to use for logging.</p>
</section>
<section id="log-file">
<span id="std-setting-LOG_FILE"></span><h5>LOG_FILE<a class="headerlink" href="#log-file" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>File name to use for logging output. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, standard error will be used.</p>
</section>
<section id="log-file-append">
<span id="std-setting-LOG_FILE_APPEND"></span><h5>LOG_FILE_APPEND<a class="headerlink" href="#log-file-append" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the log file specified with <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> will be
overwritten (discarding the output from previous runs, if any).</p>
</section>
<section id="log-format">
<span id="std-setting-LOG_FORMAT"></span><h5>LOG_FORMAT<a class="headerlink" href="#log-format" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'%(asctime)s</span> <span class="pre">[%(name)s]</span> <span class="pre">%(levelname)s:</span> <span class="pre">%(message)s'</span></code></p>
<p>String for formatting log messages. Refer to the
<a class="reference external" href="https://docs.python.org/3/library/logging.html#logrecord-attributes" title="(in Python v3.13)"><span class="xref std std-ref">Python logging documentation</span></a> for the whole
list of available placeholders.</p>
</section>
<section id="log-dateformat">
<span id="std-setting-LOG_DATEFORMAT"></span><h5>LOG_DATEFORMAT<a class="headerlink" href="#log-dateformat" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'%Y-%m-%d</span> <span class="pre">%H:%M:%S'</span></code></p>
<p>String for formatting date/time, expansion of the <code class="docutils literal notranslate"><span class="pre">%(asctime)s</span></code> placeholder
in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a>. Refer to the
<a class="reference external" href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior" title="(in Python v3.13)"><span class="xref std std-ref">Python datetime documentation</span></a> for the
whole list of available directives.</p>
</section>
<section id="log-formatter">
<span id="std-setting-LOG_FORMATTER"></span><h5>LOG_FORMATTER<a class="headerlink" href="#log-formatter" title="Permalink to this heading">¶</a></h5>
<p>Default: <a class="reference internal" href="index.html#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.logformatter.LogFormatter</span></code></a></p>
<p>The class to use for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#custom-log-formats"><span class="std std-ref">formatting log messages</span></a> for different actions.</p>
</section>
<section id="log-level">
<span id="std-setting-LOG_LEVEL"></span><h5>LOG_LEVEL<a class="headerlink" href="#log-level" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'DEBUG'</span></code></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a>.</p>
</section>
<section id="log-stdout">
<span id="std-setting-LOG_STDOUT"></span><h5>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, all standard output (and error) of your process will be redirected
to the log. For example if you <code class="docutils literal notranslate"><span class="pre">print('hello')</span></code> it will appear in the Scrapy
log.</p>
</section>
<section id="log-short-names">
<span id="std-setting-LOG_SHORT_NAMES"></span><h5>LOG_SHORT_NAMES<a class="headerlink" href="#log-short-names" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the logs will just contain the root path. If it is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>
then it displays the component responsible for the log output</p>
</section>
<section id="logstats-interval">
<span id="std-setting-LOGSTATS_INTERVAL"></span><h5>LOGSTATS_INTERVAL<a class="headerlink" href="#logstats-interval" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>The interval (in seconds) between each logging printout of the stats
by <a class="reference internal" href="index.html#scrapy.extensions.logstats.LogStats" title="scrapy.extensions.logstats.LogStats"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogStats</span></code></a>.</p>
</section>
<section id="memdebug-enabled">
<span id="std-setting-MEMDEBUG_ENABLED"></span><h5>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether to enable memory debugging.</p>
</section>
<section id="memdebug-notify">
<span id="std-setting-MEMDEBUG_NOTIFY"></span><h5>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="memusage-enabled">
<span id="std-setting-MEMUSAGE_ENABLED"></span><h5>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to enable the memory usage extension. This extension keeps track of
a peak memory used by the process (it writes it to stats). It can also
optionally shutdown the Scrapy process when it exceeds a memory limit
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>), and notify by email when that happened
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a>).</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</section>
<section id="memusage-limit-mb">
<span id="std-setting-MEMUSAGE_LIMIT_MB"></span><h5>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</section>
<section id="memusage-check-interval-seconds">
<span id="std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"></span><h5>MEMUSAGE_CHECK_INTERVAL_SECONDS<a class="headerlink" href="#memusage-check-interval-seconds" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>
checks the current memory usage, versus the limits set by
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>,
at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</section>
<section id="memusage-notify-mail">
<span id="std-setting-MEMUSAGE_NOTIFY_MAIL"></span><h5>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</section>
<section id="memusage-warning-mb">
<span id="std-setting-MEMUSAGE_WARNING_MB"></span><h5>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
</section>
<section id="newspider-module">
<span id="std-setting-NEWSPIDER_MODULE"></span><h5>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code></p>
<p>Module where to create new spiders using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</section>
<section id="randomize-download-delay">
<span id="std-setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h5>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 * <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="https://www.gnu.org/software/wget/manual/wget.html">wget</a> <code class="docutils literal notranslate"><span class="pre">--random-wait</span></code> option.</p>
<p>If <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> is zero (default) this option has no effect.</p>
</section>
<section id="reactor-threadpool-maxsize">
<span id="std-setting-REACTOR_THREADPOOL_MAXSIZE"></span><h5>REACTOR_THREADPOOL_MAXSIZE<a class="headerlink" href="#reactor-threadpool-maxsize" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">10</span></code></p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase
this value if you’re experiencing problems with insufficient blocking IO.</p>
</section>
<section id="redirect-priority-adjust">
<span id="std-setting-REDIRECT_PRIORITY_ADJUST"></span><h5>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">+2</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</span></code></p>
<p>Adjust redirect request priority relative to original request:</p>
<ul class="simple">
<li><p><strong>a positive priority adjust (default) means higher priority.</strong></p></li>
<li><p>a negative priority adjust means lower priority.</p></li>
</ul>
</section>
<section id="robotstxt-obey">
<span id="std-setting-ROBOTSTXT_OBEY"></span><h5>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.robotstxt</span></code></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-dlmw-robots"><span class="std std-ref">RobotsTxtMiddleware</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While the default value is <code class="docutils literal notranslate"><span class="pre">False</span></code> for historical reasons,
this option is enabled by default in settings.py file generated
by <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span></code> command.</p>
</div>
</section>
<section id="robotstxt-parser">
<span id="std-setting-ROBOTSTXT_PARSER"></span><h5>ROBOTSTXT_PARSER<a class="headerlink" href="#robotstxt-parser" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.robotstxt.ProtegoRobotParser'</span></code></p>
<p>The parser backend to use for parsing <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> files. For more information see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-dlmw-robots"><span class="std std-ref">RobotsTxtMiddleware</span></a>.</p>
<section id="robotstxt-user-agent">
<span id="std-setting-ROBOTSTXT_USER_AGENT"></span><h6>ROBOTSTXT_USER_AGENT<a class="headerlink" href="#robotstxt-user-agent" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The user agent string to use for matching in the robots.txt file. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the User-Agent header you are sending with the request or the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> setting (in that order) will be used for determining
the user agent to use in the robots.txt file.</p>
</section>
</section>
<section id="scheduler">
<span id="std-setting-SCHEDULER"></span><h5>SCHEDULER<a class="headerlink" href="#scheduler" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></code></p>
<p>The scheduler class to be used for crawling.
See the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">Scheduler</span></a> topic for details.</p>
</section>
<section id="scheduler-debug">
<span id="std-setting-SCHEDULER_DEBUG"></span><h5>SCHEDULER_DEBUG<a class="headerlink" href="#scheduler-debug" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Setting to <code class="docutils literal notranslate"><span class="pre">True</span></code> will log debug information about the requests scheduler.
This currently logs (only once) if the requests cannot be serialized to disk.
Stats counter (<code class="docutils literal notranslate"><span class="pre">scheduler/unserializable</span></code>) tracks the number of times this happens.</p>
<p>Example entry in logs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1956</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">31</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">+</span><span class="mi">0800</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scheduler</span><span class="p">]</span> <span class="n">ERROR</span><span class="p">:</span> <span class="n">Unable</span> <span class="n">to</span> <span class="n">serialize</span> <span class="n">request</span><span class="p">:</span>
<span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">reason</span><span class="p">:</span> <span class="n">cannot</span> <span class="n">serialize</span> <span class="o">&lt;</span><span class="n">Request</span> <span class="n">at</span> <span class="mh">0x9a7c7ec</span><span class="o">&gt;</span>
<span class="p">(</span><span class="nb">type</span> <span class="n">Request</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">no</span> <span class="n">more</span> <span class="n">unserializable</span> <span class="n">requests</span> <span class="n">will</span> <span class="n">be</span> <span class="n">logged</span>
<span class="p">(</span><span class="n">see</span> <span class="s1">&#39;scheduler/unserializable&#39;</span> <span class="n">stats</span> <span class="n">counter</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="scheduler-disk-queue">
<span id="std-setting-SCHEDULER_DISK_QUEUE"></span><h5>SCHEDULER_DISK_QUEUE<a class="headerlink" href="#scheduler-disk-queue" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.squeues.PickleLifoDiskQueue'</span></code></p>
<p>Type of disk queue that will be used by scheduler. Other available types are
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.PickleFifoDiskQueue</span></code>, <code class="docutils literal notranslate"><span class="pre">scrapy.squeues.MarshalFifoDiskQueue</span></code>,
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.MarshalLifoDiskQueue</span></code>.</p>
</section>
<section id="scheduler-memory-queue">
<span id="std-setting-SCHEDULER_MEMORY_QUEUE"></span><h5>SCHEDULER_MEMORY_QUEUE<a class="headerlink" href="#scheduler-memory-queue" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.squeues.LifoMemoryQueue'</span></code></p>
<p>Type of in-memory queue used by scheduler. Other available type is:
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.FifoMemoryQueue</span></code>.</p>
</section>
<section id="scheduler-priority-queue">
<span id="std-setting-SCHEDULER_PRIORITY_QUEUE"></span><h5>SCHEDULER_PRIORITY_QUEUE<a class="headerlink" href="#scheduler-priority-queue" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.pqueues.ScrapyPriorityQueue'</span></code></p>
<p>Type of priority queue used by the scheduler. Another available type is
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>.
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code> works better than
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.ScrapyPriorityQueue</span></code> when you crawl many different
domains in parallel. But currently <code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>
does not work together with <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>.</p>
</section>
<section id="scraper-slot-max-active-size">
<span id="std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE"></span><h5>SCRAPER_SLOT_MAX_ACTIVE_SIZE<a class="headerlink" href="#scraper-slot-max-active-size" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">5_000_000</span></code></p>
<p>Soft limit (in bytes) for response data being processed.</p>
<p>While the sum of the sizes of all responses being processed is above this value,
Scrapy does not process new requests.</p>
</section>
<section id="spider-contracts">
<span id="std-setting-SPIDER_CONTRACTS"></span><h5>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="Permalink to this heading">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
</section>
<section id="spider-contracts-base">
<span id="std-setting-SPIDER_CONTRACTS_BASE"></span><h5>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;scrapy.contracts.default.UrlContract&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.contracts.default.ReturnsContract&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.contracts.default.ScrapesContract&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the Scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a>
instead. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
<p>You can disable any of these contracts by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their class
path in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a>. E.g., to disable the built-in
<code class="docutils literal notranslate"><span class="pre">ScrapesContract</span></code>, place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scrapy.contracts.default.ScrapesContract&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="spider-loader-class">
<span id="std-setting-SPIDER_LOADER_CLASS"></span><h5>SPIDER_LOADER_CLASS<a class="headerlink" href="#spider-loader-class" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.spiderloader.SpiderLoader'</span></code></p>
<p>The class that will be used for loading spiders, which must implement the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-spiderloader"><span class="std std-ref">SpiderLoader API</span></a>.</p>
</section>
<section id="spider-loader-warn-only">
<span id="std-setting-SPIDER_LOADER_WARN_ONLY"></span><h5>SPIDER_LOADER_WARN_ONLY<a class="headerlink" href="#spider-loader-warn-only" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>By default, when Scrapy tries to import spider classes from <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>,
it will fail loudly if there is any <code class="docutils literal notranslate"><span class="pre">ImportError</span></code> or <code class="docutils literal notranslate"><span class="pre">SyntaxError</span></code> exception.
But you can choose to silence this exception and turn it into a simple
warning by setting <code class="docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-commands"><span class="std std-ref">scrapy commands</span></a> run with this setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>
already (i.e. they will only issue a warning and will not fail)
since they do not actually need to load spider classes to work:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-settings"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">settings</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-version"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code></a>.</p>
</div>
</section>
<section id="spider-middlewares">
<span id="std-setting-SPIDER_MIDDLEWARES"></span><h5>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="Permalink to this heading">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</section>
<section id="spider-middlewares-base">
<span id="std-setting-SPIDER_MIDDLEWARES_BASE"></span><h5>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="Permalink to this heading">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.spidermiddlewares.referer.RefererMiddleware&quot;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&quot;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.spidermiddlewares.depth.DepthMiddleware&quot;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the spider middlewares enabled by default in Scrapy, and
their orders. Low orders are closer to the engine, high orders are closer to
the spider. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</section>
<section id="spider-modules">
<span id="std-setting-SPIDER_MODULES"></span><h5>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mybot.spiders_prod&quot;</span><span class="p">,</span> <span class="s2">&quot;mybot.spiders_dev&quot;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="stats-class">
<span id="std-setting-STATS_CLASS"></span><h5>STATS_CLASS<a class="headerlink" href="#stats-class" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.statscollectors.MemoryStatsCollector'</span></code></p>
<p>The class to use for collecting stats, who must implement the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-stats"><span class="std std-ref">Stats Collector API</span></a>.</p>
</section>
<section id="stats-dump">
<span id="std-setting-STATS_DUMP"></span><h5>STATS_DUMP<a class="headerlink" href="#stats-dump" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Dump the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">Scrapy stats</span></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
</section>
<section id="statsmailer-rcpts">
<span id="std-setting-STATSMAILER_RCPTS"></span><h5>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<a class="reference internal" href="index.html#scrapy.extensions.statsmailer.StatsMailer" title="scrapy.extensions.statsmailer.StatsMailer"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsMailer</span></code></a> for more info.</p>
</section>
<section id="telnetconsole-enabled">
<span id="std-setting-TELNETCONSOLE_ENABLED"></span><h5>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>A boolean which specifies if the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a>
will be enabled (provided its extension is also enabled).</p>
</section>
<section id="templates-dir">
<span id="std-setting-TEMPLATES_DIR"></span><h5>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">templates</span></code> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command and new spiders with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a>
command.</p>
<p>The project name must not conflict with the name of custom files or directories
in the <code class="docutils literal notranslate"><span class="pre">project</span></code> subdirectory.</p>
</section>
<section id="twisted-reactor">
<span id="std-setting-TWISTED_REACTOR"></span><h5>TWISTED_REACTOR<a class="headerlink" href="#twisted-reactor" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Import path of a given <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a>.</p>
<p>Scrapy will install this reactor if no other reactor is installed yet, such as
when the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> CLI program is invoked or when using the
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> class.</p>
<p>If you are using the <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> class, you also
need to install the correct reactor manually. You can do that using
<a class="reference internal" href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.reactor.install_reactor">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.reactor.</span></span><span class="sig-name descname"><span class="pre">install_reactor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reactor_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">event_loop_path</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.utils.reactor.install_reactor" title="Permalink to this definition">¶</a></dt>
<dd><p>Installs the <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> with the specified
import path. Also installs the asyncio event loop with the specified import
path if the asyncio reactor is enabled</p>
</dd></dl>

<p>If a reactor is already installed,
<a class="reference internal" href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a> has no effect.</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">CrawlerRunner.__init__</span></code> raises
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Exception</span></code></a> if the installed reactor does not match the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting; therefore, having top-level
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> imports in project files and imported
third-party libraries will make Scrapy raise <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Exception</span></code></a> when
it checks which reactor is installed.</p>
<p>In order to use the reactor installed by Scrapy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;timeout&quot;</span><span class="p">,</span> <span class="s2">&quot;60&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QuotesSpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">reactor</span><span class="o">.</span><span class="n">callLater</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">)</span>

        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://quotes.toscrape.com/page/1&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()}</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;timeout&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>which raises <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Exception</span></code></a>, becomes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timeout</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;timeout&quot;</span><span class="p">,</span> <span class="s2">&quot;60&quot;</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QuotesSpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

        <span class="n">reactor</span><span class="o">.</span><span class="n">callLater</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timeout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">)</span>

        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://quotes.toscrape.com/page/1&quot;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()}</span>

    <span class="k">def</span> <span class="nf">stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;timeout&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The default value of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting is <code class="docutils literal notranslate"><span class="pre">None</span></code>, which
means that Scrapy will use the existing reactor if one is already installed, or
install the default reactor defined by Twisted for the current platform. This
is to maintain backward compatibility and avoid possible problems caused by
using a non-default reactor.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.7: </span>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command now sets this setting to
<code class="docutils literal notranslate"><span class="pre">twisted.internet.asyncioreactor.AsyncioSelectorReactor</span></code> in the generated
<code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file.</p>
</div>
<p>For additional information, see <a class="reference external" href="https://docs.twisted.org/en/stable/core/howto/choosing-reactor.html" title="(in Twisted v24.10)"><span>Choosing a Reactor and GUI Toolkit Integration</span></a>.</p>
</section>
<section id="urllength-limit">
<span id="std-setting-URLLENGTH_LIMIT"></span><h5>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">2083</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">spidermiddlewares.urllength</span></code></p>
<p>The maximum URL length to allow for crawled URLs.</p>
<p>This setting can act as a stopping condition in case of URLs of ever-increasing
length, which may be caused for example by a programming error either in the
target server or in your code. See also <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-DEPTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_LIMIT</span></code></a>.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">0</span></code> to allow URLs of any length.</p>
<p>The default value is copied from the <a class="reference external" href="https://support.microsoft.com/en-us/topic/maximum-url-length-is-2-083-characters-in-internet-explorer-174e7c8a-6666-f4e0-6fd6-908b53c12246">Microsoft Internet Explorer maximum URL
length</a>, even though this setting exists for different reasons.</p>
</section>
<section id="user-agent">
<span id="std-setting-USER_AGENT"></span><h5>USER_AGENT<a class="headerlink" href="#user-agent" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+https://scrapy.org)&quot;</span></code></p>
<p>The default User-Agent to use when crawling, unless overridden. This user agent is
also used by <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotsTxtMiddleware</span></code></a>
if <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-ROBOTSTXT_USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_USER_AGENT</span></code></a> setting is <code class="docutils literal notranslate"><span class="pre">None</span></code> and
there is no overriding User-Agent header specified for the request.</p>
</section>
<section id="settings-documented-elsewhere">
<h5>Settings documented elsewhere:<a class="headerlink" href="#settings-documented-elsewhere" title="Permalink to this heading">¶</a></h5>
<p>The following settings are documented elsewhere, please check each specific
case to see how to enable and use them.</p>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#std-setting-ADDONS">ADDONS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AJAXCRAWL_ENABLED">AJAXCRAWL_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ASYNCIO_EVENT_LOOP">ASYNCIO_EVENT_LOOP</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AUTOTHROTTLE_DEBUG">AUTOTHROTTLE_DEBUG</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AUTOTHROTTLE_ENABLED">AUTOTHROTTLE_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AUTOTHROTTLE_MAX_DELAY">AUTOTHROTTLE_MAX_DELAY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AUTOTHROTTLE_START_DELAY">AUTOTHROTTLE_START_DELAY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">AUTOTHROTTLE_TARGET_CONCURRENCY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_ACCESS_KEY_ID">AWS_ACCESS_KEY_ID</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_ENDPOINT_URL">AWS_ENDPOINT_URL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_REGION_NAME">AWS_REGION_NAME</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_SECRET_ACCESS_KEY">AWS_SECRET_ACCESS_KEY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_SESSION_TOKEN">AWS_SESSION_TOKEN</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_USE_SSL">AWS_USE_SSL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-AWS_VERIFY">AWS_VERIFY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-BOT_NAME">BOT_NAME</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_ERRORCOUNT">CLOSESPIDER_ERRORCOUNT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_ITEMCOUNT">CLOSESPIDER_ITEMCOUNT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_PAGECOUNT">CLOSESPIDER_PAGECOUNT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_PAGECOUNT_NO_ITEM">CLOSESPIDER_PAGECOUNT_NO_ITEM</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_TIMEOUT">CLOSESPIDER_TIMEOUT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM">CLOSESPIDER_TIMEOUT_NO_ITEM</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-COMMANDS_MODULE">COMMANDS_MODULE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-COMPRESSION_ENABLED">COMPRESSION_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CONCURRENT_ITEMS">CONCURRENT_ITEMS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS">CONCURRENT_REQUESTS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">CONCURRENT_REQUESTS_PER_DOMAIN</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">CONCURRENT_REQUESTS_PER_IP</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-COOKIES_DEBUG">COOKIES_DEBUG</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-COOKIES_ENABLED">COOKIES_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DEFAULT_ITEM_CLASS">DEFAULT_ITEM_CLASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DEFAULT_REQUEST_HEADERS">DEFAULT_REQUEST_HEADERS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DEPTH_LIMIT">DEPTH_LIMIT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DEPTH_PRIORITY">DEPTH_PRIORITY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DEPTH_STATS_VERBOSE">DEPTH_STATS_VERBOSE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DNSCACHE_ENABLED">DNSCACHE_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DNSCACHE_SIZE">DNSCACHE_SIZE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DNS_RESOLVER">DNS_RESOLVER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DNS_TIMEOUT">DNS_TIMEOUT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER">DOWNLOADER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">DOWNLOADER_CLIENTCONTEXTFACTORY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">DOWNLOADER_CLIENT_TLS_CIPHERS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD">DOWNLOADER_CLIENT_TLS_METHOD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_HTTPCLIENTFACTORY">DOWNLOADER_HTTPCLIENTFACTORY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOADER_STATS">DOWNLOADER_STATS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_DELAY">DOWNLOAD_DELAY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS">DOWNLOAD_FAIL_ON_DATALOSS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS">DOWNLOAD_HANDLERS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS_BASE">DOWNLOAD_HANDLERS_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE">DOWNLOAD_MAXSIZE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_SLOTS">DOWNLOAD_SLOTS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_TIMEOUT">DOWNLOAD_TIMEOUT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DOWNLOAD_WARNSIZE">DOWNLOAD_WARNSIZE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DUPEFILTER_CLASS">DUPEFILTER_CLASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-DUPEFILTER_DEBUG">DUPEFILTER_DEBUG</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-EDITOR">EDITOR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-EXTENSIONS">EXTENSIONS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-EXTENSIONS_BASE">EXTENSIONS_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEEDS">FEEDS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORTERS">FEED_EXPORTERS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORTERS_BASE">FEED_EXPORTERS_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">FEED_EXPORT_BATCH_ITEM_COUNT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORT_ENCODING">FEED_EXPORT_ENCODING</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORT_FIELDS">FEED_EXPORT_FIELDS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_EXPORT_INDENT">FEED_EXPORT_INDENT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORAGES">FEED_STORAGES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORAGES_BASE">FEED_STORAGES_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE">FEED_STORAGE_FTP_ACTIVE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORAGE_GCS_ACL">FEED_STORAGE_GCS_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORAGE_S3_ACL">FEED_STORAGE_S3_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_STORE_EMPTY">FEED_STORE_EMPTY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_TEMPDIR">FEED_TEMPDIR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FEED_URI_PARAMS">FEED_URI_PARAMS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_EXPIRES">FILES_EXPIRES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_RESULT_FIELD">FILES_RESULT_FIELD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_STORE">FILES_STORE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_STORE_GCS_ACL">FILES_STORE_GCS_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_STORE_S3_ACL">FILES_STORE_S3_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FILES_URLS_FIELD">FILES_URLS_FIELD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FTP_PASSIVE_MODE">FTP_PASSIVE_MODE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FTP_PASSWORD">FTP_PASSWORD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-FTP_USER">FTP_USER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-GCS_PROJECT_ID">GCS_PROJECT_ID</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_ALWAYS_STORE">HTTPCACHE_ALWAYS_STORE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_DIR">HTTPCACHE_DIR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_ENABLED">HTTPCACHE_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_EXPIRATION_SECS">HTTPCACHE_EXPIRATION_SECS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_GZIP">HTTPCACHE_GZIP</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_IGNORE_HTTP_CODES">HTTPCACHE_IGNORE_HTTP_CODES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_IGNORE_MISSING">HTTPCACHE_IGNORE_MISSING</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_IGNORE_SCHEMES">HTTPCACHE_IGNORE_SCHEMES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_POLICY">HTTPCACHE_POLICY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPERROR_ALLOWED_CODES">HTTPERROR_ALLOWED_CODES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPERROR_ALLOW_ALL">HTTPERROR_ALLOW_ALL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPPROXY_AUTH_ENCODING">HTTPPROXY_AUTH_ENCODING</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-HTTPPROXY_ENABLED">HTTPPROXY_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_EXPIRES">IMAGES_EXPIRES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_MIN_HEIGHT">IMAGES_MIN_HEIGHT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_MIN_WIDTH">IMAGES_MIN_WIDTH</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_RESULT_FIELD">IMAGES_RESULT_FIELD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_STORE">IMAGES_STORE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_STORE_GCS_ACL">IMAGES_STORE_GCS_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_STORE_S3_ACL">IMAGES_STORE_S3_ACL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_THUMBS">IMAGES_THUMBS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-IMAGES_URLS_FIELD">IMAGES_URLS_FIELD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ITEM_PIPELINES">ITEM_PIPELINES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ITEM_PIPELINES_BASE">ITEM_PIPELINES_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-JOBDIR">JOBDIR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOGSTATS_INTERVAL">LOGSTATS_INTERVAL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_DATEFORMAT">LOG_DATEFORMAT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_ENABLED">LOG_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_ENCODING">LOG_ENCODING</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_FILE">LOG_FILE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_FILE_APPEND">LOG_FILE_APPEND</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_FORMAT">LOG_FORMAT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_FORMATTER">LOG_FORMATTER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_LEVEL">LOG_LEVEL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_SHORT_NAMES">LOG_SHORT_NAMES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-LOG_STDOUT">LOG_STDOUT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_FROM">MAIL_FROM</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_HOST">MAIL_HOST</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_PASS">MAIL_PASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_PORT">MAIL_PORT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_SSL">MAIL_SSL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_TLS">MAIL_TLS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MAIL_USER">MAIL_USER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEDIA_ALLOW_REDIRECTS">MEDIA_ALLOW_REDIRECTS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMDEBUG_ENABLED">MEMDEBUG_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMDEBUG_NOTIFY">MEMDEBUG_NOTIFY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">MEMUSAGE_CHECK_INTERVAL_SECONDS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMUSAGE_ENABLED">MEMUSAGE_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMUSAGE_LIMIT_MB">MEMUSAGE_LIMIT_MB</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMUSAGE_NOTIFY_MAIL">MEMUSAGE_NOTIFY_MAIL</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-MEMUSAGE_WARNING_MB">MEMUSAGE_WARNING_MB</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-METAREFRESH_ENABLED">METAREFRESH_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">METAREFRESH_IGNORE_TAGS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-METAREFRESH_MAXDELAY">METAREFRESH_MAXDELAY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-NEWSPIDER_MODULE">NEWSPIDER_MODULE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-PERIODIC_LOG_DELTA">PERIODIC_LOG_DELTA</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-PERIODIC_LOG_STATS">PERIODIC_LOG_STATS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-PERIODIC_LOG_TIMING_ENABLED">PERIODIC_LOG_TIMING_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">RANDOMIZE_DOWNLOAD_DELAY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE">REACTOR_THREADPOOL_MAXSIZE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REDIRECT_ENABLED">REDIRECT_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REDIRECT_PRIORITY_ADJUST">REDIRECT_PRIORITY_ADJUST</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REFERER_ENABLED">REFERER_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REFERRER_POLICY">REFERRER_POLICY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS">REQUEST_FINGERPRINTER_CLASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RETRY_ENABLED">RETRY_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RETRY_EXCEPTIONS">RETRY_EXCEPTIONS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RETRY_HTTP_CODES">RETRY_HTTP_CODES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RETRY_PRIORITY_ADJUST">RETRY_PRIORITY_ADJUST</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-RETRY_TIMES">RETRY_TIMES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ROBOTSTXT_OBEY">ROBOTSTXT_OBEY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER">ROBOTSTXT_PARSER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-ROBOTSTXT_USER_AGENT">ROBOTSTXT_USER_AGENT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCHEDULER">SCHEDULER</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCHEDULER_DEBUG">SCHEDULER_DEBUG</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCHEDULER_DISK_QUEUE">SCHEDULER_DISK_QUEUE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">SCHEDULER_MEMORY_QUEUE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">SCHEDULER_PRIORITY_QUEUE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE">SCRAPER_SLOT_MAX_ACTIVE_SIZE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_CONTRACTS">SPIDER_CONTRACTS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_CONTRACTS_BASE">SPIDER_CONTRACTS_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS">SPIDER_LOADER_CLASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY">SPIDER_LOADER_WARN_ONLY</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES">SPIDER_MIDDLEWARES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">SPIDER_MIDDLEWARES_BASE</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-SPIDER_MODULES">SPIDER_MODULES</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-STATSMAILER_RCPTS">STATSMAILER_RCPTS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-STATS_CLASS">STATS_CLASS</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-STATS_DUMP">STATS_DUMP</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TELNETCONSOLE_ENABLED">TELNETCONSOLE_ENABLED</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TELNETCONSOLE_HOST">TELNETCONSOLE_HOST</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TELNETCONSOLE_PASSWORD">TELNETCONSOLE_PASSWORD</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TELNETCONSOLE_USERNAME">TELNETCONSOLE_USERNAME</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TEMPLATES_DIR">TEMPLATES_DIR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-TWISTED_REACTOR">TWISTED_REACTOR</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-URLLENGTH_LIMIT">URLLENGTH_LIMIT</a></p></li>
<li><p><a class="reference internal" href="index.html#std-setting-USER_AGENT">USER_AGENT</a></p></li>
</ul>
</section>
</section>
</section>
<span id="document-topics/exceptions"></span><section id="module-scrapy.exceptions">
<span id="exceptions"></span><span id="topics-exceptions"></span><h3>Exceptions<a class="headerlink" href="#module-scrapy.exceptions" title="Permalink to this heading">¶</a></h3>
<section id="built-in-exceptions-reference">
<span id="topics-exceptions-ref"></span><h4>Built-in Exceptions reference<a class="headerlink" href="#built-in-exceptions-reference" title="Permalink to this heading">¶</a></h4>
<p>Here’s a list of all exceptions included in Scrapy and their usage.</p>
<section id="closespider">
<h5>CloseSpider<a class="headerlink" href="#closespider" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.CloseSpider">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">CloseSpider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reason</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cancelled'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exceptions.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>reason</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the reason for closing</p>
</dd>
</dl>
</dd></dl>

<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;Bandwidth exceeded&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CloseSpider</span><span class="p">(</span><span class="s2">&quot;bandwidth_exceeded&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dontclosespider">
<h5>DontCloseSpider<a class="headerlink" href="#dontclosespider" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.DontCloseSpider">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">DontCloseSpider</span></span><a class="headerlink" href="#scrapy.exceptions.DontCloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised in a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_idle"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_idle</span></code></a> signal handler to
prevent the spider from being closed.</p>
</section>
<section id="dropitem">
<h5>DropItem<a class="headerlink" href="#dropitem" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.DropItem">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">DropItem</span></span><a class="headerlink" href="#scrapy.exceptions.DropItem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
</section>
<section id="ignorerequest">
<h5>IgnoreRequest<a class="headerlink" href="#ignorerequest" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.IgnoreRequest">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">IgnoreRequest</span></span><a class="headerlink" href="#scrapy.exceptions.IgnoreRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.</p>
</section>
<section id="notconfigured">
<h5>NotConfigured<a class="headerlink" href="#notconfigured" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.NotConfigured">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">NotConfigured</span></span><a class="headerlink" href="#scrapy.exceptions.NotConfigured" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by some components to indicate that they will
remain disabled. Those components include:</p>
<ul class="simple">
<li><p>Extensions</p></li>
<li><p>Item pipelines</p></li>
<li><p>Downloader middlewares</p></li>
<li><p>Spider middlewares</p></li>
</ul>
<p>The exception must be raised in the component’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</section>
<section id="notsupported">
<h5>NotSupported<a class="headerlink" href="#notsupported" title="Permalink to this heading">¶</a></h5>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.NotSupported">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">NotSupported</span></span><a class="headerlink" href="#scrapy.exceptions.NotSupported" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception is raised to indicate an unsupported feature.</p>
</section>
<section id="stopdownload">
<h5>StopDownload<a class="headerlink" href="#stopdownload" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<dl class="py exception">
<dt class="sig sig-object py" id="scrapy.exceptions.StopDownload">
<em class="property"><span class="pre">exception</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">StopDownload</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fail</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exceptions.StopDownload" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Raised from a <a class="reference internal" href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes_received</span></code></a> or <a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a>
signal handler to indicate that no further bytes should be downloaded for a response.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">fail</span></code> boolean parameter controls which method will handle the resulting
response:</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">fail=True</span></code> (default), the request errback is called. The response object is
available as the <code class="docutils literal notranslate"><span class="pre">response</span></code> attribute of the <code class="docutils literal notranslate"><span class="pre">StopDownload</span></code> exception,
which is in turn stored as the <code class="docutils literal notranslate"><span class="pre">value</span></code> attribute of the received
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Failure</span></code></a> object. This means that in an errback
defined as <code class="docutils literal notranslate"><span class="pre">def</span> <span class="pre">errback(self,</span> <span class="pre">failure)</span></code>, the response can be accessed though
<code class="docutils literal notranslate"><span class="pre">failure.value.response</span></code>.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">fail=False</span></code>, the request callback is called instead.</p></li>
</ul>
<p>In both cases, the response could have its body truncated: the body contains
all bytes received up until the exception is raised, including the bytes
received in the signal handler that raises the exception. Also, the response
object is marked with <code class="docutils literal notranslate"><span class="pre">&quot;download_stopped&quot;</span></code> in its <code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.flags</span></code>
attribute.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">fail</span></code> is a keyword-only parameter, i.e. raising
<code class="docutils literal notranslate"><span class="pre">StopDownload(False)</span></code> or <code class="docutils literal notranslate"><span class="pre">StopDownload(True)</span></code> will raise
a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">TypeError</span></code></a>.</p>
</div>
<p>See the documentation for the <a class="reference internal" href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes_received</span></code></a> and
<a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a> signals
and the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">Stopping the download of a Response</span></a> topic for additional information and examples.</p>
</section>
</section>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-topics/commands"><span class="doc">Command line tool</span></a></dt><dd><p>Learn about the command-line tool used to manage your Scrapy project.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/spiders"><span class="doc">Spiders</span></a></dt><dd><p>Write the rules to crawl your websites.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/selectors"><span class="doc">Selectors</span></a></dt><dd><p>Extract the data from web pages using XPath.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/shell"><span class="doc">Scrapy shell</span></a></dt><dd><p>Test your extraction code in an interactive environment.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/items"><span class="doc">Items</span></a></dt><dd><p>Define the data you want to scrape.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/loaders"><span class="doc">Item Loaders</span></a></dt><dd><p>Populate your items with the extracted data.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">Item Pipeline</span></a></dt><dd><p>Post-process and store your scraped data.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/feed-exports"><span class="doc">Feed exports</span></a></dt><dd><p>Output your scraped data using different formats and storages.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/request-response"><span class="doc">Requests and Responses</span></a></dt><dd><p>Understand the classes used to represent HTTP requests and responses.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/link-extractors"><span class="doc">Link Extractors</span></a></dt><dd><p>Convenient classes to extract links to follow from pages.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/settings"><span class="doc">Settings</span></a></dt><dd><p>Learn how to configure Scrapy and see all <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">available settings</span></a>.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/exceptions"><span class="doc">Exceptions</span></a></dt><dd><p>See all available exceptions and their meaning.</p>
</dd>
</dl>
</section>
<section id="built-in-services">
<h2>Built-in services<a class="headerlink" href="#built-in-services" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/logging"></span><section id="logging">
<span id="topics-logging"></span><h3>Logging<a class="headerlink" href="#logging" title="Permalink to this heading">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.log</span></code> has been deprecated alongside its functions in favor of
explicit calls to the Python standard logging. Keep reading to learn more
about the new logging system.</p>
</div>
<p>Scrapy uses <a class="reference external" href="https://docs.python.org/3/library/logging.html#module-logging" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">logging</span></code></a> for event logging. We’ll
provide some simple examples to get you started, but for more advanced
use-cases it’s strongly suggested to read thoroughly its documentation.</p>
<p>Logging works out of the box, and can be configured to some extent with the
Scrapy settings listed in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>.</p>
<p>Scrapy calls <a class="reference internal" href="#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.log.configure_logging()</span></code></a> to set some reasonable
defaults and handle those settings in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a> when
running commands, so it’s recommended to manually call it if you’re running
Scrapy from scripts as described in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
<section id="log-levels">
<span id="topics-logging-levels"></span><h4>Log levels<a class="headerlink" href="#log-levels" title="Permalink to this heading">¶</a></h4>
<p>Python’s builtin logging defines 5 different levels to indicate the severity of a
given log message. Here are the standard ones, listed in decreasing order:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">logging.CRITICAL</span></code> - for critical errors (highest severity)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logging.ERROR</span></code> - for regular errors</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code> - for warning messages</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logging.INFO</span></code> - for informational messages</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logging.DEBUG</span></code> - for debugging messages (lowest severity)</p></li>
</ol>
</section>
<section id="how-to-log-messages">
<h4>How to log messages<a class="headerlink" href="#how-to-log-messages" title="Permalink to this heading">¶</a></h4>
<p>Here’s a quick example of how to log a message using the <code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code>
level:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>There are shortcuts for issuing log messages on any of the standard 5 levels,
and there’s also a general <code class="docutils literal notranslate"><span class="pre">logging.log</span></code> method which takes a given level as
argument.  If needed, the last example could be rewritten as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span> <span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On top of that, you can create different “loggers” to encapsulate messages. (For
example, a common practice is to create different loggers for every module).
These loggers can be configured independently, and they allow hierarchical
constructions.</p>
<p>The previous examples use the root logger behind the scenes, which is a top level
logger where all messages are propagated to (unless otherwise specified). Using
<code class="docutils literal notranslate"><span class="pre">logging</span></code> helpers is merely a shortcut for getting the root logger
explicitly, so this is also an equivalent of the last snippets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can use a different logger just by getting its name with the
<code class="docutils literal notranslate"><span class="pre">logging.getLogger</span></code> function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;mycustomlogger&quot;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, you can ensure having a custom logger for any module you’re working on
by using the <code class="docutils literal notranslate"><span class="pre">__name__</span></code> variable, which is populated with current module’s
path:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt>Module logging, <a class="reference external" href="https://docs.python.org/3/howto/logging.html" title="(in Python v3.13)"><span class="xref std std-doc">HowTo</span></a></dt><dd><p>Basic Logging Tutorial</p>
</dd>
<dt>Module logging, <a class="reference external" href="https://docs.python.org/3/library/logging.html#logger" title="(in Python v3.13)"><span class="xref std std-ref">Loggers</span></a></dt><dd><p>Further documentation on loggers</p>
</dd>
</dl>
</div>
</section>
<section id="logging-from-spiders">
<span id="topics-logging-from-spiders"></span><h4>Logging from Spiders<a class="headerlink" href="#logging-from-spiders" title="Permalink to this heading">¶</a></h4>
<p>Scrapy provides a <a class="reference internal" href="index.html#scrapy.Spider.logger" title="scrapy.Spider.logger"><code class="xref py py-data docutils literal notranslate"><span class="pre">logger</span></code></a> within each Spider
instance, which can be accessed and used like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://scrapy.org&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Parse function called on </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>That logger is created using the Spider’s name, but you can use any custom
Python logger you want. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;mycustomlogger&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://scrapy.org&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Parse function called on </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="logging-configuration">
<span id="topics-logging-configuration"></span><h4>Logging configuration<a class="headerlink" href="#logging-configuration" title="Permalink to this heading">¶</a></h4>
<p>Loggers on their own don’t manage how messages sent through them are displayed.
For this task, different “handlers” can be attached to any logger instance and
they will redirect those messages to appropriate destinations, such as the
standard output, files, emails, etc.</p>
<p>By default, Scrapy sets and configures a handler for the root logger, based on
the settings below.</p>
<section id="logging-settings">
<span id="topics-logging-settings"></span><h5>Logging settings<a class="headerlink" href="#logging-settings" title="Permalink to this heading">¶</a></h5>
<p>These settings can be used to configure the logging:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE_APPEND"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE_APPEND</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_DATEFORMAT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_STDOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_STDOUT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a></p></li>
</ul>
<p>The first couple of settings define a destination for log messages. If
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> is set, messages sent through the root logger will be
redirected to a file named <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> with encoding
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code></a>. If unset and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, log
messages will be displayed on the standard error. If <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> is set
and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE_APPEND"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE_APPEND</span></code></a> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, the file will be overwritten
(discarding the output from previous runs, if any). Lastly, if
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, there won’t be any visible log output.</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a> determines the minimum level of severity to display, those
messages with lower severity will be filtered out. It ranges through the
possible levels listed in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-logging-levels"><span class="std std-ref">Log levels</span></a>.</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_DATEFORMAT</span></code></a> specify formatting strings
used as layouts for all messages. Those strings can contain any placeholders
listed in <a class="reference external" href="https://docs.python.org/3/library/logging.html#logrecord-attributes" title="(in Python v3.13)"><span class="xref std std-ref">logging’s logrecord attributes docs</span></a> and
<a class="reference external" href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior" title="(in Python v3.13)"><span class="xref std std-ref">datetime’s strftime and strptime directives</span></a>
respectively.</p>
<p>If <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a> is set, then the logs will not display the Scrapy
component that prints the log. It is unset by default, hence logs contain the
Scrapy component responsible for that log output.</p>
</section>
<section id="command-line-options">
<h5>Command-line options<a class="headerlink" href="#command-line-options" title="Permalink to this heading">¶</a></h5>
<p>There are command-line arguments, available for all commands, that you can use
to override some of the Scrapy settings regarding logging.</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">--logfile</span> <span class="pre">FILE</span></code></dt><dd><p>Overrides <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">--loglevel/-L</span> <span class="pre">LEVEL</span></code></dt><dd><p>Overrides <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">--nolog</span></code></dt><dd><p>Sets <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> to <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
</dd>
</dl>
</li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt>Module <a class="reference external" href="https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">logging.handlers</span></code></a></dt><dd><p>Further documentation on available handlers</p>
</dd>
</dl>
</div>
</section>
<section id="custom-log-formats">
<span id="id1"></span><h5>Custom Log Formats<a class="headerlink" href="#custom-log-formats" title="Permalink to this heading">¶</a></h5>
<p>A custom log format can be set for different actions by extending
<a class="reference internal" href="#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogFormatter</span></code></a> class and making
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FORMATTER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMATTER</span></code></a> point to your new class.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.logformatter.</span></span><span class="sig-name descname"><span class="pre">LogFormatter</span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Class for generating log messages for different actions.</p>
<p>All methods must return a dictionary listing the parameters <code class="docutils literal notranslate"><span class="pre">level</span></code>, <code class="docutils literal notranslate"><span class="pre">msg</span></code>
and <code class="docutils literal notranslate"><span class="pre">args</span></code> which are going to be used for constructing the log message when
calling <code class="docutils literal notranslate"><span class="pre">logging.log</span></code>.</p>
<p>Dictionary keys for the method outputs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">level</span></code> is the log level for that action, you can use those from the
<a class="reference external" href="https://docs.python.org/3/library/logging.html">python logging library</a> :
<code class="docutils literal notranslate"><span class="pre">logging.DEBUG</span></code>, <code class="docutils literal notranslate"><span class="pre">logging.INFO</span></code>, <code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code>, <code class="docutils literal notranslate"><span class="pre">logging.ERROR</span></code>
and <code class="docutils literal notranslate"><span class="pre">logging.CRITICAL</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">msg</span></code> should be a string that can contain different formatting placeholders.
This string, formatted with the provided <code class="docutils literal notranslate"><span class="pre">args</span></code>, is going to be the long message
for that action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">args</span></code> should be a tuple or dict with the formatting placeholders for <code class="docutils literal notranslate"><span class="pre">msg</span></code>.
The final log message is computed as <code class="docutils literal notranslate"><span class="pre">msg</span> <span class="pre">%</span> <span class="pre">args</span></code>.</p></li>
</ul>
<p>Users can define their own <code class="docutils literal notranslate"><span class="pre">LogFormatter</span></code> class if they want to customize how
each action is logged or if they want to omit it entirely. In order to omit
logging an action the method must return <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>Here is an example on how to create a custom log formatter to lower the severity level of
the log message when an item is dropped from the pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PoliteLogFormatter</span><span class="p">(</span><span class="n">logformatter</span><span class="o">.</span><span class="n">LogFormatter</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">dropped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">exception</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;level&#39;</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="c1"># lowering the level from logging.WARNING</span>
            <span class="s1">&#39;msg&#39;</span><span class="p">:</span> <span class="s2">&quot;Dropped: </span><span class="si">%(exception)s</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">linesep</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="si">%(item)s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="s1">&#39;args&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;exception&#39;</span><span class="p">:</span> <span class="n">exception</span><span class="p">,</span>
                <span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.crawled">
<span class="sig-name descname"><span class="pre">crawled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.request.Request"><span class="pre">Request</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.response.Response"><span class="pre">Response</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.crawled" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs a message when the crawler finds a webpage.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.download_error">
<span class="sig-name descname"><span class="pre">download_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">failure</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><span class="pre">Failure</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.request.Request"><span class="pre">Request</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">errmsg</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.download_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs a download error message from a spider (typically coming from
the engine).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.dropped">
<span class="sig-name descname"><span class="pre">dropped</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#BaseException" title="(in Python v3.13)"><span class="pre">BaseException</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.response.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs a message when an item is dropped while it is passing through the item pipeline.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.item_error">
<span class="sig-name descname"><span class="pre">item_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#BaseException" title="(in Python v3.13)"><span class="pre">BaseException</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.response.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.item_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs a message when an item causes an error while it is passing
through the item pipeline.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.scraped">
<span class="sig-name descname"><span class="pre">scraped</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.response.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><span class="pre">Failure</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.scraped" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs a message when an item is scraped by a spider.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.logformatter.LogFormatter.spider_error">
<span class="sig-name descname"><span class="pre">spider_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">failure</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><span class="pre">Failure</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.request.Request"><span class="pre">Request</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.response.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><span class="pre">Failure</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogFormatterResult</span></span></span><a class="headerlink" href="#scrapy.logformatter.LogFormatter.spider_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs an error message from a spider.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="advanced-customization">
<span id="topics-logging-advanced-customization"></span><h5>Advanced customization<a class="headerlink" href="#advanced-customization" title="Permalink to this heading">¶</a></h5>
<p>Because Scrapy uses stdlib logging module, you can customize logging using
all features of stdlib logging.</p>
<p>For example, let’s say you’re scraping a website which returns many
HTTP 404 and 500 responses, and you want to hide all messages like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">22</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">06</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">spidermiddlewares</span><span class="o">.</span><span class="n">httperror</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Ignoring</span>
<span class="n">response</span> <span class="o">&lt;</span><span class="mi">500</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">-</span><span class="mi">34</span><span class="o">/&gt;</span><span class="p">:</span> <span class="n">HTTP</span> <span class="n">status</span> <span class="n">code</span>
<span class="ow">is</span> <span class="ow">not</span> <span class="n">handled</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">allowed</span>
</pre></div>
</div>
<p>The first thing to note is a logger name - it is in brackets:
<code class="docutils literal notranslate"><span class="pre">[scrapy.spidermiddlewares.httperror]</span></code>. If you get just <code class="docutils literal notranslate"><span class="pre">[scrapy]</span></code> then
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a> is likely set to True; set it to False and re-run
the crawl.</p>
<p>Next, we can see that the message has INFO level. To hide it
we should set logging level for <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.httperror</span></code>
higher than INFO; next level after INFO is WARNING. It could be done
e.g. in the spider’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;scrapy.spidermiddlewares.httperror&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>If you run this spider again then INFO messages from
<code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.httperror</span></code> logger will be gone.</p>
<p>You can also filter log records by <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.LogRecord" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogRecord</span></code></a> data. For
example, you can filter log records by message content using a substring or
a regular expression. Create a <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.Filter" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">logging.Filter</span></code></a> subclass
and equip it with a regular expression pattern to
filter out unwanted messages:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="k">class</span> <span class="nc">ContentFilter</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">Filter</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\d</span><span class="si">{3}</span><span class="s2"> [Ee]rror, retrying&quot;</span><span class="p">,</span> <span class="n">record</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
</pre></div>
</div>
<p>A project-level filter may be attached to the root
handler created by Scrapy, this is a wieldy way to
filter all loggers in different parts of the project
(middlewares, spider, etc.):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">handler</span> <span class="ow">in</span> <span class="n">logging</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">handlers</span><span class="p">:</span>
            <span class="n">handler</span><span class="o">.</span><span class="n">addFilter</span><span class="p">(</span><span class="n">ContentFilter</span><span class="p">())</span>
</pre></div>
</div>
<p>Alternatively, you may choose a specific logger
and hide it without affecting other loggers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;my_logger&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">addFilter</span><span class="p">(</span><span class="n">ContentFilter</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
<section id="module-scrapy.utils.log">
<span id="scrapy-utils-log-module"></span><h4>scrapy.utils.log module<a class="headerlink" href="#module-scrapy.utils.log" title="Permalink to this heading">¶</a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.log.configure_logging">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.log.</span></span><span class="sig-name descname"><span class="pre">configure_logging</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><span class="pre">Settings</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">NoneType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">install_root_handler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.utils.log.configure_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize logging defaults for Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>settings</strong> (dict, <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object or <code class="docutils literal notranslate"><span class="pre">None</span></code>) – settings used to create and configure a handler for the
root logger (default: None).</p></li>
<li><p><strong>install_root_handler</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether to install root logging handler
(default: True)</p></li>
</ul>
</dd>
</dl>
<p>This function does:</p>
<ul class="simple">
<li><p>Route warnings and twisted logging through Python standard logging</p></li>
<li><p>Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively</p></li>
<li><p>Route stdout to log if LOG_STDOUT setting is True</p></li>
</ul>
<p>When <code class="docutils literal notranslate"><span class="pre">install_root_handler</span></code> is True (default), this function also
creates a handler for the root logger according to given settings
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>). You can override default options
using <code class="docutils literal notranslate"><span class="pre">settings</span></code> argument. When <code class="docutils literal notranslate"><span class="pre">settings</span></code> is empty or None, defaults
are used.</p>
<p><code class="docutils literal notranslate"><span class="pre">configure_logging</span></code> is automatically called when using Scrapy commands
or <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>, but needs to be called explicitly
when running custom scripts using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>.
In that case, its usage is not required but it’s recommended.</p>
<p>Another option when running custom scripts is to manually configure the logging.
To do this you can use <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.basicConfig" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">logging.basicConfig()</span></code></a> to set a basic root handler.</p>
<p>Note that <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> automatically calls <code class="docutils literal notranslate"><span class="pre">configure_logging</span></code>,
so it is recommended to only use <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.basicConfig" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">logging.basicConfig()</span></code></a> together with
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>.</p>
<p>This is an example on how to redirect <code class="docutils literal notranslate"><span class="pre">INFO</span></code> or higher messages to a file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;log.txt&quot;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(levelname)s</span><span class="s2">: </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Refer to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for more details about using Scrapy this
way.</p>
</dd></dl>

</section>
</section>
<span id="document-topics/stats"></span><section id="stats-collection">
<span id="topics-stats"></span><h3>Stats Collection<a class="headerlink" href="#stats-collection" title="Permalink to this heading">¶</a></h3>
<p>Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stats</span></code></a>
attribute of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a>, as illustrated by the examples in
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-stats-usecases"><span class="std std-ref">Common Stats Collector uses</span></a> section below.</p>
<p>However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it’s disabled, the API
will still work but it won’t collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you’re
using the Stats Collector from.</p>
<p>Another feature of the Stats Collector is that it’s very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.</p>
<section id="common-stats-collector-uses">
<span id="topics-stats-usecases"></span><h4>Common Stats Collector uses<a class="headerlink" href="#common-stats-collector-uses" title="Permalink to this heading">¶</a></h4>
<p>Access the stats collector through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stats</span></code></a>
attribute. Here is an example of an extension that access stats:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExtensionThatAccessStats</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">stats</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
</pre></div>
</div>
<p>Increment stat value:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">inc_value</span><span class="p">(</span><span class="s2">&quot;custom_count&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if greater than previous:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">max_value</span><span class="p">(</span><span class="s2">&quot;max_items_scraped&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if lower than previous:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">min_value</span><span class="p">(</span><span class="s2">&quot;min_free_memory_percent&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Get stat value:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s2">&quot;custom_count&quot;</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Get all stats:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="go">{&#39;custom_count&#39;: 1, &#39;start_time&#39;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}</span>
</pre></div>
</div>
</section>
<section id="available-stats-collectors">
<h4>Available Stats Collectors<a class="headerlink" href="#available-stats-collectors" title="Permalink to this heading">¶</a></h4>
<p>Besides the basic <code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code> there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-STATS_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_CLASS</span></code></a> setting. The
default Stats Collector used is the <code class="xref py py-class docutils literal notranslate"><span class="pre">MemoryStatsCollector</span></code>.</p>
<section id="memorystatscollector">
<h5>MemoryStatsCollector<a class="headerlink" href="#memorystatscollector" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.statscollectors.MemoryStatsCollector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.statscollectors.</span></span><span class="sig-name descname"><span class="pre">MemoryStatsCollector</span></span><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they’re closed. The stats can be accessed
through the <a class="reference internal" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="scrapy.statscollectors.MemoryStatsCollector.spider_stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">spider_stats</span></code></a> attribute, which is a dict keyed by spider
domain name.</p>
<p>This is the default Stats Collector used in Scrapy.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.statscollectors.MemoryStatsCollector.spider_stats">
<span class="sig-name descname"><span class="pre">spider_stats</span></span><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.</p>
</dd></dl>

</dd></dl>

</section>
<section id="dummystatscollector">
<h5>DummyStatsCollector<a class="headerlink" href="#dummystatscollector" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.statscollectors.DummyStatsCollector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.statscollectors.</span></span><span class="sig-name descname"><span class="pre">DummyStatsCollector</span></span><a class="headerlink" href="#scrapy.statscollectors.DummyStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-STATS_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_CLASS</span></code></a>
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.</p>
</dd></dl>

</section>
</section>
</section>
<span id="document-topics/email"></span><section id="module-scrapy.mail">
<span id="sending-e-mail"></span><span id="topics-email"></span><h3>Sending e-mail<a class="headerlink" href="#module-scrapy.mail" title="Permalink to this heading">¶</a></h3>
<p>Although Python makes sending e-mails relatively easy via the <a class="reference external" href="https://docs.python.org/3/library/smtplib.html#module-smtplib" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">smtplib</span></code></a>
library, Scrapy provides its own facility for sending e-mails which is very
easy to use and it’s implemented using <a class="reference external" href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v24.10)"><span class="xref std std-doc">Twisted non-blocking IO</span></a>, to avoid interfering with the non-blocking
IO of the crawler. It also provides a simple API for sending attachments and
it’s very easy to configure, with a few <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>.</p>
<section id="quick-example">
<h4>Quick example<a class="headerlink" href="#quick-example" title="Permalink to this heading">¶</a></h4>
<p>There are two ways to instantiate the mail sender. You can instantiate it using
the standard <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.mail</span> <span class="kn">import</span> <span class="n">MailSender</span>

<span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="p">()</span>
</pre></div>
</div>
<p>Or you can instantiate it passing a <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Crawler</span></code> instance, which
will respect the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">)</span>
</pre></div>
</div>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mailer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span>
    <span class="n">to</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;someone@example.com&quot;</span><span class="p">],</span>
    <span class="n">subject</span><span class="o">=</span><span class="s2">&quot;Some subject&quot;</span><span class="p">,</span>
    <span class="n">body</span><span class="o">=</span><span class="s2">&quot;Some body&quot;</span><span class="p">,</span>
    <span class="n">cc</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;another@example.com&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mailsender-class-reference">
<h4>MailSender class reference<a class="headerlink" href="#mailsender-class-reference" title="Permalink to this heading">¶</a></h4>
<p>MailSender is the preferred class to use for sending emails from Scrapy, as it
uses <a class="reference external" href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v24.10)"><span class="xref std std-doc">Twisted non-blocking IO</span></a>, like the
rest of the framework.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.mail.MailSender">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.mail.</span></span><span class="sig-name descname"><span class="pre">MailSender</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">smtphost</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mailfrom</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smtpuser</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smtppass</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smtpport</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>smtphost</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – the SMTP host to use for sending the emails. If omitted, the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MAIL_HOST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_HOST</span></code></a> setting will be used.</p></li>
<li><p><strong>mailfrom</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the address used to send emails (in the <code class="docutils literal notranslate"><span class="pre">From:</span></code> header).
If omitted, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MAIL_FROM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_FROM</span></code></a> setting will be used.</p></li>
<li><p><strong>smtpuser</strong> – the SMTP user. If omitted, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MAIL_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_USER</span></code></a>
setting will be used. If not given, no SMTP authentication will be
performed.</p></li>
<li><p><strong>smtppass</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – the SMTP pass for authentication.</p></li>
<li><p><strong>smtpport</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the SMTP port to connect to</p></li>
<li><p><strong>smtptls</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – enforce using SMTP STARTTLS</p></li>
<li><p><strong>smtpssl</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – enforce using a secure SSL connection</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.mail.MailSender.from_crawler">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate using a <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Crawler</span></code> instance, which will
respect <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-email-settings"><span class="std std-ref">these Scrapy settings</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> – the crawler</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.mail.MailSender.send">
<span class="sig-name descname"><span class="pre">send</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">to</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subject</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attachs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mimetype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'text/plain'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">charset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.send" title="Permalink to this definition">¶</a></dt>
<dd><p>Send email to the given recipients.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>to</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – the e-mail recipients as a string or as a list of strings</p></li>
<li><p><strong>subject</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the subject of the e-mail</p></li>
<li><p><strong>cc</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – the e-mails to CC as a string or as a list of strings</p></li>
<li><p><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the e-mail body</p></li>
<li><p><strong>attachs</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable" title="(in Python v3.13)"><em>collections.abc.Iterable</em></a>) – an iterable of tuples <code class="docutils literal notranslate"><span class="pre">(attach_name,</span> <span class="pre">mimetype,</span>
<span class="pre">file_object)</span></code> where  <code class="docutils literal notranslate"><span class="pre">attach_name</span></code> is a string with the name that will
appear on the e-mail’s attachment, <code class="docutils literal notranslate"><span class="pre">mimetype</span></code> is the mimetype of the
attachment and <code class="docutils literal notranslate"><span class="pre">file_object</span></code> is a readable file object with the
contents of the attachment</p></li>
<li><p><strong>mimetype</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the MIME type of the e-mail</p></li>
<li><p><strong>charset</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the character encoding to use for the e-mail contents</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="mail-settings">
<span id="topics-email-settings"></span><h4>Mail settings<a class="headerlink" href="#mail-settings" title="Permalink to this heading">¶</a></h4>
<p>These settings define the default <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method values of the <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a>
class, and can be used to configure e-mail notifications in your project without
writing any code (for those extensions and code that uses <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a>).</p>
<section id="mail-from">
<span id="std-setting-MAIL_FROM"></span><h5>MAIL_FROM<a class="headerlink" href="#mail-from" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy&#64;localhost'</span></code></p>
<p>Sender email to use (<code class="docutils literal notranslate"><span class="pre">From:</span></code> header) for sending emails.</p>
</section>
<section id="mail-host">
<span id="std-setting-MAIL_HOST"></span><h5>MAIL_HOST<a class="headerlink" href="#mail-host" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'localhost'</span></code></p>
<p>SMTP host to use for sending emails.</p>
</section>
<section id="mail-port">
<span id="std-setting-MAIL_PORT"></span><h5>MAIL_PORT<a class="headerlink" href="#mail-port" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">25</span></code></p>
<p>SMTP port to use for sending emails.</p>
</section>
<section id="mail-user">
<span id="std-setting-MAIL_USER"></span><h5>MAIL_USER<a class="headerlink" href="#mail-user" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.</p>
</section>
<section id="mail-pass">
<span id="std-setting-MAIL_PASS"></span><h5>MAIL_PASS<a class="headerlink" href="#mail-pass" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Password to use for SMTP authentication, along with <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-MAIL_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_USER</span></code></a>.</p>
</section>
<section id="mail-tls">
<span id="std-setting-MAIL_TLS"></span><h5>MAIL_TLS<a class="headerlink" href="#mail-tls" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.</p>
</section>
<section id="mail-ssl">
<span id="std-setting-MAIL_SSL"></span><h5>MAIL_SSL<a class="headerlink" href="#mail-ssl" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enforce connecting using an SSL encrypted connection</p>
</section>
</section>
</section>
<span id="document-topics/telnetconsole"></span><section id="telnet-console">
<span id="topics-telnetconsole"></span><h3>Telnet Console<a class="headerlink" href="#telnet-console" title="Permalink to this heading">¶</a></h3>
<p>Scrapy comes with a built-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.</p>
<p>The telnet console is a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref"><span class="std std-ref">built-in Scrapy extension</span></a> which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-telnetconsole"><span class="std std-ref">Telnet console extension</span></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is not secure to use telnet console via public networks, as telnet
doesn’t provide any transport-layer security. Having username/password
authentication doesn’t change that.</p>
<p>Intended usage is connecting to a running Scrapy spider locally
(spider process and telnet client are on the same machine)
or over a secure connection (VPN, SSH tunnel).
Please avoid using telnet console over insecure connections,
or disable it completely using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a> option.</p>
</div>
<section id="how-to-access-the-telnet-console">
<h4>How to access the telnet console<a class="headerlink" href="#how-to-access-the-telnet-console" title="Permalink to this heading">¶</a></h4>
<p>The telnet console listens in the TCP port defined in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PORT</span></code></a> setting, which defaults to <code class="docutils literal notranslate"><span class="pre">6023</span></code>. To access
the console you need to type:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
Trying localhost...
Connected to localhost.
Escape character is &#39;^]&#39;.
Username:
Password:
&gt;&gt;&gt;
</pre></div>
</div>
<p>By default Username is <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> and Password is autogenerated. The
autogenerated Password can be seen on Scrapy logs like the example below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>2018-10-16 14:35:21 [scrapy.extensions.telnet] INFO: Telnet Password: 16f92501e8a59326
</pre></div>
</div>
<p>Default Username and Password can be overridden by the settings
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TELNETCONSOLE_USERNAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_USERNAME</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-TELNETCONSOLE_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PASSWORD</span></code></a>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Username and password provide only a limited protection, as telnet
is not using secure transport - by default traffic is not encrypted
even if username and password are set.</p>
</div>
<p>You need the telnet program which comes installed by default in Windows, and
most Linux distros.</p>
</section>
<section id="available-variables-in-the-telnet-console">
<h4>Available variables in the telnet console<a class="headerlink" href="#available-variables-in-the-telnet-console" title="Permalink to this heading">¶</a></h4>
<p>The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.</p>
<p>However, the telnet console comes with some default variables defined for
convenience:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Shortcut</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">crawler</span></code></p></td>
<td><p>the Scrapy Crawler (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></a> object)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">engine</span></code></p></td>
<td><p>Crawler.engine attribute</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">spider</span></code></p></td>
<td><p>the active spider</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">slot</span></code></p></td>
<td><p>the engine slot</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">extensions</span></code></p></td>
<td><p>the Extension Manager (Crawler.extensions attribute)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">stats</span></code></p></td>
<td><p>the Stats Collector (Crawler.stats attribute)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">settings</span></code></p></td>
<td><p>the Scrapy settings object (Crawler.settings attribute)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">est</span></code></p></td>
<td><p>print a report of the engine status</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">prefs</span></code></p></td>
<td><p>for memory debugging (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">p</span></code></p></td>
<td><p>a shortcut to the <a class="reference external" href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">pprint.pprint()</span></code></a> function</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">hpy</span></code></p></td>
<td><p>for memory debugging (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="telnet-console-usage-examples">
<h4>Telnet console usage examples<a class="headerlink" href="#telnet-console-usage-examples" title="Permalink to this heading">¶</a></h4>
<p>Here are some example tasks you can do with the telnet console:</p>
<section id="view-engine-status">
<h5>View engine status<a class="headerlink" href="#view-engine-status" title="Permalink to this heading">¶</a></h5>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">est()</span></code> method of the Scrapy engine to quickly show its state
using the telnet console:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle()                         : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</pre></div>
</div>
</section>
<section id="pause-resume-and-stop-the-scrapy-engine">
<h5>Pause, resume and stop the Scrapy engine<a class="headerlink" href="#pause-resume-and-stop-the-scrapy-engine" title="Permalink to this heading">¶</a></h5>
<p>To pause:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To resume:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To stop:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
</pre></div>
</div>
</section>
</section>
<section id="telnet-console-signals">
<h4>Telnet Console signals<a class="headerlink" href="#telnet-console-signals" title="Permalink to this heading">¶</a></h4>
<span class="target" id="std-signal-update_telnet_vars"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.extensions.telnet.update_telnet_vars">
<span class="sig-prename descclassname"><span class="pre">scrapy.extensions.telnet.</span></span><span class="sig-name descname"><span class="pre">update_telnet_vars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">telnet_vars</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.telnet.update_telnet_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
<code class="docutils literal notranslate"><span class="pre">telnet_vars</span></code> dict in your handler.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>telnet_vars</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – the dict of telnet variables</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="telnet-settings">
<h4>Telnet settings<a class="headerlink" href="#telnet-settings" title="Permalink to this heading">¶</a></h4>
<p>These are the settings that control the telnet console’s behaviour:</p>
<section id="telnetconsole-port">
<span id="std-setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code>, a dynamically
assigned port is used.</p>
</section>
<section id="telnetconsole-host">
<span id="std-setting-TELNETCONSOLE_HOST"></span><h5>TELNETCONSOLE_HOST<a class="headerlink" href="#telnetconsole-host" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'127.0.0.1'</span></code></p>
<p>The interface the telnet console should listen on</p>
</section>
<section id="telnetconsole-username">
<span id="std-setting-TELNETCONSOLE_USERNAME"></span><h5>TELNETCONSOLE_USERNAME<a class="headerlink" href="#telnetconsole-username" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy'</span></code></p>
<p>The username used for the telnet console</p>
</section>
<section id="telnetconsole-password">
<span id="std-setting-TELNETCONSOLE_PASSWORD"></span><h5>TELNETCONSOLE_PASSWORD<a class="headerlink" href="#telnetconsole-password" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The password used for the telnet console, default behaviour is to have it
autogenerated</p>
</section>
</section>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-topics/logging"><span class="doc">Logging</span></a></dt><dd><p>Learn how to use Python’s builtin logging on Scrapy.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/stats"><span class="doc">Stats Collection</span></a></dt><dd><p>Collect statistics about your scraping crawler.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/email"><span class="doc">Sending e-mail</span></a></dt><dd><p>Send email notifications when certain events occur.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/telnetconsole"><span class="doc">Telnet Console</span></a></dt><dd><p>Inspect a running crawler using a built-in Python console.</p>
</dd>
</dl>
</section>
<section id="solving-specific-problems">
<h2>Solving specific problems<a class="headerlink" href="#solving-specific-problems" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-faq"></span><section id="frequently-asked-questions">
<span id="faq"></span><h3>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">¶</a></h3>
<section id="how-does-scrapy-compare-to-beautifulsoup-or-lxml">
<span id="faq-scrapy-bs-cmp"></span><h4>How does Scrapy compare to BeautifulSoup or lxml?<a class="headerlink" href="#how-does-scrapy-compare-to-beautifulsoup-or-lxml" title="Permalink to this heading">¶</a></h4>
<p><a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> and <a class="reference external" href="https://lxml.de/">lxml</a> are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>) but you can easily use <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>
(or <a class="reference external" href="https://lxml.de/">lxml</a>) instead, if you feel more comfortable working with them. After
all, they’re just parsing libraries which can be imported and used from any
Python code.</p>
<p>In other words, comparing <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (or <a class="reference external" href="https://lxml.de/">lxml</a>) to Scrapy is like
comparing <a class="reference external" href="https://palletsprojects.com/projects/jinja/">jinja2</a> to <a class="reference external" href="https://www.djangoproject.com/">Django</a>.</p>
</section>
<section id="can-i-use-scrapy-with-beautifulsoup">
<h4>Can I use Scrapy with BeautifulSoup?<a class="headerlink" href="#can-i-use-scrapy-with-beautifulsoup" title="Permalink to this heading">¶</a></h4>
<p>Yes, you can.
As mentioned <a class="hxr-hoverxref hxr-tooltip reference internal" href="#faq-scrapy-bs-cmp"><span class="std std-ref">above</span></a>, <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> can be used
for parsing HTML responses in Scrapy callbacks.
You just have to feed the response’s body into a <code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code> object
and extract whatever data you need from it.</p>
<p>Here’s an example spider using BeautifulSoup API, with <code class="docutils literal notranslate"><span class="pre">lxml</span></code> as the HTML parser:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">ExampleSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;http://www.example.com/&quot;</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># use lxml to get decent HTML parsing speed</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot;lxml&quot;</span><span class="p">)</span>
        <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">soup</span><span class="o">.</span><span class="n">h1</span><span class="o">.</span><span class="n">string</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code> supports several HTML/XML parsers.
See <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use">BeautifulSoup’s official documentation</a> on which ones are available.</p>
</div>
</section>
<section id="did-scrapy-steal-x-from-django">
<h4>Did Scrapy “steal” X from Django?<a class="headerlink" href="#did-scrapy-steal-x-from-django" title="Permalink to this heading">¶</a></h4>
<p>Probably, but we don’t like that word. We think <a class="reference external" href="https://www.djangoproject.com/">Django</a> is a great open source
project and an example to follow, so we’ve used it as an inspiration for
Scrapy.</p>
<p>We believe that, if something is already done well, there’s no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.</p>
<p>We’d be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!</p>
</section>
<section id="does-scrapy-work-with-http-proxies">
<h4>Does Scrapy work with HTTP proxies?<a class="headerlink" href="#does-scrapy-work-with-http-proxies" title="Permalink to this heading">¶</a></h4>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>.</p>
</section>
<section id="how-can-i-scrape-an-item-with-attributes-in-different-pages">
<h4>How can I scrape an item with attributes in different pages?<a class="headerlink" href="#how-can-i-scrape-an-item-with-attributes-in-different-pages" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</section>
<section id="how-can-i-simulate-a-user-login-in-my-spider">
<h4>How can I simulate a user login in my spider?<a class="headerlink" href="#how-can-i-simulate-a-user-login-in-my-spider" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
</section>
<section id="does-scrapy-crawl-in-breadth-first-or-depth-first-order">
<span id="faq-bfo-dfo"></span><h4>Does Scrapy crawl in breadth-first or depth-first order?<a class="headerlink" href="#does-scrapy-crawl-in-breadth-first-or-depth-first-order" title="Permalink to this heading">¶</a></h4>
<p>By default, Scrapy uses a <a class="reference external" href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">LIFO</a> queue for storing pending requests, which
basically means that it crawls in <a class="reference external" href="https://en.wikipedia.org/wiki/Depth-first_search">DFO order</a>. This order is more convenient
in most cases.</p>
<p>If you do want to crawl in true <a class="reference external" href="https://en.wikipedia.org/wiki/Breadth-first_search">BFO order</a>, you can do it by
setting the following settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DEPTH_PRIORITY</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">SCHEDULER_DISK_QUEUE</span> <span class="o">=</span> <span class="s2">&quot;scrapy.squeues.PickleFifoDiskQueue&quot;</span>
<span class="n">SCHEDULER_MEMORY_QUEUE</span> <span class="o">=</span> <span class="s2">&quot;scrapy.squeues.FifoMemoryQueue&quot;</span>
</pre></div>
</div>
<p>While pending requests are below the configured values of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>, those requests are sent
concurrently. As a result, the first few requests of a crawl rarely follow the
desired order. Lowering those settings to <code class="docutils literal notranslate"><span class="pre">1</span></code> enforces the desired order, but
it significantly slows down the crawl as a whole.</p>
</section>
<section id="my-scrapy-crawler-has-memory-leaks-what-can-i-do">
<h4>My Scrapy crawler has memory leaks. What can I do?<a class="headerlink" href="#my-scrapy-crawler-has-memory-leaks-what-can-i-do" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-without-leaks"><span class="std std-ref">Leaks without leaks</span></a>.</p>
</section>
<section id="how-can-i-make-scrapy-consume-less-memory">
<h4>How can I make Scrapy consume less memory?<a class="headerlink" href="#how-can-i-make-scrapy-consume-less-memory" title="Permalink to this heading">¶</a></h4>
<p>See previous question.</p>
</section>
<section id="how-can-i-prevent-memory-errors-due-to-many-allowed-domains">
<h4>How can I prevent memory errors due to many allowed domains?<a class="headerlink" href="#how-can-i-prevent-memory-errors-due-to-many-allowed-domains" title="Permalink to this heading">¶</a></h4>
<p>If you have a spider with a long list of <a class="reference internal" href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a>
(e.g. 50,000+), consider replacing the default
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a> downloader
middleware with a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware-custom"><span class="std std-ref">custom downloader middleware</span></a> that requires less memory. For example:</p>
<ul class="simple">
<li><p>If your domain names are similar enough, use your own regular expression
instead joining the strings in <a class="reference internal" href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> into
a complex regular expression.</p></li>
<li><p>If you can meet the installation requirements, use <a class="reference external" href="https://github.com/andreasvc/pyre2">pyre2</a> instead of
Python’s <a class="reference external" href="https://docs.python.org/3/library/re.html">re</a> to compile your URL-filtering regular expression. See
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1908">issue 1908</a>.</p></li>
</ul>
<p>See also <a class="reference external" href="https://stackoverflow.com/q/36440681">other suggestions at StackOverflow</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember to disable
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.offsite.OffsiteMiddleware</span></code></a> when you
enable your custom implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.offsite.OffsiteMiddleware&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;myproject.middlewares.CustomOffsiteMiddleware&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</section>
<section id="can-i-use-basic-http-authentication-in-my-spiders">
<h4>Can I use Basic HTTP Authentication in my spiders?<a class="headerlink" href="#can-i-use-basic-http-authentication-in-my-spiders" title="Permalink to this heading">¶</a></h4>
<p>Yes, see <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>.</p>
</section>
<section id="why-does-scrapy-download-pages-in-english-instead-of-my-native-language">
<h4>Why does Scrapy download pages in English instead of my native language?<a class="headerlink" href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-language" title="Permalink to this heading">¶</a></h4>
<p>Try changing the default <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> request header by overriding the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</section>
<section id="where-can-i-find-some-example-scrapy-projects">
<h4>Where can I find some example Scrapy projects?<a class="headerlink" href="#where-can-i-find-some-example-scrapy-projects" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-examples"><span class="std std-ref">Examples</span></a>.</p>
</section>
<section id="can-i-run-a-spider-without-creating-a-project">
<h4>Can I run a spider without creating a project?<a class="headerlink" href="#can-i-run-a-spider-without-creating-a-project" title="Permalink to this heading">¶</a></h4>
<p>Yes. You can use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command. For example, if you have a
spider written in a <code class="docutils literal notranslate"><span class="pre">my_spider.py</span></code> file you can run it with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">my_spider</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command for more info.</p>
</section>
<section id="i-get-filtered-offsite-request-messages-how-can-i-fix-them">
<h4>I get “Filtered offsite request” messages. How can I fix them?<a class="headerlink" href="#i-get-filtered-offsite-request-messages-how-can-i-fix-them" title="Permalink to this heading">¶</a></h4>
<p>Those messages (logged with <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level) don’t necessarily mean there is a
problem, so you may not need to fix them.</p>
<p>Those messages are thrown by
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a>, which is a
downloader middleware (enabled by default) whose purpose is to filter out
requests to domains outside the ones covered by the spider.</p>
</section>
<section id="what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">
<h4>What is the recommended way to deploy a Scrapy crawler in production?<a class="headerlink" href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-deploy"><span class="std std-ref">Deploying Spiders</span></a>.</p>
</section>
<section id="can-i-use-json-for-large-exports">
<h4>Can I use JSON for large exports?<a class="headerlink" href="#can-i-use-json-for-large-exports" title="Permalink to this heading">¶</a></h4>
<p>It’ll depend on how large your output is. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#json-with-large-data"><span class="std std-ref">this warning</span></a> in <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>
documentation.</p>
</section>
<section id="can-i-return-twisted-deferreds-from-signal-handlers">
<h4>Can I return (Twisted) deferreds from signal handlers?<a class="headerlink" href="#can-i-return-twisted-deferreds-from-signal-handlers" title="Permalink to this heading">¶</a></h4>
<p>Some signals support returning deferreds from their handlers, others don’t. See
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> to know which ones.</p>
</section>
<section id="what-does-the-response-status-code-999-mean">
<h4>What does the response status code 999 mean?<a class="headerlink" href="#what-does-the-response-status-code-999-mean" title="Permalink to this heading">¶</a></h4>
<p>999 is a custom response status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of <code class="docutils literal notranslate"><span class="pre">2</span></code> (or
higher) in your spider:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>

    <span class="n">download_delay</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># [ ... rest of the spider code ... ]</span>
</pre></div>
</div>
<p>Or by setting a global download delay in your project with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</p>
</section>
<section id="can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">
<h4>Can I call <code class="docutils literal notranslate"><span class="pre">pdb.set_trace()</span></code> from my spiders to debug them?<a class="headerlink" href="#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them" title="Permalink to this heading">¶</a></h4>
<p>Yes, but you can also use the Scrapy shell which allows you to quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old <code class="docutils literal notranslate"><span class="pre">pdb.set_trace()</span></code>.</p>
<p>For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
</section>
<section id="simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">
<h4>Simplest way to dump all my scraped items into a JSON/CSV/XML file?<a class="headerlink" href="#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file" title="Permalink to this heading">¶</a></h4>
<p>To dump into a JSON file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">O</span> <span class="n">items</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>To dump into a CSV file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">O</span> <span class="n">items</span><span class="o">.</span><span class="n">csv</span>
</pre></div>
</div>
<p>To dump into an XML file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">O</span> <span class="n">items</span><span class="o">.</span><span class="n">xml</span>
</pre></div>
</div>
<p>For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a></p>
</section>
<section id="what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">
<h4>What’s this huge cryptic <code class="docutils literal notranslate"><span class="pre">__VIEWSTATE</span></code> parameter used in some forms?<a class="headerlink" href="#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms" title="Permalink to this heading">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">__VIEWSTATE</span></code> parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see <a class="reference external" href="https://metacpan.org/release/ECARROLL/HTML-TreeBuilderX-ASP_NET-0.09/view/lib/HTML/TreeBuilderX/ASP_NET.pm">this page</a>. Also, here’s an <a class="reference external" href="https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example spider</a>
which scrapes one of these sites.</p>
</section>
<section id="what-s-the-best-way-to-parse-big-xml-csv-data-feeds">
<h4>What’s the best way to parse big XML/CSV data feeds?<a class="headerlink" href="#what-s-the-best-way-to-parse-big-xml-csv-data-feeds" title="Permalink to this heading">¶</a></h4>
<p>Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can use
the <a class="reference internal" href="#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a> and
<a class="reference internal" href="#scrapy.utils.iterators.csviter" title="scrapy.utils.iterators.csviter"><code class="xref py py-func docutils literal notranslate"><span class="pre">csviter()</span></code></a> functions. In fact, this is what
<a class="reference internal" href="index.html#scrapy.spiders.XMLFeedSpider" title="scrapy.spiders.XMLFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code></a> uses.</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.iterators.xmliter_lxml">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.iterators.</span></span><span class="sig-name descname"><span class="pre">xmliter_lxml</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">nodename</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">namespace</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'x'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><span class="pre">Selector</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.utils.iterators.xmliter_lxml" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.iterators.csviter">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.iterators.</span></span><span class="sig-name descname"><span class="pre">csviter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><span class="pre">Response</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">delimiter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">headers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quotechar</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.utils.iterators.csviter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator of dictionaries from the given csv object</p>
<p>obj can be:
- a Response object
- a unicode string
- a string encoded as utf-8</p>
<p>delimiter is the character used to separate fields on the given obj.</p>
<p>headers is an iterable that when provided offers the keys
for the returned dictionaries, if not the first row is used.</p>
<p>quotechar is the character used to enclosure fields on the given obj.</p>
</dd></dl>

</section>
<section id="does-scrapy-manage-cookies-automatically">
<h4>Does Scrapy manage cookies automatically?<a class="headerlink" href="#does-scrapy-manage-cookies-automatically" title="Permalink to this heading">¶</a></h4>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.</p>
<p>For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-response"><span class="std std-ref">Requests and Responses</span></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
</section>
<section id="how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">
<h4>How can I see the cookies being sent and received from Scrapy?<a class="headerlink" href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy" title="Permalink to this heading">¶</a></h4>
<p>Enable the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a> setting.</p>
</section>
<section id="how-can-i-instruct-a-spider-to-stop-itself">
<h4>How can I instruct a spider to stop itself?<a class="headerlink" href="#how-can-i-instruct-a-spider-to-stop-itself" title="Permalink to this heading">¶</a></h4>
<p>Raise the <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a> exception from a callback. For
more info see: <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a>.</p>
</section>
<section id="how-can-i-prevent-my-scrapy-bot-from-getting-banned">
<h4>How can I prevent my Scrapy bot from getting banned?<a class="headerlink" href="#how-can-i-prevent-my-scrapy-bot-from-getting-banned" title="Permalink to this heading">¶</a></h4>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#bans"><span class="std std-ref">Avoiding getting banned</span></a>.</p>
</section>
<section id="should-i-use-spider-arguments-or-settings-to-configure-my-spider">
<h4>Should I use spider arguments or settings to configure my spider?<a class="headerlink" href="#should-i-use-spider-arguments-or-settings-to-configure-my-spider" title="Permalink to this heading">¶</a></h4>
<p>Both <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">spider arguments</span></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don’t change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.</p>
</section>
<section id="i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">
<h4>I’m scraping a XML document and my XPath selector doesn’t return any items<a class="headerlink" href="#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items" title="Permalink to this heading">¶</a></h4>
<p>You may need to remove namespaces. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#removing-namespaces"><span class="std std-ref">Removing namespaces</span></a>.</p>
</section>
<section id="how-to-split-an-item-into-multiple-items-in-an-item-pipeline">
<span id="faq-split-item"></span><h4>How to split an item into multiple items in an item pipeline?<a class="headerlink" href="#how-to-split-an-item-into-multiple-items-in-an-item-pipeline" title="Permalink to this heading">¶</a></h4>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item pipelines</span></a> cannot yield multiple items per
input item. <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#custom-spider-middleware"><span class="std std-ref">Create a spider middleware</span></a>
instead, and use its
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method for this purpose. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">is_item</span><span class="p">,</span> <span class="n">ItemAdapter</span>


<span class="k">class</span> <span class="nc">MultiplyItemsMiddleware</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">process_spider_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_item</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
                <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;multiply_by&quot;</span><span class="p">]):</span>
                    <span class="k">yield</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="does-scrapy-support-ipv6-addresses">
<h4>Does Scrapy support IPv6 addresses?<a class="headerlink" href="#does-scrapy-support-ipv6-addresses" title="Permalink to this heading">¶</a></h4>
<p>Yes, by setting <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_RESOLVER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_RESOLVER</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.resolver.CachingHostnameResolver</span></code>.
Note that by doing so, you lose the ability to set a specific timeout for DNS requests
(the value of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code></a> setting is ignored).</p>
</section>
<section id="how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions">
<span id="faq-specific-reactor"></span><h4>How to deal with <code class="docutils literal notranslate"><span class="pre">&lt;class</span> <span class="pre">'ValueError'&gt;:</span> <span class="pre">filedescriptor</span> <span class="pre">out</span> <span class="pre">of</span> <span class="pre">range</span> <span class="pre">in</span> <span class="pre">select()</span></code> exceptions?<a class="headerlink" href="#how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions" title="Permalink to this heading">¶</a></h4>
<p>This issue <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2905">has been reported</a> to appear when running broad crawls in macOS, where the default
Twisted reactor is <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.selectreactor.SelectReactor.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">twisted.internet.selectreactor.SelectReactor</span></code></a>. Switching to a
different reactor is possible by using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting.</p>
</section>
<section id="how-can-i-cancel-the-download-of-a-given-response">
<span id="faq-stop-response-download"></span><h4>How can I cancel the download of a given response?<a class="headerlink" href="#how-can-i-cancel-the-download-of-a-given-response" title="Permalink to this heading">¶</a></h4>
<p>In some situations, it might be useful to stop the download of a certain response.
For instance, sometimes you can determine whether or not you need the full contents
of a response by inspecting its headers or the first bytes of its body. In that case,
you could save resources by attaching a handler to the <a class="reference internal" href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes_received</span></code></a>
or <a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a> signals and raising a
<a class="reference internal" href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload"><code class="xref py py-exc docutils literal notranslate"><span class="pre">StopDownload</span></code></a> exception. Please refer to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">Stopping the download of a Response</span></a> topic for additional information and examples.</p>
</section>
<section id="how-can-i-make-a-blank-request">
<span id="faq-blank-request"></span><h4>How can I make a blank request?<a class="headerlink" href="#how-can-i-make-a-blank-request" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Request</span>


<span class="n">blank_request</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="s2">&quot;data:,&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, the URL is set to a data URI scheme. Data URLs allow you to include data
inline within web pages, similar to external resources. The “data:” scheme with an empty
content (“,”) essentially creates a request to a data URL without any specific content.</p>
</section>
<section id="running-runspider-i-get-error-no-spider-found-in-file-filename">
<h4>Running <code class="docutils literal notranslate"><span class="pre">runspider</span></code> I get <code class="docutils literal notranslate"><span class="pre">error:</span> <span class="pre">No</span> <span class="pre">spider</span> <span class="pre">found</span> <span class="pre">in</span> <span class="pre">file:</span> <span class="pre">&lt;filename&gt;</span></code><a class="headerlink" href="#running-runspider-i-get-error-no-spider-found-in-file-filename" title="Permalink to this heading">¶</a></h4>
<p>This may happen if your Scrapy project has a spider module with a name that
conflicts with the name of one of the <a class="reference external" href="https://docs.python.org/3/py-modindex.html">Python standard library modules</a>, such
as <code class="docutils literal notranslate"><span class="pre">csv.py</span></code> or <code class="docutils literal notranslate"><span class="pre">os.py</span></code>, or any <a class="reference external" href="https://pypi.org/">Python package</a> that you have installed.
See <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2680">issue 2680</a>.</p>
</section>
</section>
<span id="document-topics/debug"></span><section id="debugging-spiders">
<span id="topics-debug"></span><h3>Debugging Spiders<a class="headerlink" href="#debugging-spiders" title="Permalink to this heading">¶</a></h3>
<p>This document explains the most common techniques for debugging spiders.
Consider the following Scrapy spider below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">MyItem</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;http://example.com/page1&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.com/page2&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># &lt;processing code not shown&gt;</span>
        <span class="c1"># collect `item_urls`</span>
        <span class="k">for</span> <span class="n">item_url</span> <span class="ow">in</span> <span class="n">item_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">item_url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_item</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># &lt;processing code not shown&gt;</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
        <span class="c1"># populate `item` fields</span>
        <span class="c1"># and extract item_details_url</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
            <span class="n">item_details_url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_details</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;item&quot;</span><span class="p">:</span> <span class="n">item</span><span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the <code class="docutils literal notranslate"><span class="pre">cb_kwargs</span></code> functionality of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> to pass a
partially populated item.</p>
<section id="parse-command">
<h4>Parse Command<a class="headerlink" href="#parse-command" title="Permalink to this heading">¶</a></h4>
<p>The most basic way of checking the output of your spider is to use the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">--verbose</span></code> or <code class="docutils literal notranslate"><span class="pre">-v</span></code> option we can see the status at each depth level:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Checking items scraped from a single start_url, can also be easily achieved
using:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -d 3 &#39;http://example.com/page1&#39;
</pre></div>
</div>
</section>
<section id="scrapy-shell">
<h4>Scrapy Shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this heading">¶</a></h4>
<p>While the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
<code class="docutils literal notranslate"><span class="pre">parse_details</span></code> sometimes receives no item?</p>
<p>Fortunately, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> is your bread and butter in this case (see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="kn">import</span> <span class="n">inspect_response</span>


<span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">item</span><span class="p">:</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
<p>See also: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
</section>
<section id="open-in-browser">
<h4>Open in browser<a class="headerlink" href="#open-in-browser" title="Permalink to this heading">¶</a></h4>
<p>Sometimes you just want to see how a certain response looks in a browser, you
can use the <a class="reference internal" href="#scrapy.utils.response.open_in_browser" title="scrapy.utils.response.open_in_browser"><code class="xref py py-func docutils literal notranslate"><span class="pre">open_in_browser()</span></code></a> function for that:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.response.open_in_browser">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.response.</span></span><span class="sig-name descname"><span class="pre">open_in_browser</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">response:</span> <span class="pre">TextResponse,</span> <span class="pre">_openfunc:</span> <span class="pre">Callable[[str],</span> <span class="pre">Any]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">open&gt;</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="headerlink" href="#scrapy.utils.response.open_in_browser" title="Permalink to this definition">¶</a></dt>
<dd><p>Open <em>response</em> in a local web browser, adjusting the <a class="reference external" href="https://www.w3schools.com/tags/tag_base.asp">base tag</a> for
external links to work, e.g. so that images and styles are displayed.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.utils.response</span> <span class="kn">import</span> <span class="n">open_in_browser</span>


<span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;item name&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="n">open_in_browser</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="Permalink to this heading">¶</a></h4>
<p>Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">item</span><span class="p">:</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No item received for </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, check the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a> section.</p>
</section>
<section id="visual-studio-code">
<span id="debug-vscode"></span><h4>Visual Studio Code<a class="headerlink" href="#visual-studio-code" title="Permalink to this heading">¶</a></h4>
<p>To debug spiders with Visual Studio Code you can use the following <code class="docutils literal notranslate"><span class="pre">launch.json</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;version&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0.1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;configurations&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Python: Launch Scrapy Spider&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;request&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;launch&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;module&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;scrapy&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="s2">&quot;runspider&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="s2">&quot;${file}&quot;</span>
<span class="w">            </span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;console&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;integratedTerminal&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Also, make sure you enable “User Uncaught Exceptions”, to catch exceptions in
your Scrapy spider.</p>
</section>
</section>
<span id="document-topics/contracts"></span><section id="spiders-contracts">
<span id="topics-contracts"></span><h3>Spiders Contracts<a class="headerlink" href="#spiders-contracts" title="Permalink to this heading">¶</a></h3>
<p>Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> and included in the docstring. See the
following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function parses a sample response. Some contracts are mingled</span>
<span class="sd">    with this docstring.</span>

<span class="sd">    @url http://www.example.com/s?field-keywords=selfish+gene</span>
<span class="sd">    @returns items 1 16</span>
<span class="sd">    @returns requests 0 0</span>
<span class="sd">    @scrapes Title Author Year Price</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>You can use the following contracts:</p>
<span class="target" id="module-scrapy.contracts.default"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.default.UrlContract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.default.</span></span><span class="sig-name descname"><span class="pre">UrlContract</span></span><a class="headerlink" href="#scrapy.contracts.default.UrlContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;url</span></code>) sets the sample URL used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@url</span> <span class="n">url</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.default.CallbackKeywordArgumentsContract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.default.</span></span><span class="sig-name descname"><span class="pre">CallbackKeywordArgumentsContract</span></span><a class="headerlink" href="#scrapy.contracts.default.CallbackKeywordArgumentsContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;cb_kwargs</span></code>) sets the <code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code>
attribute for the sample request. It must be a valid JSON dictionary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@cb_kwargs</span> <span class="p">{</span><span class="s2">&quot;arg1&quot;</span><span class="p">:</span> <span class="s2">&quot;value1&quot;</span><span class="p">,</span> <span class="s2">&quot;arg2&quot;</span><span class="p">:</span> <span class="s2">&quot;value2&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.default.MetadataContract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.default.</span></span><span class="sig-name descname"><span class="pre">MetadataContract</span></span><a class="headerlink" href="#scrapy.contracts.default.MetadataContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;meta</span></code>) sets the <code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code>
attribute for the sample request. It must be a valid JSON dictionary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@meta</span> <span class="p">{</span><span class="s2">&quot;arg1&quot;</span><span class="p">:</span> <span class="s2">&quot;value1&quot;</span><span class="p">,</span> <span class="s2">&quot;arg2&quot;</span><span class="p">:</span> <span class="s2">&quot;value2&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.default.ReturnsContract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.default.</span></span><span class="sig-name descname"><span class="pre">ReturnsContract</span></span><a class="headerlink" href="#scrapy.contracts.default.ReturnsContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;returns</span></code>) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@returns</span> <span class="n">item</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">|</span><span class="n">request</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="p">[</span><span class="nb">min</span> <span class="p">[</span><span class="nb">max</span><span class="p">]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.default.ScrapesContract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.default.</span></span><span class="sig-name descname"><span class="pre">ScrapesContract</span></span><a class="headerlink" href="#scrapy.contracts.default.ScrapesContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;scrapes</span></code>) checks that all the items returned by the
callback have the specified fields:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@scrapes</span> <span class="n">field_1</span> <span class="n">field_2</span> <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<p>Use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a> command to run the contract checks.</p>
<section id="custom-contracts">
<h4>Custom Contracts<a class="headerlink" href="#custom-contracts" title="Permalink to this heading">¶</a></h4>
<p>If you find you need more power than the built-in Scrapy contracts you can
create and load your own contracts in the project by using the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a> setting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;myproject.contracts.ResponseCheck&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;myproject.contracts.ItemValidate&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Each contract must inherit from <a class="reference internal" href="#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><code class="xref py py-class docutils literal notranslate"><span class="pre">Contract</span></code></a> and can
override three methods:</p>
<span class="target" id="module-scrapy.contracts"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.contracts.Contract">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.contracts.</span></span><span class="sig-name descname"><span class="pre">Contract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>collections.abc.Callable</em></a>) – callback function to which the contract is associated</p></li>
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a>) – list of arguments passed into the docstring (whitespace
separated)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.contracts.Contract.adjust_request_args">
<span class="sig-name descname"><span class="pre">adjust_request_args</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.adjust_request_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This receives a <code class="docutils literal notranslate"><span class="pre">dict</span></code> as an argument containing default arguments
for request object. <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> is used by default,
but this can be changed with the <code class="docutils literal notranslate"><span class="pre">request_cls</span></code> attribute.
If multiple contracts in chain have this attribute defined, the last one is used.</p>
<p>Must return the same or a modified version of it.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.contracts.Contract.pre_process">
<span class="sig-name descname"><span class="pre">pre_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.pre_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows hooking in various checks on the response received from the
sample request, before it’s being passed to the callback.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.contracts.Contract.post_process">
<span class="sig-name descname"><span class="pre">post_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows processing the output of the callback. Iterators are
converted to lists before being passed to this hook.</p>
</dd></dl>

</dd></dl>

<p>Raise <a class="reference internal" href="#scrapy.exceptions.ContractFail" title="scrapy.exceptions.ContractFail"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContractFail</span></code></a> from
<a class="reference internal" href="#scrapy.contracts.Contract.pre_process" title="scrapy.contracts.Contract.pre_process"><code class="xref py py-class docutils literal notranslate"><span class="pre">pre_process</span></code></a> or
<a class="reference internal" href="#scrapy.contracts.Contract.post_process" title="scrapy.contracts.Contract.post_process"><code class="xref py py-class docutils literal notranslate"><span class="pre">post_process</span></code></a> if expectations are not met:</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exceptions.ContractFail">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exceptions.</span></span><span class="sig-name descname"><span class="pre">ContractFail</span></span><a class="headerlink" href="#scrapy.exceptions.ContractFail" title="Permalink to this definition">¶</a></dt>
<dd><p>Error raised in case of a failing contract</p>
</dd></dl>

<p>Here is a demo contract which checks the presence of a custom header in the
response received:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.contracts</span> <span class="kn">import</span> <span class="n">Contract</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">ContractFail</span>


<span class="k">class</span> <span class="nc">HasHeaderContract</span><span class="p">(</span><span class="n">Contract</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demo contract which checks the presence of a custom header</span>
<span class="sd">    @has_header X-CustomHeader</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;has_header&quot;</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">header</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">header</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">ContractFail</span><span class="p">(</span><span class="s2">&quot;X-CustomHeader not present&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="detecting-check-runs">
<span id="detecting-contract-check-runs"></span><h4>Detecting check runs<a class="headerlink" href="#detecting-check-runs" title="Permalink to this heading">¶</a></h4>
<p>When <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> is running, the <code class="docutils literal notranslate"><span class="pre">SCRAPY_CHECK</span></code> environment variable is
set to the <code class="docutils literal notranslate"><span class="pre">true</span></code> string. You can use <a class="reference external" href="https://docs.python.org/3/library/os.html#os.environ" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">os.environ</span></code></a> to perform any change to
your spiders or your settings when <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> is used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">ExampleSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SCRAPY_CHECK&quot;</span><span class="p">):</span>
            <span class="k">pass</span>  <span class="c1"># Do some scraper adjustments when a check is running</span>
</pre></div>
</div>
</section>
</section>
<span id="document-topics/practices"></span><section id="common-practices">
<span id="topics-practices"></span><h3>Common Practices<a class="headerlink" href="#common-practices" title="Permalink to this heading">¶</a></h3>
<p>This section documents common practices when using Scrapy. These are things
that cover many topics and don’t often fall into any other specific section.</p>
<section id="run-scrapy-from-a-script">
<span id="run-from-script"></span><h4>Run Scrapy from a script<a class="headerlink" href="#run-scrapy-from-a-script" title="Permalink to this heading">¶</a></h4>
<p>You can use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api"><span class="std std-ref">API</span></a> to run Scrapy from a script, instead of
the typical way of running Scrapy via <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>.</p>
<p>Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need to run it inside the Twisted reactor.</p>
<p>The first utility you can use to run your spiders is
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerProcess</span></code></a>. This class will start a Twisted reactor
for you, configuring the logging and setting shutdown handlers. This class is
the one used by all Scrapy commands.</p>
<p>Here’s an example showing how to run a single spider with it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your spider definition</span>
    <span class="o">...</span>


<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span>
    <span class="n">settings</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;FEEDS&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;items.json&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">},</span>
        <span class="p">},</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>  <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<p>Define settings within dictionary in CrawlerProcess. Make sure to check <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>
documentation to get acquainted with its usage details.</p>
<p>If you are inside a Scrapy project there are some additional helpers you can
use to import those components within the project. You can automatically import
your spiders passing their name to <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>, and
use <code class="docutils literal notranslate"><span class="pre">get_project_settings</span></code> to get a <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a>
instance with your project settings.</p>
<p>What follows is a working example of how to do that, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a>
project as example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>

<span class="c1"># &#39;followall&#39; is the name of one of the spiders of the project.</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="s2">&quot;followall&quot;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s2">&quot;scrapy.org&quot;</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>  <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<p>There’s another Scrapy utility that provides more control over the crawling
process: <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a>. This class is a thin wrapper
that encapsulates some simple helpers to run multiple crawlers, but it won’t
start or interfere with existing reactors in any way.</p>
<p>Using this class the reactor should be explicitly run after scheduling your
spiders. It’s recommended you use <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>
instead of <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> if your application is
already using Twisted and you want to run Scrapy in the same reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by adding callbacks to the deferred
returned by the <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> method.</p>
<p>Here’s an example of its usage, along with a callback to manually stop the
reactor after <code class="docutils literal notranslate"><span class="pre">MySpider</span></code> has finished running.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="kn">import</span> <span class="n">configure_logging</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your spider definition</span>
    <span class="o">...</span>


<span class="n">configure_logging</span><span class="p">({</span><span class="s2">&quot;LOG_FORMAT&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">%(levelname)s</span><span class="s2">: </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">})</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">()</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

<span class="n">d</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">())</span>
<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>  <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<p>Same example but using a non-default reactor, it’s only necessary call
<code class="docutils literal notranslate"><span class="pre">install_reactor</span></code> if you are using <code class="docutils literal notranslate"><span class="pre">CrawlerRunner</span></code> since <code class="docutils literal notranslate"><span class="pre">CrawlerProcess</span></code> already does this automatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="kn">import</span> <span class="n">configure_logging</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your spider definition</span>
    <span class="o">...</span>


<span class="n">configure_logging</span><span class="p">({</span><span class="s2">&quot;LOG_FORMAT&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">%(levelname)s</span><span class="s2">: </span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">})</span>

<span class="kn">from</span> <span class="nn">scrapy.utils.reactor</span> <span class="kn">import</span> <span class="n">install_reactor</span>

<span class="n">install_reactor</span><span class="p">(</span><span class="s2">&quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;</span><span class="p">)</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">()</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

<span class="n">d</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">())</span>
<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>  <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://docs.twisted.org/en/stable/core/howto/reactor-basics.html" title="(in Twisted v24.10)"><span>Reactor Overview</span></a></p>
</div>
</section>
<section id="running-multiple-spiders-in-the-same-process">
<span id="run-multiple-spiders"></span><h4>Running multiple spiders in the same process<a class="headerlink" href="#running-multiple-spiders-in-the-same-process" title="Permalink to this heading">¶</a></h4>
<p>By default, Scrapy runs a single spider per process when you run <code class="docutils literal notranslate"><span class="pre">scrapy</span>
<span class="pre">crawl</span></code>. However, Scrapy supports running multiple spiders per process using
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api"><span class="std std-ref">internal API</span></a>.</p>
<p>Here is an example that runs multiple spiders simultaneously:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>


<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>


<span class="n">settings</span> <span class="o">=</span> <span class="n">get_project_settings</span><span class="p">()</span>
<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>  <span class="c1"># the script will block here until all crawling jobs are finished</span>
</pre></div>
</div>
<p>Same example using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="kn">import</span> <span class="n">configure_logging</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>


<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>


<span class="n">configure_logging</span><span class="p">()</span>
<span class="n">settings</span> <span class="o">=</span> <span class="n">get_project_settings</span><span class="p">()</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

<span class="n">d</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">())</span>

<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>  <span class="c1"># the script will block here until all crawling jobs are finished</span>
</pre></div>
</div>
<p>Same example but running the spiders sequentially by chaining the deferreds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">defer</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="kn">import</span> <span class="n">configure_logging</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="kn">import</span> <span class="n">get_project_settings</span>


<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>


<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>


<span class="n">settings</span> <span class="o">=</span> <span class="n">get_project_settings</span><span class="p">()</span>
<span class="n">configure_logging</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>


<span class="nd">@defer</span><span class="o">.</span><span class="n">inlineCallbacks</span>
<span class="k">def</span> <span class="nf">crawl</span><span class="p">():</span>
    <span class="k">yield</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
    <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>


<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

<span class="n">crawl</span><span class="p">()</span>
<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>  <span class="c1"># the script will block here until the last crawl call is finished</span>
</pre></div>
</div>
<p>Different spiders can set different values for the same setting, but when they
run in the same process it may be impossible, by design or because of some
limitations, to use these different values. What happens in practice is
different for different settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> and the ones used by its value
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span></code></a> for the
default one) cannot be read from the per-spider settings. These are applied
when the <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> or
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> object is created.</p></li>
<li><p>For <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ASYNCIO_EVENT_LOOP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ASYNCIO_EVENT_LOOP</span></code></a> the first
available value is used, and if a spider requests a different reactor an
exception will be raised. These are applied when the reactor is installed.</p></li>
<li><p>For <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_RESOLVER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_RESOLVER</span></code></a> and the
ones used by the resolver (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_SIZE</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code></a> for ones included in Scrapy)
the first available value is used. These are applied when the reactor is
started.</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
</div>
</section>
<section id="distributed-crawls">
<span id="id1"></span><h4>Distributed crawls<a class="headerlink" href="#distributed-crawls" title="Permalink to this heading">¶</a></h4>
<p>Scrapy doesn’t provide any built-in facility for running crawls in a distribute
(multi-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part1</span><span class="o">.</span><span class="n">list</span>
<span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part2</span><span class="o">.</span><span class="n">list</span>
<span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part3</span><span class="o">.</span><span class="n">list</span>
</pre></div>
</div>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument <code class="docutils literal notranslate"><span class="pre">part</span></code> with the number of the partition to
crawl:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy1</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">1</span>
<span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy2</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">2</span>
<span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy3</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">3</span>
</pre></div>
</div>
</section>
<section id="avoiding-getting-banned">
<span id="bans"></span><h4>Avoiding getting banned<a class="headerlink" href="#avoiding-getting-banned" title="Permalink to this heading">¶</a></h4>
<p>Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting <a class="reference external" href="https://scrapy.org/support/">commercial support</a> if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kinds of sites:</p>
<ul class="simple">
<li><p>rotate your user agent from a pool of well-known ones from browsers (google
around to get a list of them)</p></li>
<li><p>disable cookies (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a>) as some sites may use
cookies to spot bot behaviour</p></li>
<li><p>use download delays (2 or higher). See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</p></li>
<li><p>if possible, use <a class="reference external" href="https://commoncrawl.org/">Common Crawl</a> to fetch pages, instead of hitting the sites
directly</p></li>
<li><p>use a pool of rotating IPs. For example, the free <a class="reference external" href="https://www.torproject.org/">Tor project</a> or paid
services like <a class="reference external" href="https://proxymesh.com/">ProxyMesh</a>. An open source alternative is <a class="reference external" href="https://scrapoxy.io/">scrapoxy</a>, a
super proxy that you can attach your own proxies to.</p></li>
<li><p>use a ban avoidance service, such as <a class="reference external" href="https://docs.zyte.com/zyte-api/get-started.html">Zyte API</a>, which provides a <a class="reference external" href="https://github.com/scrapy-plugins/scrapy-zyte-api">Scrapy
plugin</a> and additional
features, like <a class="reference external" href="https://www.zyte.com/ai-web-scraping/">AI web scraping</a></p></li>
</ul>
<p>If you are still unable to prevent your bot getting banned, consider contacting
<a class="reference external" href="https://scrapy.org/support/">commercial support</a>.</p>
</section>
</section>
<span id="document-topics/broad-crawls"></span><section id="broad-crawls">
<span id="topics-broad-crawls"></span><h3>Broad Crawls<a class="headerlink" href="#broad-crawls" title="Permalink to this heading">¶</a></h3>
<p>Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).</p>
<p>In addition to this “focused crawl”, there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called “broad crawls” and is the typical crawlers employed by search
engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<ul class="simple">
<li><p>they crawl many domains (often, unbounded) instead of a specific set of sites</p></li>
<li><p>they don’t necessarily crawl domains to completion, because it would be
impractical (or impossible) to do so, and instead limit the crawl by time or
number of pages crawled</p></li>
<li><p>they are simpler in logic (as opposed to very complex spiders with many
extraction rules) because data is often post-processed in a separate stage</p></li>
<li><p>they crawl many domains concurrently, which allows them to achieve faster
crawl speeds by not being limited by any particular site constraint (each site
is crawled slowly to respect politeness, but many sites are crawled in
parallel)</p></li>
</ul>
<p>As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarizes some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.</p>
<section id="use-the-right-scheduler-priority-queue">
<span id="broad-crawls-scheduler-priority-queue"></span><h4>Use the right <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a><a class="headerlink" href="#use-the-right-scheduler-priority-queue" title="Permalink to this heading">¶</a></h4>
<p>Scrapy’s default scheduler priority queue is <code class="docutils literal notranslate"><span class="pre">'scrapy.pqueues.ScrapyPriorityQueue'</span></code>.
It works best during single-domain crawl. It does not work well with crawling
many different domains in parallel</p>
<p>To apply the recommended priority queue use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SCHEDULER_PRIORITY_QUEUE</span> <span class="o">=</span> <span class="s2">&quot;scrapy.pqueues.DownloaderAwarePriorityQueue&quot;</span>
</pre></div>
</div>
</section>
<section id="increase-concurrency">
<span id="broad-crawls-concurrency"></span><h4>Increase concurrency<a class="headerlink" href="#increase-concurrency" title="Permalink to this heading">¶</a></h4>
<p>Concurrency is the number of requests that are processed in parallel. There is
a global limit (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>) and an additional limit that
can be set either per domain (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>) or per
IP (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The scheduler priority queue <a class="hxr-hoverxref hxr-tooltip reference internal" href="#broad-crawls-scheduler-priority-queue"><span class="std std-ref">recommended for broad crawls</span></a> does not support
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>.</p>
</div>
<p>The default global concurrency limit in Scrapy is not suitable for crawling
many different domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU and memory your crawler will have
available.</p>
<p>A good starting point is <code class="docutils literal notranslate"><span class="pre">100</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">CONCURRENT_REQUESTS</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>But the best way to find out is by doing some trials and identifying at what
concurrency your Scrapy process gets CPU bounded. For optimum performance, you
should pick a concurrency where CPU usage is at 80-90%.</p>
<p>Increasing concurrency also increases memory usage. If memory usage is a
concern, you might need to lower your global concurrency limit accordingly.</p>
</section>
<section id="increase-twisted-io-thread-pool-maximum-size">
<h4>Increase Twisted IO thread pool maximum size<a class="headerlink" href="#increase-twisted-io-thread-pool-maximum-size" title="Permalink to this heading">¶</a></h4>
<p>Currently Scrapy does DNS resolution in a blocking way with usage of thread
pool. With higher concurrency levels the crawling could be slow or even fail
hitting DNS resolver timeouts. Possible solution to increase the number of
threads handling DNS queries. The DNS queue will be processed faster speeding
up establishing of connection and crawling overall.</p>
<p>To increase maximum thread pool size use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">REACTOR_THREADPOOL_MAXSIZE</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</section>
<section id="setup-your-own-dns">
<h4>Setup your own DNS<a class="headerlink" href="#setup-your-own-dns" title="Permalink to this heading">¶</a></h4>
<p>If you have multiple crawling processes and single central DNS, it can act
like DoS attack on the DNS server resulting to slow down of entire network or
even blocking your machines. To avoid this setup your own DNS server with
local cache and upstream to some large DNS like OpenDNS or Verizon.</p>
</section>
<section id="reduce-log-level">
<h4>Reduce log level<a class="headerlink" href="#reduce-log-level" title="Permalink to this heading">¶</a></h4>
<p>When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
<code class="docutils literal notranslate"><span class="pre">INFO</span></code> log level. In order to save CPU (and log storage requirements) you
should not use <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> log level when performing large broad crawls in
production. Using <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level when developing your (broad) crawler may be
fine though.</p>
<p>To set the log level use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">LOG_LEVEL</span> <span class="o">=</span> <span class="s2">&quot;INFO&quot;</span>
</pre></div>
</div>
</section>
<section id="disable-cookies">
<h4>Disable cookies<a class="headerlink" href="#disable-cookies" title="Permalink to this heading">¶</a></h4>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.</p>
<p>To disable cookies use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">COOKIES_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
<section id="disable-retries">
<h4>Disable retries<a class="headerlink" href="#disable-retries" title="Permalink to this heading">¶</a></h4>
<p>Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.</p>
<p>To disable retries use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">RETRY_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
<section id="reduce-download-timeout">
<h4>Reduce download timeout<a class="headerlink" href="#reduce-download-timeout" title="Permalink to this heading">¶</a></h4>
<p>Unless you are crawling from a very slow connection (which shouldn’t be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.</p>
<p>To reduce the download timeout use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_TIMEOUT</span> <span class="o">=</span> <span class="mi">15</span>
</pre></div>
</div>
</section>
<section id="disable-redirects">
<h4>Disable redirects<a class="headerlink" href="#disable-redirects" title="Permalink to this heading">¶</a></h4>
<p>Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it’s common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.</p>
<p>To disable redirects use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">REDIRECT_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</section>
<section id="enable-crawling-of-ajax-crawlable-pages">
<h4>Enable crawling of “Ajax Crawlable Pages”<a class="headerlink" href="#enable-crawling-of-ajax-crawlable-pages" title="Permalink to this heading">¶</a></h4>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as ajax crawlable. This means they provide plain HTML
version of content that is usually available only via AJAX.
Pages can indicate it in two ways:</p>
<ol class="arabic simple">
<li><p>by using <code class="docutils literal notranslate"><span class="pre">#!</span></code> in URL - this is the default way;</p></li>
<li><p>by using a special meta tag - this way is used on
“main”, “index” website pages.</p></li>
</ol>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#ajaxcrawl-middleware"><span class="std std-ref">AjaxCrawlMiddleware</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AJAXCRAWL_ENABLED</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>When doing broad crawls it’s common to crawl a lot of “index” web pages;
AjaxCrawlMiddleware helps to crawl them correctly.
It is turned OFF by default because it has some performance overhead,
and enabling it for focused crawls doesn’t make much sense.</p>
</section>
<section id="crawl-in-bfo-order">
<span id="broad-crawls-bfo"></span><h4>Crawl in BFO order<a class="headerlink" href="#crawl-in-bfo-order" title="Permalink to this heading">¶</a></h4>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Scrapy crawls in DFO order by default</span></a>.</p>
<p>In broad crawls, however, page crawling tends to be faster than page
processing. As a result, unprocessed early requests stay in memory until the
final depth is reached, which can significantly increase memory usage.</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Crawl in BFO order</span></a> instead to save memory.</p>
</section>
<section id="be-mindful-of-memory-leaks">
<h4>Be mindful of memory leaks<a class="headerlink" href="#be-mindful-of-memory-leaks" title="Permalink to this heading">¶</a></h4>
<p>If your broad crawl shows a high memory usage, in addition to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#broad-crawls-bfo"><span class="std std-ref">crawling in
BFO order</span></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#broad-crawls-concurrency"><span class="std std-ref">lowering concurrency</span></a> you should <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks"><span class="std std-ref">debug your memory leaks</span></a>.</p>
</section>
<section id="install-a-specific-twisted-reactor">
<h4>Install a specific Twisted reactor<a class="headerlink" href="#install-a-specific-twisted-reactor" title="Permalink to this heading">¶</a></h4>
<p>If the crawl is exceeding the system’s capabilities, you might want to try
installing a specific Twisted reactor, via the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting.</p>
</section>
</section>
<span id="document-topics/developer-tools"></span><section id="using-your-browser-s-developer-tools-for-scraping">
<span id="topics-developer-tools"></span><h3>Using your browser’s Developer Tools for scraping<a class="headerlink" href="#using-your-browser-s-developer-tools-for-scraping" title="Permalink to this heading">¶</a></h3>
<p>Here is a general guide on how to use your browser’s Developer Tools
to ease the scraping process. Today almost all browsers come with
built in <a class="reference external" href="https://en.wikipedia.org/wiki/Web_development_tools">Developer Tools</a> and although we will use Firefox in this
guide, the concepts are applicable to any other browser.</p>
<p>In this guide we’ll introduce the basic tools to use from a browser’s
Developer Tools by scraping <a class="reference external" href="https://quotes.toscrape.com">quotes.toscrape.com</a>.</p>
<section id="caveats-with-inspecting-the-live-browser-dom">
<span id="topics-livedom"></span><h4>Caveats with inspecting the live browser DOM<a class="headerlink" href="#caveats-with-inspecting-the-live-browser-dom" title="Permalink to this heading">¶</a></h4>
<p>Since Developer Tools operate on a live browser DOM, what you’ll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing JavaScript code.  Firefox,
in particular, is known for adding <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won’t be able to
extract any data if you use <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things:</p>
<ul class="simple">
<li><p>Disable JavaScript while inspecting the DOM looking for XPaths to be
used in Scrapy (in the Developer Tools settings click <cite>Disable JavaScript</cite>)</p></li>
<li><p>Never use full XPath paths, use relative and clever ones based on attributes
(such as <code class="docutils literal notranslate"><span class="pre">id</span></code>, <code class="docutils literal notranslate"><span class="pre">class</span></code>, <code class="docutils literal notranslate"><span class="pre">width</span></code>, etc) or any identifying features like
<code class="docutils literal notranslate"><span class="pre">contains(&#64;href,</span> <span class="pre">'image')</span></code>.</p></li>
<li><p>Never include <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> elements in your XPath expressions unless you
really know what you’re doing</p></li>
</ul>
</section>
<section id="inspecting-a-website">
<span id="topics-inspector"></span><h4>Inspecting a website<a class="headerlink" href="#inspecting-a-website" title="Permalink to this heading">¶</a></h4>
<p>By far the most handy feature of the Developer Tools is the <cite>Inspector</cite>
feature, which allows you to inspect the underlying HTML code of
any webpage. To demonstrate the Inspector, let’s look at the
<a class="reference external" href="https://quotes.toscrape.com">quotes.toscrape.com</a>-site.</p>
<p>On the site we have a total of ten quotes from various authors with specific
tags, as well as the Top Ten Tags. Let’s say we want to extract all the quotes
on this page, without any meta-information about authors, tags, etc.</p>
<p>Instead of viewing the whole source code for the page, we can simply right click
on a quote and select <code class="docutils literal notranslate"><span class="pre">Inspect</span> <span class="pre">Element</span> <span class="pre">(Q)</span></code>, which opens up the <cite>Inspector</cite>.
In it you should see something like this:</p>
<a class="reference internal image-reference" href="_images/inspector_01.png"><img alt="Firefox's Inspector-tool" src="_images/inspector_01.png" style="width: 777px; height: 469px;" /></a>
<p>The interesting part for us is this:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span> <span class="na">itemscope</span><span class="o">=</span><span class="s">&quot;&quot;</span> <span class="na">itemtype</span><span class="o">=</span><span class="s">&quot;http://schema.org/CreativeWork&quot;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span> <span class="na">itemprop</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>If you hover over the first <code class="docutils literal notranslate"><span class="pre">div</span></code> directly above the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag highlighted
in the screenshot, you’ll see that the corresponding section of the webpage gets
highlighted as well. So now we have a section, but we can’t find our quote text
anywhere.</p>
<p>The advantage of the <cite>Inspector</cite> is that it automatically expands and collapses
sections and tags of a webpage, which greatly improves readability. You can
expand and collapse a tag by clicking on the arrow in front of it or by double
clicking directly on the tag. If we expand the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=</span>
<span class="pre">&quot;text&quot;</span></code> we will see the quote-text we clicked on. The <cite>Inspector</cite> lets you
copy XPaths to selected elements. Let’s try it out.</p>
<p>First open the Scrapy shell at <a class="reference external" href="https://quotes.toscrape.com/">https://quotes.toscrape.com/</a> in a terminal:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy shell &quot;https://quotes.toscrape.com/&quot;
</pre></div>
</div>
<p>Then, back to your web browser, right-click on the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag, select
<code class="docutils literal notranslate"><span class="pre">Copy</span> <span class="pre">&gt;</span> <span class="pre">XPath</span></code> and paste it in the Scrapy shell like so:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;/html/body/div/div[2]/div[1]/div[1]/span[1]/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;]</span>
</pre></div>
</div>
<p>Adding <code class="docutils literal notranslate"><span class="pre">text()</span></code> at the end we are able to extract the first quote with this
basic selector. But this XPath is not really that clever. All it does is
go down a desired path in the source code starting from <code class="docutils literal notranslate"><span class="pre">html</span></code>. So let’s
see if we can refine our XPath a bit:</p>
<p>If we check the <cite>Inspector</cite> again we’ll see that directly beneath our
expanded <code class="docutils literal notranslate"><span class="pre">div</span></code> tag we have nine identical <code class="docutils literal notranslate"><span class="pre">div</span></code> tags, each with the
same attributes as our first. If we expand any of them, we’ll see the same
structure as with our first quote: Two <code class="docutils literal notranslate"><span class="pre">span</span></code> tags and one <code class="docutils literal notranslate"><span class="pre">div</span></code> tag. We can
expand each <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> inside our <code class="docutils literal notranslate"><span class="pre">div</span></code> tags and
see each quote:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span> <span class="na">itemscope</span><span class="o">=</span><span class="s">&quot;&quot;</span> <span class="na">itemtype</span><span class="o">=</span><span class="s">&quot;http://schema.org/CreativeWork&quot;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span> <span class="na">itemprop</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>
    “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”
  <span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>With this knowledge we can refine our XPath: Instead of a path to follow,
we’ll simply select all <code class="docutils literal notranslate"><span class="pre">span</span></code> tags with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> by using
the <a class="reference external" href="https://parsel.readthedocs.io/en/latest/usage.html#other-xpath-extensions">has-class-extension</a>:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span[has-class(&quot;text&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;,</span>
<span class="go">&#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#39;,</span>
<span class="go">&#39;“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”&#39;,</span>
<span class="go">...]</span>
</pre></div>
</div>
<p>And with one simple, cleverer XPath we are able to extract all quotes from
the page. We could have constructed a loop over our first XPath to increase
the number of the last <code class="docutils literal notranslate"><span class="pre">div</span></code>, but this would have been unnecessarily
complex and by simply constructing an XPath with <code class="docutils literal notranslate"><span class="pre">has-class(&quot;text&quot;)</span></code>
we were able to extract all quotes in one line.</p>
<p>The <cite>Inspector</cite> has a lot of other helpful features, such as searching in the
source code or directly scrolling to an element you selected. Let’s demonstrate
a use case:</p>
<p>Say you want to find the <code class="docutils literal notranslate"><span class="pre">Next</span></code> button on the page. Type <code class="docutils literal notranslate"><span class="pre">Next</span></code> into the
search bar on the top right of the <cite>Inspector</cite>. You should get two results.
The first is a <code class="docutils literal notranslate"><span class="pre">li</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;next&quot;</span></code>, the second the text
of an <code class="docutils literal notranslate"><span class="pre">a</span></code> tag. Right click on the <code class="docutils literal notranslate"><span class="pre">a</span></code> tag and select <code class="docutils literal notranslate"><span class="pre">Scroll</span> <span class="pre">into</span> <span class="pre">View</span></code>.
If you hover over the tag, you’ll see the button highlighted. From here
we could easily create a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractor</span></a> to
follow the pagination. On a simple site such as this, there may not be
the need to find an element visually but the <code class="docutils literal notranslate"><span class="pre">Scroll</span> <span class="pre">into</span> <span class="pre">View</span></code> function
can be quite useful on complex sites.</p>
<p>Note that the search bar can also be used to search for and test CSS
selectors. For example, you could search for <code class="docutils literal notranslate"><span class="pre">span.text</span></code> to find
all quote texts. Instead of a full text search, this searches for
exactly the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> in the page.</p>
</section>
<section id="the-network-tool">
<span id="topics-network-tool"></span><h4>The Network-tool<a class="headerlink" href="#the-network-tool" title="Permalink to this heading">¶</a></h4>
<p>While scraping you may come across dynamic webpages where some parts
of the page are loaded dynamically through multiple requests. While
this can be quite tricky, the <cite>Network</cite>-tool in the Developer Tools
greatly facilitates this task. To demonstrate the Network-tool, let’s
take a look at the page <a class="reference external" href="https://quotes.toscrape.com/scroll">quotes.toscrape.com/scroll</a>.</p>
<p>The page is quite similar to the basic <a class="reference external" href="https://quotes.toscrape.com">quotes.toscrape.com</a>-page,
but instead of the above-mentioned <code class="docutils literal notranslate"><span class="pre">Next</span></code> button, the page
automatically loads new quotes when you scroll to the bottom. We
could go ahead and try out different XPaths directly, but instead
we’ll check another quite useful command from the Scrapy shell:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ scrapy shell &quot;quotes.toscrape.com/scroll&quot;
(...)
&gt;&gt;&gt; view(response)
</pre></div>
</div>
<p>A browser window should open with the webpage but with one
crucial difference: Instead of the quotes we just see a greenish
bar with the word <code class="docutils literal notranslate"><span class="pre">Loading...</span></code>.</p>
<a class="reference internal image-reference" href="_images/network_01.png"><img alt="Response from quotes.toscrape.com/scroll" src="_images/network_01.png" style="width: 777px; height: 296px;" /></a>
<p>The <code class="docutils literal notranslate"><span class="pre">view(response)</span></code> command let’s us view the response our
shell or later our spider receives from the server. Here we see
that some basic template is loaded which includes the title,
the login-button and the footer, but the quotes are missing. This
tells us that the quotes are being loaded from a different request
than <code class="docutils literal notranslate"><span class="pre">quotes.toscrape/scroll</span></code>.</p>
<p>If you click on the <code class="docutils literal notranslate"><span class="pre">Network</span></code> tab, you will probably only see
two entries. The first thing we do is enable persistent logs by
clicking on <code class="docutils literal notranslate"><span class="pre">Persist</span> <span class="pre">Logs</span></code>. If this option is disabled, the
log is automatically cleared each time you navigate to a different
page. Enabling this option is a good default, since it gives us
control on when to clear the logs.</p>
<p>If we reload the page now, you’ll see the log get populated with six
new requests.</p>
<a class="reference internal image-reference" href="_images/network_02.png"><img alt="Network tab with persistent logs and requests" src="_images/network_02.png" style="width: 777px; height: 241px;" /></a>
<p>Here we see every request that has been made when reloading the page
and can inspect each request and its response. So let’s find out
where our quotes are coming from:</p>
<p>First click on the request with the name <code class="docutils literal notranslate"><span class="pre">scroll</span></code>. On the right
you can now inspect the request. In <code class="docutils literal notranslate"><span class="pre">Headers</span></code> you’ll find details
about the request headers, such as the URL, the method, the IP-address,
and so on. We’ll ignore the other tabs and click directly on <code class="docutils literal notranslate"><span class="pre">Response</span></code>.</p>
<p>What you should see in the <code class="docutils literal notranslate"><span class="pre">Preview</span></code> pane is the rendered HTML-code,
that is exactly what we saw when we called <code class="docutils literal notranslate"><span class="pre">view(response)</span></code> in the
shell. Accordingly the <code class="docutils literal notranslate"><span class="pre">type</span></code> of the request in the log is <code class="docutils literal notranslate"><span class="pre">html</span></code>.
The other requests have types like <code class="docutils literal notranslate"><span class="pre">css</span></code> or <code class="docutils literal notranslate"><span class="pre">js</span></code>, but what
interests us is the one request called <code class="docutils literal notranslate"><span class="pre">quotes?page=1</span></code> with the
type <code class="docutils literal notranslate"><span class="pre">json</span></code>.</p>
<p>If we click on this request, we see that the request URL is
<code class="docutils literal notranslate"><span class="pre">https://quotes.toscrape.com/api/quotes?page=1</span></code> and the response
is a JSON-object that contains our quotes. We can also right-click
on the request and open <code class="docutils literal notranslate"><span class="pre">Open</span> <span class="pre">in</span> <span class="pre">new</span> <span class="pre">tab</span></code> to get a better overview.</p>
<a class="reference internal image-reference" href="_images/network_03.png"><img alt="JSON-object returned from the quotes.toscrape API" src="_images/network_03.png" style="width: 777px; height: 375px;" /></a>
<p>With this response we can now easily parse the JSON-object and
also request each page to get every quote on the site:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">import</span> <span class="nn">json</span>


<span class="k">class</span> <span class="nc">QuoteSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quote&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;quotes.toscrape.com&quot;</span><span class="p">]</span>
    <span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://quotes.toscrape.com/api/quotes?page=1&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;quotes&quot;</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;quote&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]}</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;has_next&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">page</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;https://quotes.toscrape.com/api/quotes?page=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">page</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>This spider starts at the first page of the quotes-API. With each
response, we parse the <code class="docutils literal notranslate"><span class="pre">response.text</span></code> and assign it to <code class="docutils literal notranslate"><span class="pre">data</span></code>.
This lets us operate on the JSON-object like on a Python dictionary.
We iterate through the <code class="docutils literal notranslate"><span class="pre">quotes</span></code> and print out the <code class="docutils literal notranslate"><span class="pre">quote[&quot;text&quot;]</span></code>.
If the handy <code class="docutils literal notranslate"><span class="pre">has_next</span></code> element is <code class="docutils literal notranslate"><span class="pre">true</span></code> (try loading
<a class="reference external" href="https://quotes.toscrape.com/api/quotes?page=10">quotes.toscrape.com/api/quotes?page=10</a> in your browser or a
page-number greater than 10), we increment the <code class="docutils literal notranslate"><span class="pre">page</span></code> attribute
and <code class="docutils literal notranslate"><span class="pre">yield</span></code> a new request, inserting the incremented page-number
into our <code class="docutils literal notranslate"><span class="pre">url</span></code>.</p>
<p id="requests-from-curl">In more complex websites, it could be difficult to easily reproduce the
requests, as we could need to add <code class="docutils literal notranslate"><span class="pre">headers</span></code> or <code class="docutils literal notranslate"><span class="pre">cookies</span></code> to make it work.
In those cases you can export the requests in <a class="reference external" href="https://curl.se/">cURL</a>
format, by right-clicking on each of them in the network tool and using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">from_curl()</span></code> method to generate an equivalent
request:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Request</span>

<span class="n">request</span> <span class="o">=</span> <span class="n">Request</span><span class="o">.</span><span class="n">from_curl</span><span class="p">(</span>
    <span class="s2">&quot;curl &#39;https://quotes.toscrape.com/api/quotes?page=1&#39; -H &#39;User-Agent: Mozil&quot;</span>
    <span class="s2">&quot;la/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0&#39; -H &#39;Acce&quot;</span>
    <span class="s2">&quot;pt: */*&#39; -H &#39;Accept-Language: ca,en-US;q=0.7,en;q=0.3&#39; --compressed -H &#39;X&quot;</span>
    <span class="s2">&quot;-Requested-With: XMLHttpRequest&#39; -H &#39;Proxy-Authorization: Basic QFRLLTAzM&quot;</span>
    <span class="s2">&quot;zEwZTAxLTk5MWUtNDFiNC1iZWRmLTJjNGI4M2ZiNDBmNDpAVEstMDMzMTBlMDEtOTkxZS00MW&quot;</span>
    <span class="s2">&quot;I0LWJlZGYtMmM0YjgzZmI0MGY0&#39; -H &#39;Connection: keep-alive&#39; -H &#39;Referer: http&quot;</span>
    <span class="s2">&quot;://quotes.toscrape.com/scroll&#39; -H &#39;Cache-Control: max-age=0&#39;&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, if you want to know the arguments needed to recreate that
request you can use the <a class="reference internal" href="#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs"><code class="xref py py-func docutils literal notranslate"><span class="pre">curl_to_request_kwargs()</span></code></a>
function to get a dictionary with the equivalent arguments:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.curl.curl_to_request_kwargs">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.curl.</span></span><span class="sig-name descname"><span class="pre">curl_to_request_kwargs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">curl_command</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_unknown_options</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.utils.curl.curl_to_request_kwargs" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a cURL command syntax to Request kwargs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>curl_command</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – string containing the curl command</p></li>
<li><p><strong>ignore_unknown_options</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If true, only a warning is emitted when
cURL options are unknown. Otherwise
raises an error. (default: True)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>dictionary of Request kwargs</p>
</dd>
</dl>
</dd></dl>

<p>Note that to translate a cURL command into a Scrapy request,
you may use <a class="reference external" href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>.</p>
<p>As you can see, with a few inspections in the <cite>Network</cite>-tool we
were able to easily replicate the dynamic requests of the scrolling
functionality of the page. Crawling dynamic pages can be quite
daunting and pages can be very complex, but it (mostly) boils down
to identifying the correct request and replicating it in your spider.</p>
</section>
</section>
<span id="document-topics/dynamic-content"></span><section id="selecting-dynamically-loaded-content">
<span id="topics-dynamic-content"></span><h3>Selecting dynamically-loaded content<a class="headerlink" href="#selecting-dynamically-loaded-content" title="Permalink to this heading">¶</a></h3>
<p>Some webpages show the desired data when you load them in a web browser.
However, when you download them using Scrapy, you cannot reach the desired data
using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<p>When this happens, the recommended approach is to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-finding-data-source"><span class="std std-ref">find the data source</span></a> and extract the data
from it.</p>
<p>If you fail to do that, and you can nonetheless access the desired data through
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a> from your web browser, see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-javascript-rendering"><span class="std std-ref">Pre-rendering JavaScript</span></a>.</p>
<section id="finding-the-data-source">
<span id="topics-finding-data-source"></span><h4>Finding the data source<a class="headerlink" href="#finding-the-data-source" title="Permalink to this heading">¶</a></h4>
<p>To extract the desired data, you must first find its source location.</p>
<p>If the data is in a non-text-based format, such as an image or a PDF document,
use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-network-tool"><span class="std std-ref">network tool</span></a> of your web browser to find
the corresponding request, and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-reproducing-requests"><span class="std std-ref">reproduce it</span></a>.</p>
<p>If your web browser lets you select the desired data as text, the data may be
defined in embedded JavaScript code, or loaded from an external resource in a
text-based format.</p>
<p>In that case, you can use a tool like <a class="reference external" href="https://github.com/stav/wgrep">wgrep</a> to find the URL of that resource.</p>
<p>If the data turns out to come from the original URL itself, you must
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-inspecting-source"><span class="std std-ref">inspect the source code of the webpage</span></a> to
determine where the data is located.</p>
<p>If the data comes from a different URL, you will need to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-reproducing-requests"><span class="std std-ref">reproduce the
corresponding request</span></a>.</p>
</section>
<section id="inspecting-the-source-code-of-a-webpage">
<span id="topics-inspecting-source"></span><h4>Inspecting the source code of a webpage<a class="headerlink" href="#inspecting-the-source-code-of-a-webpage" title="Permalink to this heading">¶</a></h4>
<p>Sometimes you need to inspect the source code of a webpage (not the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a>) to determine where some desired data is located.</p>
<p>Use Scrapy’s <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a> command to download the webpage contents as seen
by Scrapy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">fetch</span> <span class="o">--</span><span class="n">nolog</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span> <span class="o">&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
<p>If the desired data is in embedded JavaScript code within a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code>
element, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-parsing-javascript"><span class="std std-ref">Parsing JavaScript code</span></a>.</p>
<p>If you cannot find the desired data, first make sure it’s not just Scrapy:
download the webpage with an HTTP client like <a class="reference external" href="https://curl.se/">curl</a> or <a class="reference external" href="https://www.gnu.org/software/wget/">wget</a> and see if the
information can be found in the response they get.</p>
<p>If they get a response with the desired data, modify your Scrapy
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> to match that of the other HTTP client. For
example, try using the same user-agent string (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a>) or the
same <code class="xref py py-attr docutils literal notranslate"><span class="pre">headers</span></code>.</p>
<p>If they also get a response without the desired data, you’ll need to take
steps to make your request more similar to that of the web browser. See
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-reproducing-requests"><span class="std std-ref">Reproducing requests</span></a>.</p>
</section>
<section id="reproducing-requests">
<span id="topics-reproducing-requests"></span><h4>Reproducing requests<a class="headerlink" href="#reproducing-requests" title="Permalink to this heading">¶</a></h4>
<p>Sometimes we need to reproduce a request the way our web browser performs it.</p>
<p>Use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-network-tool"><span class="std std-ref">network tool</span></a> of your web browser to see
how your web browser performs the desired request, and try to reproduce that
request with Scrapy.</p>
<p>It might be enough to yield a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> with the same HTTP
method and URL. However, you may also need to reproduce the body, headers and
form parameters (see <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code>) of that request.</p>
<p>As all major browsers allow to export the requests in <a class="reference external" href="https://curl.se/">curl</a> format, Scrapy
incorporates the method <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_curl()</span></code> to generate an equivalent
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> from a cURL command. To get more information
visit <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#requests-from-curl"><span class="std std-ref">request from curl</span></a> inside the network
tool section.</p>
<p>Once you get the expected response, you can <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-handling-response-formats"><span class="std std-ref">extract the desired data from
it</span></a>.</p>
<p>You can reproduce any request with Scrapy. However, some times reproducing all
necessary requests may not seem efficient in developer time. If that is your
case, and crawling speed is not a major concern for you, you can alternatively
consider <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-javascript-rendering"><span class="std std-ref">JavaScript pre-rendering</span></a>.</p>
<p>If you get the expected response <cite>sometimes</cite>, but not always, the issue is
probably not your request, but the target server. The target server might be
buggy, overloaded, or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#bans"><span class="std std-ref">banning</span></a> some of your requests.</p>
<p>Note that to translate a cURL command into a Scrapy request,
you may use <a class="reference external" href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>.</p>
</section>
<section id="handling-different-response-formats">
<span id="topics-handling-response-formats"></span><h4>Handling different response formats<a class="headerlink" href="#handling-different-response-formats" title="Permalink to this heading">¶</a></h4>
<p>Once you have a response with the desired data, how you extract the desired
data from it depends on the type of response:</p>
<ul>
<li><p>If the response is HTML, XML or JSON, use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a> as usual.</p></li>
<li><p>If the response is JSON, use <code class="xref py py-func docutils literal notranslate"><span class="pre">response.json()</span></code> to load the desired data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</pre></div>
</div>
<p>If the desired data is inside HTML or XML code embedded within JSON data,
you can load that HTML or XML code into a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code> and then
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">use it</span></a> as usual:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;html&quot;</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p>If the response is JavaScript, or HTML with a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element
containing the desired data, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-parsing-javascript"><span class="std std-ref">Parsing JavaScript code</span></a>.</p></li>
<li><p>If the response is CSS, use a <a class="reference external" href="https://docs.python.org/3/library/re.html" title="(in Python v3.13)"><span class="xref std std-doc">regular expression</span></a> to
extract the desired data from
<a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">response.text</span></code></a>.</p></li>
</ul>
<ul id="topics-parsing-images">
<li><p>If the response is an image or another format based on images (e.g. PDF),
read the response as bytes from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">response.body</span></code> and use an OCR
solution to extract the desired data as text.</p>
<p>For example, you can use <a class="reference external" href="https://github.com/madmaze/pytesseract">pytesseract</a>. To read a table from a PDF,
<a class="reference external" href="https://github.com/chezou/tabula-py">tabula-py</a> may be a better choice.</p>
</li>
<li><p>If the response is SVG, or HTML with embedded SVG containing the desired
data, you may be able to extract the desired data using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>, since SVG is based on XML.</p>
<p>Otherwise, you might need to convert the SVG code into a raster image, and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-parsing-images"><span class="std std-ref">handle that raster image</span></a>.</p>
</li>
</ul>
</section>
<section id="parsing-javascript-code">
<span id="topics-parsing-javascript"></span><h4>Parsing JavaScript code<a class="headerlink" href="#parsing-javascript-code" title="Permalink to this heading">¶</a></h4>
<p>If the desired data is hardcoded in JavaScript, you first need to get the
JavaScript code:</p>
<ul class="simple">
<li><p>If the JavaScript code is in a JavaScript file, simply read
<a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">response.text</span></code></a>.</p></li>
<li><p>If the JavaScript code is within a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element of an HTML page,
use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a> to extract the text within that
<code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element.</p></li>
</ul>
<p>Once you have a string with the JavaScript code, you can extract the desired
data from it:</p>
<ul>
<li><p>You might be able to use a <a class="reference external" href="https://docs.python.org/3/library/re.html" title="(in Python v3.13)"><span class="xref std std-doc">regular expression</span></a> to
extract the desired data in JSON format, which you can then parse with
<a class="reference external" href="https://docs.python.org/3/library/json.html#json.loads" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">json.loads()</span></code></a>.</p>
<p>For example, if the JavaScript code contains a separate line like
<code class="docutils literal notranslate"><span class="pre">var</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">{&quot;field&quot;:</span> <span class="pre">&quot;value&quot;};</span></code> you can extract that data as follows:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;\bvar\s+data\s*=\s*(\{.*?\})\s*;\s*\n&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;script::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>
<span class="go">{&#39;field&#39;: &#39;value&#39;}</span>
</pre></div>
</div>
</li>
<li><p><a class="reference external" href="https://github.com/Nykakin/chompjs">chompjs</a> provides an API to parse JavaScript objects into a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
<p>For example, if the JavaScript code contains
<code class="docutils literal notranslate"><span class="pre">var</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">{field:</span> <span class="pre">&quot;value&quot;,</span> <span class="pre">secondField:</span> <span class="pre">&quot;second</span> <span class="pre">value&quot;};</span></code>
you can extract that data as follows:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">chompjs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">javascript</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;script::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">chompjs</span><span class="o">.</span><span class="n">parse_js_object</span><span class="p">(</span><span class="n">javascript</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span>
<span class="go">{&#39;field&#39;: &#39;value&#39;, &#39;secondField&#39;: &#39;second value&#39;}</span>
</pre></div>
</div>
</li>
<li><p>Otherwise, use <a class="reference external" href="https://github.com/scrapinghub/js2xml">js2xml</a> to convert the JavaScript code into an XML document
that you can parse using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<p>For example, if the JavaScript code contains
<code class="docutils literal notranslate"><span class="pre">var</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">{field:</span> <span class="pre">&quot;value&quot;};</span></code> you can extract that data as follows:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">js2xml</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">lxml.etree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">parsel</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">javascript</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;script::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xml</span> <span class="o">=</span> <span class="n">lxml</span><span class="o">.</span><span class="n">etree</span><span class="o">.</span><span class="n">tostring</span><span class="p">(</span><span class="n">js2xml</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">javascript</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;unicode&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">xml</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;var[name=&quot;data&quot;]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;&lt;var name=&quot;data&quot;&gt;&lt;object&gt;&lt;property name=&quot;field&quot;&gt;&lt;string&gt;value&lt;/string&gt;&lt;/property&gt;&lt;/object&gt;&lt;/var&gt;&#39;</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="pre-rendering-javascript">
<span id="topics-javascript-rendering"></span><h4>Pre-rendering JavaScript<a class="headerlink" href="#pre-rendering-javascript" title="Permalink to this heading">¶</a></h4>
<p>On webpages that fetch data from additional requests, reproducing those
requests that contain the desired data is the preferred approach. The effort is
often worth the result: structured, complete data with minimum parsing time and
network transfer.</p>
<p>However, sometimes it can be really hard to reproduce certain requests. Or you
may need something that no request can give you, such as a screenshot of a
webpage as seen in a web browser.</p>
<p>In these cases use the <a class="reference external" href="https://github.com/scrapinghub/splash">Splash</a> JavaScript-rendering service, along with
<a class="reference external" href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a> for seamless integration.</p>
<p>Splash returns as HTML the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a> of a webpage, so that
you can parse it with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>. It provides great
flexibility through <a class="reference external" href="https://splash.readthedocs.io/en/stable/api.html">configuration</a> or <a class="reference external" href="https://splash.readthedocs.io/en/stable/scripting-tutorial.html">scripting</a>.</p>
<p>If you need something beyond what Splash offers, such as interacting with the
DOM on-the-fly from Python code instead of using a previously-written script,
or handling multiple web browser windows, you might need to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-headless-browsing"><span class="std std-ref">use a headless browser</span></a> instead.</p>
</section>
<section id="using-a-headless-browser">
<span id="topics-headless-browsing"></span><h4>Using a headless browser<a class="headerlink" href="#using-a-headless-browser" title="Permalink to this heading">¶</a></h4>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Headless_browser">headless browser</a> is a special web browser that provides an API for
automation. By installing the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#install-asyncio"><span class="std std-ref">asyncio reactor</span></a>,
it is possible to integrate <code class="docutils literal notranslate"><span class="pre">asyncio</span></code>-based libraries which handle headless browsers.</p>
<p>One such library is <a class="reference external" href="https://github.com/microsoft/playwright-python">playwright-python</a> (an official Python port of <a class="reference external" href="https://github.com/microsoft/playwright">playwright</a>).
The following is a simple snippet to illustrate its usage within a Scrapy spider:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">playwright.async_api</span> <span class="kn">import</span> <span class="n">async_playwright</span>


<span class="k">class</span> <span class="nc">PlaywrightSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;playwright&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;data:,&quot;</span><span class="p">]</span>  <span class="c1"># avoid using the default Scrapy downloader</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">async_playwright</span><span class="p">()</span> <span class="k">as</span> <span class="n">pw</span><span class="p">:</span>
            <span class="n">browser</span> <span class="o">=</span> <span class="k">await</span> <span class="n">pw</span><span class="o">.</span><span class="n">chromium</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
            <span class="n">page</span> <span class="o">=</span> <span class="k">await</span> <span class="n">browser</span><span class="o">.</span><span class="n">new_page</span><span class="p">()</span>
            <span class="k">await</span> <span class="n">page</span><span class="o">.</span><span class="n">goto</span><span class="p">(</span><span class="s2">&quot;https://example.org&quot;</span><span class="p">)</span>
            <span class="n">title</span> <span class="o">=</span> <span class="k">await</span> <span class="n">page</span><span class="o">.</span><span class="n">title</span><span class="p">()</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">title</span><span class="p">}</span>
</pre></div>
</div>
<p>However, using <a class="reference external" href="https://github.com/microsoft/playwright-python">playwright-python</a> directly as in the above example
circumvents most of the Scrapy components (middlewares, dupefilter, etc).
We recommend using <a class="reference external" href="https://github.com/scrapy-plugins/scrapy-playwright">scrapy-playwright</a> for a better integration.</p>
</section>
</section>
<span id="document-topics/leaks"></span><section id="debugging-memory-leaks">
<span id="topics-leaks"></span><h3>Debugging memory leaks<a class="headerlink" href="#debugging-memory-leaks" title="Permalink to this heading">¶</a></h3>
<p>In Scrapy, objects such as requests, responses and items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it’s time to process
it. For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-architecture"><span class="std std-ref">Architecture overview</span></a>.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a “memory leak”.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-leaks-trackrefs"><span class="std std-ref">trackref</span></a>,
and you can also use a third-party library called <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-leaks-muppy"><span class="std std-ref">muppy</span></a> for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
<section id="common-causes-of-memory-leaks">
<h4>Common causes of memory leaks<a class="headerlink" href="#common-causes-of-memory-leaks" title="Permalink to this heading">¶</a></h4>
<p>It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code> or <code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code>
attributes or the request callback function) and that effectively bounds the
lifetime of those referenced objects to the lifetime of the Request. This is,
by far, the most common cause of memory leaks in Scrapy projects, and a quite
difficult one to debug for newcomers.</p>
<p>In big projects, the spiders are typically written by different people and some
of those spiders could be “leaking” and thus affecting the rest of the other
(well-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.</p>
<p>The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, allocating resources on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>
but not releasing them on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> may cause problems if
you’re running <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-multiple-spiders"><span class="std std-ref">multiple spiders per process</span></a>.</p>
<section id="too-many-requests">
<h5>Too Many Requests?<a class="headerlink" href="#too-many-requests" title="Permalink to this heading">¶</a></h5>
<p>By default Scrapy keeps the request queue in memory; it includes
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects and all objects
referenced in Request attributes (e.g. in <code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code>).
While not necessarily a leak, this can take a lot of memory. Enabling
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a> could help keeping memory usage
in control.</p>
</section>
</section>
<section id="debugging-memory-leaks-with-trackref">
<span id="topics-leaks-trackrefs"></span><h4>Debugging memory leaks with <code class="docutils literal notranslate"><span class="pre">trackref</span></code><a class="headerlink" href="#debugging-memory-leaks-with-trackref" title="Permalink to this heading">¶</a></h4>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code> is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Request,
Response, Item, Spider and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the <code class="docutils literal notranslate"><span class="pre">prefs()</span></code> function which is an
alias to the <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">telnet</span> <span class="n">localhost</span> <span class="mi">6023</span>

<span class="o">..</span> <span class="n">code</span><span class="o">-</span><span class="n">block</span><span class="p">::</span> <span class="n">pycon</span>

    <span class="o">&gt;&gt;&gt;</span> <span class="n">prefs</span><span class="p">()</span>
    <span class="n">Live</span> <span class="n">References</span>

    <span class="n">ExampleSpider</span>                       <span class="mi">1</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">ago</span>
    <span class="n">HtmlResponse</span>                       <span class="mi">10</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">1</span><span class="n">s</span> <span class="n">ago</span>
    <span class="n">Selector</span>                            <span class="mi">2</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">0</span><span class="n">s</span> <span class="n">ago</span>
    <span class="n">FormRequest</span>                       <span class="mi">878</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">7</span><span class="n">s</span> <span class="n">ago</span>
</pre></div>
</div>
<p>As you can see, that report also shows the “age” of the oldest object in each
class. If you’re running multiple spiders per process chances are you can
figure out which spider is leaking by looking at the oldest request or response.
You can get the oldest object of each class using the
<a class="reference internal" href="#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_oldest()</span></code></a> function (from the telnet console).</p>
<section id="which-objects-are-tracked">
<h5>Which objects are tracked?<a class="headerlink" href="#which-objects-are-tracked" title="Permalink to this heading">¶</a></h5>
<p>The objects tracked by <code class="docutils literal notranslate"><span class="pre">trackrefs</span></code> are all from these classes (and all its
subclasses):</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Request</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.Response</span></code></a></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Item</span></code></a></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Selector</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Spider</span></code></a></p></li>
</ul>
</section>
<section id="a-real-example">
<h5>A real example<a class="headerlink" href="#a-real-example" title="Permalink to this heading">¶</a></h5>
<p>Let’s see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">Request</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;http://www.somenastyspider.com/product.php?pid=</span><span class="si">{</span><span class="n">product_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
               <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;referer&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>
</pre></div>
</div>
<p>That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests’ one, and that would definitely
cause memory leaks.</p>
<p>Let’s see how we can discover the cause (without knowing it
a priori, of course) by using the <code class="docutils literal notranslate"><span class="pre">trackref</span></code> tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prefs</span><span class="p">()</span>
<span class="go">Live References</span>

<span class="go">SomenastySpider                     1   oldest: 15s ago</span>
<span class="go">HtmlResponse                     3890   oldest: 265s ago</span>
<span class="go">Selector                            2   oldest: 0s ago</span>
<span class="go">Request                          3878   oldest: 250s ago</span>
</pre></div>
</div>
<p>The fact that there are so many live responses (and that they’re so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. The number of responses is similar to the number
of requests, so it looks like they are tied in a some way. We can now go
and check the code of the spider to discover the nasty line that is
generating the leaks (passing response references inside requests).</p>
<p>Sometimes extra information about live objects can be helpful.
Let’s check the oldest response:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.utils.trackref</span> <span class="kn">import</span> <span class="n">get_oldest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">get_oldest</span><span class="p">(</span><span class="s2">&quot;HtmlResponse&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">url</span>
<span class="go">&#39;http://www.somenastyspider.com/product.php?pid=123&#39;</span>
</pre></div>
</div>
<p>If you want to iterate over all objects, instead of getting the oldest one, you
can use the <a class="reference internal" href="#scrapy.utils.trackref.iter_all" title="scrapy.utils.trackref.iter_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.trackref.iter_all()</span></code></a> function:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.utils.trackref</span> <span class="kn">import</span> <span class="n">iter_all</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">url</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">iter_all</span><span class="p">(</span><span class="s2">&quot;HtmlResponse&quot;</span><span class="p">)]</span>
<span class="go">[&#39;http://www.somenastyspider.com/product.php?pid=123&#39;,</span>
<span class="go">&#39;http://www.somenastyspider.com/product.php?pid=584&#39;,</span>
<span class="go">...]</span>
</pre></div>
</div>
</section>
<section id="too-many-spiders">
<h5>Too many spiders?<a class="headerlink" href="#too-many-spiders" title="Permalink to this heading">¶</a></h5>
<p>If your project has too many spiders executed in parallel,
the output of <code class="xref py py-func docutils literal notranslate"><span class="pre">prefs()</span></code> can be difficult to read.
For this reason, that function has a <code class="docutils literal notranslate"><span class="pre">ignore</span></code> argument which can be used to
ignore a particular class (and all its subclasses). For
example, this won’t show any live references to spiders:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">Spider</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prefs</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="n">Spider</span><span class="p">)</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.utils.trackref"></span></section>
<section id="scrapy-utils-trackref-module">
<h5>scrapy.utils.trackref module<a class="headerlink" href="#scrapy-utils-trackref-module" title="Permalink to this heading">¶</a></h5>
<p>Here are the functions available in the <a class="reference internal" href="#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code></a> module.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.utils.trackref.object_ref">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.utils.trackref.</span></span><span class="sig-name descname"><span class="pre">object_ref</span></span><a class="headerlink" href="#scrapy.utils.trackref.object_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Inherit from this class if you want to track live
instances with the <code class="docutils literal notranslate"><span class="pre">trackref</span></code> module.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.trackref.print_live_refs">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.trackref.</span></span><span class="sig-name descname"><span class="pre">print_live_refs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">class_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">NoneType</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.print_live_refs" title="Permalink to this definition">¶</a></dt>
<dd><p>Print a report of live references, grouped by class name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ignore</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a>) – if given, all objects from the specified class (or tuple of
classes) will be ignored.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.trackref.get_oldest">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.trackref.</span></span><span class="sig-name descname"><span class="pre">get_oldest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">class_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.get_oldest" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the oldest object alive with the given class name, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> first to get a list of all
tracked live objects per class name.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.trackref.iter_all">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.trackref.</span></span><span class="sig-name descname"><span class="pre">iter_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">class_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.iter_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterator over all objects alive with the given class name, or
<code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> first to get a list
of all tracked live objects per class name.</p>
</dd></dl>

</section>
</section>
<section id="debugging-memory-leaks-with-muppy">
<span id="topics-leaks-muppy"></span><h4>Debugging memory leaks with muppy<a class="headerlink" href="#debugging-memory-leaks-with-muppy" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">trackref</span></code> provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks. However, there are other cases where the memory leaks could come
from other (more or less obscure) objects. If this is your case, and you can’t
find your leaks using <code class="docutils literal notranslate"><span class="pre">trackref</span></code>, you still have another resource: the muppy
library.</p>
<p>You can use muppy from <a class="reference external" href="https://pypi.org/project/Pympler/">Pympler</a>.</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">pip</span></code>, you can install muppy with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Pympler</span>
</pre></div>
</div>
<p>Here’s an example to view all Python objects available in
the heap using muppy:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pympler</span> <span class="kn">import</span> <span class="n">muppy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">all_objects</span> <span class="o">=</span> <span class="n">muppy</span><span class="o">.</span><span class="n">get_objects</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">all_objects</span><span class="p">)</span>
<span class="go">28667</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pympler</span> <span class="kn">import</span> <span class="n">summary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">suml</span> <span class="o">=</span> <span class="n">summary</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">all_objects</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span><span class="o">.</span><span class="n">print_</span><span class="p">(</span><span class="n">suml</span><span class="p">)</span>
<span class="go">                               types |   # objects |   total size</span>
<span class="go">==================================== | =========== | ============</span>
<span class="go">                         &lt;class &#39;str |        9822 |      1.10 MB</span>
<span class="go">                        &lt;class &#39;dict |        1658 |    856.62 KB</span>
<span class="go">                        &lt;class &#39;type |         436 |    443.60 KB</span>
<span class="go">                        &lt;class &#39;code |        2974 |    419.56 KB</span>
<span class="go">          &lt;class &#39;_io.BufferedWriter |           2 |    256.34 KB</span>
<span class="go">                         &lt;class &#39;set |         420 |    159.88 KB</span>
<span class="go">          &lt;class &#39;_io.BufferedReader |           1 |    128.17 KB</span>
<span class="go">          &lt;class &#39;wrapper_descriptor |        1130 |     88.28 KB</span>
<span class="go">                       &lt;class &#39;tuple |        1304 |     86.57 KB</span>
<span class="go">                     &lt;class &#39;weakref |        1013 |     79.14 KB</span>
<span class="go">  &lt;class &#39;builtin_function_or_method |         958 |     67.36 KB</span>
<span class="go">           &lt;class &#39;method_descriptor |         865 |     60.82 KB</span>
<span class="go">                 &lt;class &#39;abc.ABCMeta |          62 |     59.96 KB</span>
<span class="go">                        &lt;class &#39;list |         446 |     58.52 KB</span>
<span class="go">                         &lt;class &#39;int |        1425 |     43.20 KB</span>
</pre></div>
</div>
<p>For more info about muppy, refer to the <a class="reference external" href="https://pythonhosted.org/Pympler/muppy.html">muppy documentation</a>.</p>
</section>
<section id="leaks-without-leaks">
<span id="topics-leaks-without-leaks"></span><h4>Leaks without leaks<a class="headerlink" href="#leaks-without-leaks" title="Permalink to this heading">¶</a></h4>
<p>Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.evanjones.ca/python-memory.html">Python Memory Management</a></p></li>
<li><p><a class="reference external" href="https://www.evanjones.ca/python-memory-part2.html">Python Memory Management Part 2</a></p></li>
<li><p><a class="reference external" href="https://www.evanjones.ca/python-memory-part3.html">Python Memory Management Part 3</a></p></li>
</ul>
<p>The improvements proposed by Evan Jones, which are detailed in <a class="reference external" href="https://www.evanjones.ca/memoryallocator/">this paper</a>,
got merged in Python 2.5, but this only reduces the problem, it doesn’t fix it
completely. To quote the paper:</p>
<blockquote>
<div><p><em>Unfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a large
issue. An application could have many megabytes of free memory, scattered
throughout all the arenas, but it will be unable to free any of it. This is
a problem experienced by all memory allocators. The only way to solve it is
to move to a compacting garbage collector, which is able to move objects in
memory. This would require significant changes to the Python interpreter.</em></p>
</div></blockquote>
<p>To keep memory consumption reasonable you can split the job into several
smaller jobs or enable <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a>
and stop/start spider from time to time.</p>
</section>
</section>
<span id="document-topics/media-pipeline"></span><section id="downloading-and-processing-files-and-images">
<span id="topics-media-pipeline"></span><h3>Downloading and processing files and images<a class="headerlink" href="#downloading-and-processing-files-and-images" title="Permalink to this heading">¶</a></h3>
<p>Scrapy provides reusable <a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">item pipelines</span></a> for
downloading files attached to a particular item (for example, when you scrape
products and also want to download their images locally). These pipelines share
a bit of functionality and structure (we refer to them as media pipelines), but
typically you’ll either use the Files Pipeline or the Images Pipeline.</p>
<p>Both pipelines implement these features:</p>
<ul class="simple">
<li><p>Avoid re-downloading media that was downloaded recently</p></li>
<li><p>Specifying where to store the media (filesystem directory, FTP server, Amazon S3 bucket,
Google Cloud Storage bucket)</p></li>
</ul>
<p>The Images Pipeline has a few extra functions for processing images:</p>
<ul class="simple">
<li><p>Convert all downloaded images to a common format (JPG) and mode (RGB)</p></li>
<li><p>Thumbnail generation</p></li>
<li><p>Check images width/height to make sure they meet a minimum constraint</p></li>
</ul>
<p>The pipelines also keep an internal queue of those media URLs which are currently
being scheduled for download, and connect those responses that arrive containing
the same media to that queue. This avoids downloading the same media more than
once when it’s shared by several items.</p>
<section id="using-the-files-pipeline">
<h4>Using the Files Pipeline<a class="headerlink" href="#using-the-files-pipeline" title="Permalink to this heading">¶</a></h4>
<p>The typical workflow, when using the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code> goes like
this:</p>
<ol class="arabic simple">
<li><p>In a Spider, you scrape an item and put the URLs of the desired into a
<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field.</p></li>
<li><p>The item is returned from the spider and goes to the item pipeline.</p></li>
<li><p>When the item reaches the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>, the URLs in the
<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field are scheduled for download using the standard
Scrapy scheduler and downloader (which means the scheduler and downloader
middlewares are reused), but with a higher priority, processing them before other
pages are scraped. The item remains “locked” at that particular pipeline stage
until the files have finish downloading (or fail for some reason).</p></li>
<li><p>When the files are downloaded, another field (<code class="docutils literal notranslate"><span class="pre">files</span></code>) will be populated
with the results. This field will contain a list of dicts with information
about the downloaded files, such as the downloaded path, the original
scraped url (taken from the <code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field), the file checksum and the file status.
The files in the list of the <code class="docutils literal notranslate"><span class="pre">files</span></code> field will retain the same order of
the original <code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field. If some file failed downloading, an
error will be logged and the file won’t be present in the <code class="docutils literal notranslate"><span class="pre">files</span></code> field.</p></li>
</ol>
</section>
<section id="using-the-images-pipeline">
<span id="images-pipeline"></span><h4>Using the Images Pipeline<a class="headerlink" href="#using-the-images-pipeline" title="Permalink to this heading">¶</a></h4>
<p>Using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> is a lot like using the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>,
except the default field names used are different: you use <code class="docutils literal notranslate"><span class="pre">image_urls</span></code> for
the image URLs of an item and it will populate an <code class="docutils literal notranslate"><span class="pre">images</span></code> field for the information
about the downloaded images.</p>
<p>The advantage of using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> for image files is that you
can configure some extra functions like generating thumbnails and filtering
the images based on their size.</p>
<p>The Images Pipeline requires <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> 7.1.0 or greater. It is used for
thumbnailing and normalizing images to JPEG/RGB format.</p>
</section>
<section id="enabling-your-media-pipeline">
<span id="topics-media-pipeline-enabling"></span><h4>Enabling your Media Pipeline<a class="headerlink" href="#enabling-your-media-pipeline" title="Permalink to this heading">¶</a></h4>
<span class="target" id="std-setting-IMAGES_STORE"></span><p id="std-setting-FILES_STORE">To enable your media pipeline you must first add it to your project
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For Images Pipeline, use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scrapy.pipelines.images.ImagesPipeline&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<p>For Files Pipeline, use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scrapy.pipelines.files.FilesPipeline&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also use both the Files and Images Pipeline at the same time.</p>
</div>
<p>Then, configure the target storage setting to a valid value that will be used
for storing the downloaded images. Otherwise the pipeline will remain disabled,
even if you include it in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For the Files Pipeline, set the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> setting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FILES_STORE</span> <span class="o">=</span> <span class="s2">&quot;/path/to/valid/dir&quot;</span>
</pre></div>
</div>
<p>For the Images Pipeline, set the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> setting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s2">&quot;/path/to/valid/dir&quot;</span>
</pre></div>
</div>
</section>
<section id="file-naming">
<span id="topics-file-naming"></span><h4>File Naming<a class="headerlink" href="#file-naming" title="Permalink to this heading">¶</a></h4>
<section id="default-file-naming">
<h5>Default File Naming<a class="headerlink" href="#default-file-naming" title="Permalink to this heading">¶</a></h5>
<p>By default, files are stored using an <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA-1 hash</a> of their URLs for the file names.</p>
<p>For example, the following image URL:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">image</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>Whose <code class="docutils literal notranslate"><span class="pre">SHA-1</span> <span class="pre">hash</span></code> is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span><span class="n">afec3b4765f8f0a07b78f98c07b83f013567a0a</span>
</pre></div>
</div>
<p>Will be downloaded and stored using your chosen <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-supported-storage"><span class="std std-ref">storage method</span></a> and the following file name:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span><span class="n">afec3b4765f8f0a07b78f98c07b83f013567a0a</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
</section>
<section id="custom-file-naming">
<h5>Custom File Naming<a class="headerlink" href="#custom-file-naming" title="Permalink to this heading">¶</a></h5>
<p>You may wish to use a different calculated file name for saved files.
For example, classifying an image by including meta in the file name.</p>
<p>Customize file names by overriding the <code class="docutils literal notranslate"><span class="pre">file_path</span></code> method of your
media pipeline.</p>
<p>For example, an image pipeline with image URL:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">product</span><span class="o">/</span><span class="n">images</span><span class="o">/</span><span class="n">large</span><span class="o">/</span><span class="n">front</span><span class="o">/</span><span class="mi">0000000004166</span>
</pre></div>
</div>
<p>Can be processed into a file name with a condensed hash and the perspective
<code class="docutils literal notranslate"><span class="pre">front</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">00</span><span class="n">b08510e4_front</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>By overriding <code class="docutils literal notranslate"><span class="pre">file_path</span></code> like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hashlib</span>


<span class="k">def</span> <span class="nf">file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">image_url_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">shake_256</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">encode</span><span class="p">())</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">image_perspective</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">image_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">image_url_hash</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">image_perspective</span><span class="si">}</span><span class="s2">.jpg&quot;</span>

    <span class="k">return</span> <span class="n">image_filename</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If your custom file name scheme relies on meta data that can vary between
scrapes it may lead to unexpected re-downloading of existing media using
new file names.</p>
<p>For example, if your custom file name scheme uses a product title and the
site changes an item’s product title between scrapes, Scrapy will re-download
the same media using updated file names.</p>
</div>
<p>For more information about the <code class="docutils literal notranslate"><span class="pre">file_path</span></code> method, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-media-pipeline-override"><span class="std std-ref">Extending the Media Pipelines</span></a>.</p>
</section>
</section>
<section id="supported-storage">
<span id="topics-supported-storage"></span><h4>Supported Storage<a class="headerlink" href="#supported-storage" title="Permalink to this heading">¶</a></h4>
<section id="file-system-storage">
<h5>File system storage<a class="headerlink" href="#file-system-storage" title="Permalink to this heading">¶</a></h5>
<p>File system storage will save files to the following path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">full</span><span class="o">/&lt;</span><span class="n">FILE_NAME</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;IMAGES_STORE&gt;</span></code> is the directory defined in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> setting
for the Images Pipeline.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">full</span></code> is a sub-directory to separate full images from thumbnails (if
used). For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-images-thumbnails"><span class="std std-ref">Thumbnail generation for images</span></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;FILE_NAME&gt;</span></code> is the file name assigned to the file.  For more info see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-file-naming"><span class="std std-ref">File Naming</span></a>.</p></li>
</ul>
</section>
<section id="ftp-server-storage">
<span id="media-pipeline-ftp"></span><h5>FTP server storage<a class="headerlink" href="#ftp-server-storage" title="Permalink to this heading">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can point to an FTP server.
Scrapy will automatically upload the files to the server.</p>
<p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> should be written in one of the
following forms:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ftp</span><span class="p">:</span><span class="o">//</span><span class="n">username</span><span class="p">:</span><span class="n">password</span><span class="nd">@address</span><span class="p">:</span><span class="n">port</span><span class="o">/</span><span class="n">path</span>
<span class="n">ftp</span><span class="p">:</span><span class="o">//</span><span class="n">address</span><span class="p">:</span><span class="n">port</span><span class="o">/</span><span class="n">path</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">username</span></code> and <code class="docutils literal notranslate"><span class="pre">password</span></code> are not provided, they are taken from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_USER</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_PASSWORD</span></code></a> settings respectively.</p>
<p>FTP supports two different connection modes: active or passive. Scrapy uses
the passive connection mode by default. To use the active connection mode instead,
set the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</section>
<section id="amazon-s3-storage">
<span id="media-pipelines-s3"></span><h5>Amazon S3 storage<a class="headerlink" href="#amazon-s3-storage" title="Permalink to this heading">¶</a></h5>
<span class="target" id="std-setting-FILES_STORE_S3_ACL"></span><p id="std-setting-IMAGES_STORE_S3_ACL">If <a class="reference external" href="https://github.com/boto/botocore">botocore</a> &gt;= 1.4.87 is installed, <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can represent an Amazon S3 bucket. Scrapy will
automatically upload the files to the bucket.</p>
<p>For example, this is a valid <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> value:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s2">&quot;s3://bucket/images&quot;</span>
</pre></div>
</div>
<p>You can modify the Access Control List (ACL) policy used for the stored files,
which is defined by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_S3_ACL</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_S3_ACL</span></code></a> settings. By default, the ACL is set to
<code class="docutils literal notranslate"><span class="pre">private</span></code>. To make the files publicly available use the <code class="docutils literal notranslate"><span class="pre">public-read</span></code>
policy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE_S3_ACL</span> <span class="o">=</span> <span class="s2">&quot;public-read&quot;</span>
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl">canned ACLs</a> in the Amazon S3 Developer Guide.</p>
<p>You can also use other S3-like storages. Storages like self-hosted <a class="reference external" href="https://github.com/minio/minio">Minio</a> or
<a class="reference external" href="https://www.zenko.io/cloudserver/">Zenko CloudServer</a>. All you need to do is set endpoint option in you Scrapy
settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AWS_ENDPOINT_URL</span> <span class="o">=</span> <span class="s2">&quot;http://minio.example.com:9000&quot;</span>
</pre></div>
</div>
<p>For self-hosting you also might feel the need not to use SSL and not to verify SSL connection:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AWS_USE_SSL</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># or True (None by default)</span>
<span class="n">AWS_VERIFY</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># or True (None by default)</span>
</pre></div>
</div>
</section>
<section id="google-cloud-storage">
<span id="media-pipeline-gcs"></span><h5>Google Cloud Storage<a class="headerlink" href="#google-cloud-storage" title="Permalink to this heading">¶</a></h5>
<span class="target" id="std-setting-FILES_STORE_GCS_ACL"></span><p id="std-setting-IMAGES_STORE_GCS_ACL"><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can represent a Google Cloud Storage
bucket. Scrapy will automatically upload the files to the bucket. (requires <a class="reference external" href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a> )</p>
<p>For example, these are valid <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-GCS_PROJECT_ID"><code class="xref std std-setting docutils literal notranslate"><span class="pre">GCS_PROJECT_ID</span></code></a> settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s2">&quot;gs://bucket/images/&quot;</span>
<span class="n">GCS_PROJECT_ID</span> <span class="o">=</span> <span class="s2">&quot;project_id&quot;</span>
</pre></div>
</div>
<p>For information about authentication, see this <a class="reference external" href="https://cloud.google.com/docs/authentication">documentation</a>.</p>
<p>You can modify the Access Control List (ACL) policy used for the stored files,
which is defined by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_GCS_ACL</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_GCS_ACL</span></code></a> settings. By default, the ACL is set to
<code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string) which means that Cloud Storage applies the bucket’s default object ACL to the object.
To make the files publicly available use the <code class="docutils literal notranslate"><span class="pre">publicRead</span></code>
policy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE_GCS_ACL</span> <span class="o">=</span> <span class="s2">&quot;publicRead&quot;</span>
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://cloud.google.com/storage/docs/access-control/lists#predefined-acl">Predefined ACLs</a> in the Google Cloud Platform Developer Guide.</p>
</section>
</section>
<section id="usage-example">
<h4>Usage example<a class="headerlink" href="#usage-example" title="Permalink to this heading">¶</a></h4>
<span class="target" id="std-setting-FILES_URLS_FIELD"></span><span class="target" id="std-setting-FILES_RESULT_FIELD"></span><span class="target" id="std-setting-IMAGES_URLS_FIELD"></span><p id="std-setting-IMAGES_RESULT_FIELD">In order to use a media pipeline, first <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-media-pipeline-enabling"><span class="std std-ref">enable it</span></a>.</p>
<p>Then, if a spider returns an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item object</span></a> with the URLs
field (<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> or <code class="docutils literal notranslate"><span class="pre">image_urls</span></code>, for the Files or Images Pipeline
respectively), the pipeline will put the results under the respective field
(<code class="docutils literal notranslate"><span class="pre">files</span></code> or <code class="docutils literal notranslate"><span class="pre">images</span></code>).</p>
<p>When using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item types</span></a> for which fields are defined beforehand,
you must define both the URLs field and the results field. For example, when
using the images pipeline, items must define both the <code class="docutils literal notranslate"><span class="pre">image_urls</span></code> and the
<code class="docutils literal notranslate"><span class="pre">images</span></code> field. For instance, using the <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MyItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="c1"># ... other item fields ...</span>
    <span class="n">image_urls</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>If you want to use another field name for the URLs key or for the results key,
it is also possible to override it.</p>
<p>For the Files Pipeline, set <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_URLS_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_URLS_FIELD</span></code></a> and/or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_RESULT_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FILES_URLS_FIELD</span> <span class="o">=</span> <span class="s2">&quot;field_name_for_your_files_urls&quot;</span>
<span class="n">FILES_RESULT_FIELD</span> <span class="o">=</span> <span class="s2">&quot;field_name_for_your_processed_files&quot;</span>
</pre></div>
</div>
<p>For the Images Pipeline, set <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_URLS_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_URLS_FIELD</span></code></a> and/or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_RESULT_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_URLS_FIELD</span> <span class="o">=</span> <span class="s2">&quot;field_name_for_your_images_urls&quot;</span>
<span class="n">IMAGES_RESULT_FIELD</span> <span class="o">=</span> <span class="s2">&quot;field_name_for_your_processed_images&quot;</span>
</pre></div>
</div>
<p>If you need something more complex and want to override the custom pipeline
behaviour, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-media-pipeline-override"><span class="std std-ref">Extending the Media Pipelines</span></a>.</p>
<p>If you have multiple image pipelines inheriting from ImagePipeline and you want
to have different settings in different pipelines you can set setting keys
preceded with uppercase name of your pipeline class. E.g. if your pipeline is
called MyPipeline and you want to have custom IMAGES_URLS_FIELD you define
setting MYPIPELINE_IMAGES_URLS_FIELD and your custom settings will be used.</p>
</section>
<section id="additional-features">
<h4>Additional features<a class="headerlink" href="#additional-features" title="Permalink to this heading">¶</a></h4>
<section id="file-expiration">
<span id="id2"></span><h5>File expiration<a class="headerlink" href="#file-expiration" title="Permalink to this heading">¶</a></h5>
<span class="target" id="std-setting-IMAGES_EXPIRES"></span><p id="std-setting-FILES_EXPIRES">The Image Pipeline avoids downloading files that were downloaded recently. To
adjust this retention delay use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_EXPIRES</span></code></a> setting (or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_EXPIRES</span></code></a>, in case of Images Pipeline), which
specifies the delay in number of days:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 120 days of delay for files expiration</span>
<span class="n">FILES_EXPIRES</span> <span class="o">=</span> <span class="mi">120</span>

<span class="c1"># 30 days of delay for images expiration</span>
<span class="n">IMAGES_EXPIRES</span> <span class="o">=</span> <span class="mi">30</span>
</pre></div>
</div>
<p>The default value for both settings is 90 days.</p>
<p>If you have pipeline that subclasses FilesPipeline and you’d like to have
different setting for it you can set setting keys preceded by uppercase
class name. E.g. given pipeline class called MyPipeline you can set setting key:</p>
<blockquote>
<div><p>MYPIPELINE_FILES_EXPIRES = 180</p>
</div></blockquote>
<p>and pipeline class MyPipeline will have expiration time set to 180.</p>
<p>The last modified time from the file is used to determine the age of the file in days,
which is then compared to the set expiration time to determine if the file is expired.</p>
</section>
<section id="thumbnail-generation-for-images">
<span id="topics-images-thumbnails"></span><h5>Thumbnail generation for images<a class="headerlink" href="#thumbnail-generation-for-images" title="Permalink to this heading">¶</a></h5>
<p>The Images Pipeline can automatically create thumbnails of the downloaded
images.</p>
<p id="std-setting-IMAGES_THUMBS">In order to use this feature, you must set <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_THUMBS</span></code></a> to a dictionary
where the keys are the thumbnail names and the values are their dimensions.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_THUMBS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;small&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="s2">&quot;big&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">270</span><span class="p">,</span> <span class="mi">270</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/&lt;</span><span class="n">size_name</span><span class="o">&gt;/&lt;</span><span class="n">image_id</span><span class="o">&gt;.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;size_name&gt;</span></code> is the one specified in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_THUMBS</span></code></a>
dictionary keys (<code class="docutils literal notranslate"><span class="pre">small</span></code>, <code class="docutils literal notranslate"><span class="pre">big</span></code>, etc)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;image_id&gt;</span></code> is the <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA-1 hash</a> of the image url</p></li>
</ul>
<p>Example of image files stored using <code class="docutils literal notranslate"><span class="pre">small</span></code> and <code class="docutils literal notranslate"><span class="pre">big</span></code> thumbnail names:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">full</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
<span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/</span><span class="n">small</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
<span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/</span><span class="n">big</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>The first one is the full image, as downloaded from the site.</p>
</section>
<section id="filtering-out-small-images">
<h5>Filtering out small images<a class="headerlink" href="#filtering-out-small-images" title="Permalink to this heading">¶</a></h5>
<span class="target" id="std-setting-IMAGES_MIN_HEIGHT"></span><p id="std-setting-IMAGES_MIN_WIDTH">When using the Images Pipeline, you can drop images which are too small, by
specifying the minimum allowed size in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_MIN_HEIGHT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_MIN_HEIGHT</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_MIN_WIDTH"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_MIN_WIDTH</span></code></a> settings.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_MIN_HEIGHT</span> <span class="o">=</span> <span class="mi">110</span>
<span class="n">IMAGES_MIN_WIDTH</span> <span class="o">=</span> <span class="mi">110</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The size constraints don’t affect thumbnail generation at all.</p>
</div>
<p>It is possible to set just one size constraint or both. When setting both of
them, only images that satisfy both minimum sizes will be saved. For the
above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will
all be dropped because at least one dimension is shorter than the constraint.</p>
<p>By default, there are no size constraints, so all images are processed.</p>
</section>
<section id="allowing-redirections">
<h5>Allowing redirections<a class="headerlink" href="#allowing-redirections" title="Permalink to this heading">¶</a></h5>
<p id="std-setting-MEDIA_ALLOW_REDIRECTS">By default media pipelines ignore redirects, i.e. an HTTP redirection
to a media file URL request will mean the media download is considered failed.</p>
<p>To handle media redirections, set this setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MEDIA_ALLOW_REDIRECTS</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
</section>
<section id="module-scrapy.pipelines.files">
<span id="extending-the-media-pipelines"></span><span id="topics-media-pipeline-override"></span><h4>Extending the Media Pipelines<a class="headerlink" href="#module-scrapy.pipelines.files" title="Permalink to this heading">¶</a></h4>
<p>See here the methods that you can override in your custom Files Pipeline:</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.pipelines.files.FilesPipeline">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.pipelines.files.</span></span><span class="sig-name descname"><span class="pre">FilesPipeline</span></span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.files.FilesPipeline.file_path">
<span class="sig-name descname"><span class="pre">file_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.file_path" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called once per downloaded item. It returns the
download path of the file originating from the specified
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">response</span></code>, this method receives the original
<code class="xref py py-class docutils literal notranslate"><span class="pre">request</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">info</span></code> and
<a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">item</span></code></a></p>
<p>You can override this method to customize the download path of each file.</p>
<p>For example, if file URLs end like regular paths (e.g.
<code class="docutils literal notranslate"><span class="pre">https://example.com/a/b/c/foo.png</span></code>), you can use the following
approach to download all files into the <code class="docutils literal notranslate"><span class="pre">files</span></code> folder with their
original filenames (e.g. <code class="docutils literal notranslate"><span class="pre">files/foo.png</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">PurePosixPath</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.httpobj</span> <span class="kn">import</span> <span class="n">urlparse_cached</span>

<span class="kn">from</span> <span class="nn">scrapy.pipelines.files</span> <span class="kn">import</span> <span class="n">FilesPipeline</span>


<span class="k">class</span> <span class="nc">MyFilesPipeline</span><span class="p">(</span><span class="n">FilesPipeline</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;files/&quot;</span> <span class="o">+</span> <span class="n">PurePosixPath</span><span class="p">(</span><span class="n">urlparse_cached</span><span class="p">(</span><span class="n">request</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
</pre></div>
</div>
<p>Similarly, you can use the <code class="docutils literal notranslate"><span class="pre">item</span></code> to determine the file path based on some item
property.</p>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a> method returns
<code class="docutils literal notranslate"><span class="pre">full/&lt;request</span> <span class="pre">URL</span> <span class="pre">hash&gt;.&lt;extension&gt;</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4: </span>The <em>item</em> parameter.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.files.FilesPipeline.get_media_requests">
<span class="sig-name descname"><span class="pre">get_media_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you can override the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method and return a Request for each
file URL:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>


<span class="k">def</span> <span class="nf">get_media_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
    <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">file_url</span> <span class="ow">in</span> <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;file_urls&quot;</span><span class="p">]:</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">file_url</span><span class="p">)</span>
</pre></div>
</div>
<p>Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method, as a list of 2-element tuples.
Each tuple will contain <code class="docutils literal notranslate"><span class="pre">(success,</span> <span class="pre">file_info_or_error)</span></code> where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">success</span></code> is a boolean which is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the image was downloaded
successfully or <code class="docutils literal notranslate"><span class="pre">False</span></code> if it failed for some reason</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">file_info_or_error</span></code> is a dict containing the following keys (if
success is <code class="docutils literal notranslate"><span class="pre">True</span></code>) or a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Failure</span></code></a> if
there was a problem.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">url</span></code> - the url where the file was downloaded from. This is the url of
the request returned from the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a>
method.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">path</span></code> - the path (relative to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a>) where the file
was stored</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">checksum</span></code> - a <a class="reference external" href="https://en.wikipedia.org/wiki/MD5">MD5 hash</a> of the image contents</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">status</span></code> - the file status indication.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<p>It can be one of the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">downloaded</span></code> - file was downloaded.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uptodate</span></code> - file was not downloaded, as it was downloaded recently,
according to the file expiration policy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cached</span></code> - file was already scheduled for download, by another item
sharing the same file.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The list of tuples received by <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> is
guaranteed to retain the same order of the requests returned from the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method.</p>
<p>Here’s a typical value of the <code class="docutils literal notranslate"><span class="pre">results</span></code> argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">(</span>
        <span class="kc">True</span><span class="p">,</span>
        <span class="p">{</span>
            <span class="s2">&quot;checksum&quot;</span><span class="p">:</span> <span class="s2">&quot;2b00042f7481c7b056c4b410d28f33cf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="s2">&quot;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&quot;</span><span class="p">,</span>
            <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="s2">&quot;http://www.example.com/files/product1.pdf&quot;</span><span class="p">,</span>
            <span class="s2">&quot;status&quot;</span><span class="p">:</span> <span class="s2">&quot;downloaded&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">),</span>
    <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">Failure</span><span class="p">(</span><span class="o">...</span><span class="p">)),</span>
<span class="p">]</span>
</pre></div>
</div>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method returns <code class="docutils literal notranslate"><span class="pre">None</span></code> which
means there are no files to download for the item.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.files.FilesPipeline.item_completed">
<span class="sig-name descname"><span class="pre">item_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.item_completed()</span></code></a> method called when all file
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.</p>
<p>Here is an example of the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method where we
store the downloaded file paths (passed in results) in the <code class="docutils literal notranslate"><span class="pre">file_paths</span></code>
item field, and we drop the item if it doesn’t contain any files:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>


<span class="k">def</span> <span class="nf">item_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
    <span class="n">file_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;path&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ok</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">if</span> <span class="n">ok</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">file_paths</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Item contains no files&quot;</span><span class="p">)</span>
    <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
    <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;file_paths&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_paths</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-scrapy.pipelines.images"></span><p>See here the methods that you can override in your custom Images Pipeline:</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.pipelines.images.ImagesPipeline">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.pipelines.images.</span></span><span class="sig-name descname"><span class="pre">ImagesPipeline</span></span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> is an extension of the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>,
customizing the field names and adding custom behavior for images.</p>
</div></blockquote>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.images.ImagesPipeline.file_path">
<span class="sig-name descname"><span class="pre">file_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.file_path" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called once per downloaded item. It returns the
download path of the file originating from the specified
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">response</span></code>, this method receives the original
<code class="xref py py-class docutils literal notranslate"><span class="pre">request</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">info</span></code> and
<a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">item</span></code></a></p>
<p>You can override this method to customize the download path of each file.</p>
<p>For example, if file URLs end like regular paths (e.g.
<code class="docutils literal notranslate"><span class="pre">https://example.com/a/b/c/foo.png</span></code>), you can use the following
approach to download all files into the <code class="docutils literal notranslate"><span class="pre">files</span></code> folder with their
original filenames (e.g. <code class="docutils literal notranslate"><span class="pre">files/foo.png</span></code>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">PurePosixPath</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.httpobj</span> <span class="kn">import</span> <span class="n">urlparse_cached</span>

<span class="kn">from</span> <span class="nn">scrapy.pipelines.images</span> <span class="kn">import</span> <span class="n">ImagesPipeline</span>


<span class="k">class</span> <span class="nc">MyImagesPipeline</span><span class="p">(</span><span class="n">ImagesPipeline</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;files/&quot;</span> <span class="o">+</span> <span class="n">PurePosixPath</span><span class="p">(</span><span class="n">urlparse_cached</span><span class="p">(</span><span class="n">request</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
</pre></div>
</div>
<p>Similarly, you can use the <code class="docutils literal notranslate"><span class="pre">item</span></code> to determine the file path based on some item
property.</p>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a> method returns
<code class="docutils literal notranslate"><span class="pre">full/&lt;request</span> <span class="pre">URL</span> <span class="pre">hash&gt;.&lt;extension&gt;</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.4: </span>The <em>item</em> parameter.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.images.ImagesPipeline.thumb_path">
<span class="sig-name descname"><span class="pre">thumb_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thumb_id</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for every item of  <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_THUMBS</span></code></a> per downloaded item. It returns the
thumbnail download path of the image originating from the specified
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">response</span></code>, this method receives the original
<code class="xref py py-class docutils literal notranslate"><span class="pre">request</span></code>,
<code class="docutils literal notranslate"><span class="pre">thumb_id</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">info</span></code> and
<a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">item</span></code></a>.</p>
<p>You can override this method to customize the thumbnail download path of each image.
You can use the <code class="docutils literal notranslate"><span class="pre">item</span></code> to determine the file path based on some item
property.</p>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">thumb_path()</span></code></a> method returns
<code class="docutils literal notranslate"><span class="pre">thumbs/&lt;size</span> <span class="pre">name&gt;/&lt;request</span> <span class="pre">URL</span> <span class="pre">hash&gt;.&lt;extension&gt;</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.images.ImagesPipeline.get_media_requests">
<span class="sig-name descname"><span class="pre">get_media_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>Works the same way as <code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.get_media_requests()</span></code> method,
but using a different field name for image urls.</p>
<p>Must return a Request for each image URL.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.pipelines.images.ImagesPipeline.item_completed">
<span class="sig-name descname"><span class="pre">item_completed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">info</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.item_completed()</span></code></a> method is called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>Works the same way as <code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.item_completed()</span></code> method,
but using a different field names for storing image downloading results.</p>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

</section>
<section id="custom-images-pipeline-example">
<span id="media-pipeline-example"></span><h4>Custom Images pipeline example<a class="headerlink" href="#custom-images-pipeline-example" title="Permalink to this heading">¶</a></h4>
<p>Here is a full example of the Images Pipeline whose methods are exemplified
above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
<span class="kn">from</span> <span class="nn">scrapy.pipelines.images</span> <span class="kn">import</span> <span class="n">ImagesPipeline</span>


<span class="k">class</span> <span class="nc">MyImagesPipeline</span><span class="p">(</span><span class="n">ImagesPipeline</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_media_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">image_url</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;image_urls&quot;</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">item_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="n">image_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;path&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ok</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">if</span> <span class="n">ok</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">image_paths</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Item contains no images&quot;</span><span class="p">)</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;image_paths&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>To enable your custom media pipeline component you must add its class import path to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting, like in the following example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;myproject.pipelines.MyImagesPipeline&quot;</span><span class="p">:</span> <span class="mi">300</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<span id="document-topics/deploy"></span><section id="deploying-spiders">
<span id="topics-deploy"></span><h3>Deploying Spiders<a class="headerlink" href="#deploying-spiders" title="Permalink to this heading">¶</a></h3>
<p>This section describes the different options you have for deploying your Scrapy
spiders to run them on a regular basis. Running Scrapy spiders in your local
machine is very convenient for the (early) development stage, but not so much
when you need to execute long-running spiders or move spiders to run in
production continuously. This is where the solutions for deploying Scrapy
spiders come in.</p>
<p>Popular choices for deploying Scrapy spiders are:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#deploy-scrapyd"><span class="std std-ref">Scrapyd</span></a> (open source)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#deploy-scrapy-cloud"><span class="std std-ref">Zyte Scrapy Cloud</span></a> (cloud-based)</p></li>
</ul>
<section id="deploying-to-a-scrapyd-server">
<span id="deploy-scrapyd"></span><h4>Deploying to a Scrapyd Server<a class="headerlink" href="#deploying-to-a-scrapyd-server" title="Permalink to this heading">¶</a></h4>
<p><a class="reference external" href="https://github.com/scrapy/scrapyd">Scrapyd</a> is an open source application to run Scrapy spiders. It provides
a server with HTTP API, capable of running and monitoring Scrapy spiders.</p>
<p>To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by
the <a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a> package. Please refer to the <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">scrapyd-deploy
documentation</a> for more information.</p>
<p>Scrapyd is maintained by some of the Scrapy developers.</p>
</section>
<section id="deploying-to-zyte-scrapy-cloud">
<span id="deploy-scrapy-cloud"></span><h4>Deploying to Zyte Scrapy Cloud<a class="headerlink" href="#deploying-to-zyte-scrapy-cloud" title="Permalink to this heading">¶</a></h4>
<p><a class="reference external" href="https://www.zyte.com/scrapy-cloud/">Zyte Scrapy Cloud</a> is a hosted, cloud-based service by <a class="reference external" href="https://www.zyte.com/">Zyte</a>, the company
behind Scrapy.</p>
<p>Zyte Scrapy Cloud removes the need to setup and monitor servers and provides a
nice UI to manage spiders and review scraped items, logs and stats.</p>
<p>To deploy spiders to Zyte Scrapy Cloud you can use the <a class="reference external" href="https://shub.readthedocs.io/en/latest/">shub</a> command line
tool.
Please refer to the <a class="reference external" href="https://docs.zyte.com/scrapy-cloud.html">Zyte Scrapy Cloud documentation</a> for more information.</p>
<p>Zyte Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file
just like <code class="docutils literal notranslate"><span class="pre">scrapyd-deploy</span></code>.</p>
</section>
</section>
<span id="document-topics/autothrottle"></span><section id="autothrottle-extension">
<span id="topics-autothrottle"></span><h3>AutoThrottle extension<a class="headerlink" href="#autothrottle-extension" title="Permalink to this heading">¶</a></h3>
<p>This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.</p>
<section id="design-goals">
<h4>Design goals<a class="headerlink" href="#design-goals" title="Permalink to this heading">¶</a></h4>
<ol class="arabic simple">
<li><p>be nicer to sites instead of using default download delay of zero</p></li>
<li><p>automatically adjust Scrapy to the optimum crawling speed, so the user
doesn’t have to tune the download delays to find the optimum one.
The user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.</p></li>
</ol>
</section>
<section id="how-it-works">
<span id="autothrottle-algorithm"></span><h4>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this heading">¶</a></h4>
<p>Scrapy allows defining the concurrency and delay of different download slots,
e.g. through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_SLOTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_SLOTS</span></code></a> setting. By default requests are
assigned to slots based on their URL domain, although it is possible to
customize the download slot of any request.</p>
<p>The AutoThrottle extension adjusts the delay of each download slot dynamically,
to make your spider send <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> concurrent
requests on average to each remote website.</p>
<p>It uses download latency to compute the delays. The main idea is the
following: if a server needs <code class="docutils literal notranslate"><span class="pre">latency</span></code> seconds to respond, a client
should send a request each <code class="docutils literal notranslate"><span class="pre">latency/N</span></code> seconds to have <code class="docutils literal notranslate"><span class="pre">N</span></code> requests
processed in parallel.</p>
<p>Instead of adjusting the delays one can just set a small fixed
download delay and impose hard limits on concurrency using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options. It will provide a similar
effect, but there are some important differences:</p>
<ul class="simple">
<li><p>because the download delay is small there will be occasional bursts
of requests;</p></li>
<li><p>often non-200 (error) responses can be returned faster than regular
responses, so with a small download delay and a hard concurrency limit
crawler will be sending requests to server faster when server starts to
return errors. But this is an opposite of what crawler should do - in case
of errors it makes more sense to slow down: these errors may be caused by
the high request rate.</p></li>
</ul>
<p>AutoThrottle doesn’t have these issues.</p>
</section>
<section id="throttling-algorithm">
<h4>Throttling algorithm<a class="headerlink" href="#throttling-algorithm" title="Permalink to this heading">¶</a></h4>
<p>AutoThrottle algorithm adjusts download delays based on the following rules:</p>
<ol class="arabic simple">
<li><p>spiders always start with a download delay of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a>;</p></li>
<li><p>when a response is received, the target download delay is calculated as
<code class="docutils literal notranslate"><span class="pre">latency</span> <span class="pre">/</span> <span class="pre">N</span></code> where <code class="docutils literal notranslate"><span class="pre">latency</span></code> is a latency of the response,
and <code class="docutils literal notranslate"><span class="pre">N</span></code> is <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a>.</p></li>
<li><p>download delay for next requests is set to the average of previous
download delay and the target download delay;</p></li>
<li><p>latencies of non-200 responses are not allowed to decrease the delay;</p></li>
<li><p>download delay can’t become less than <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> or greater
than <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options and
never set a download delay lower than <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
</div>
<p id="download-latency">In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.</p>
</section>
<section id="prevent-specific-requests-from-triggering-slot-delay-adjustments">
<span id="std-reqmeta-autothrottle_dont_adjust_delay"></span><h4>Prevent specific requests from triggering slot delay adjustments<a class="headerlink" href="#prevent-specific-requests-from-triggering-slot-delay-adjustments" title="Permalink to this heading">¶</a></h4>
<p>AutoThrottle adjusts the delay of download slots based on the latencies of
responses that belong to that download slot. The only exceptions are non-200
responses, which are only taken into account to increase that delay, but
ignored if they would decrease that delay.</p>
<p>You can also set the <code class="docutils literal notranslate"><span class="pre">autothrottle_dont_adjust_delay</span></code> request metadata key to
<code class="docutils literal notranslate"><span class="pre">True</span></code> in any request to prevent its response latency from impacting the
delay of its download slot:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Request</span>

<span class="n">Request</span><span class="p">(</span><span class="s2">&quot;https://example.com&quot;</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;autothrottle_dont_adjust_delay&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</pre></div>
</div>
<p>Note, however, that AutoThrottle still determines the starting delay of every
download slot by setting the <code class="docutils literal notranslate"><span class="pre">download_delay</span></code> attribute on the running
spider. If you want AutoThrottle not to impact a download slot at all, in
addition to setting this meta key in all requests that use that download slot,
you might want to set a custom value for the <code class="docutils literal notranslate"><span class="pre">delay</span></code> attribute of that
download slot, e.g. using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_SLOTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_SLOTS</span></code></a>.</p>
</section>
<section id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this heading">¶</a></h4>
<p>The settings used to control the AutoThrottle extension are:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-AUTOTHROTTLE_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_DEBUG</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a></p></li>
</ul>
<p>For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#autothrottle-algorithm"><span class="std std-ref">How it works</span></a>.</p>
<section id="autothrottle-enabled">
<span id="std-setting-AUTOTHROTTLE_ENABLED"></span><h5>AUTOTHROTTLE_ENABLED<a class="headerlink" href="#autothrottle-enabled" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enables the AutoThrottle extension.</p>
</section>
<section id="autothrottle-start-delay">
<span id="std-setting-AUTOTHROTTLE_START_DELAY"></span><h5>AUTOTHROTTLE_START_DELAY<a class="headerlink" href="#autothrottle-start-delay" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">5.0</span></code></p>
<p>The initial download delay (in seconds).</p>
</section>
<section id="autothrottle-max-delay">
<span id="std-setting-AUTOTHROTTLE_MAX_DELAY"></span><h5>AUTOTHROTTLE_MAX_DELAY<a class="headerlink" href="#autothrottle-max-delay" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>
</section>
<section id="autothrottle-target-concurrency">
<span id="std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"></span><h5>AUTOTHROTTLE_TARGET_CONCURRENCY<a class="headerlink" href="#autothrottle-target-concurrency" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code></p>
<p>Average number of requests Scrapy should be sending in parallel to remote
websites. It must be higher than <code class="docutils literal notranslate"><span class="pre">0.0</span></code>.</p>
<p>By default, AutoThrottle adjusts the delay to send a single
concurrent request to each of the remote websites. Set this option to
a higher value (e.g. <code class="docutils literal notranslate"><span class="pre">2.0</span></code>) to increase the throughput and the load on remote
servers. A lower <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> value
(e.g. <code class="docutils literal notranslate"><span class="pre">0.5</span></code>) makes the crawler more conservative and polite.</p>
<p>Note that <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>
and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options are still respected
when AutoThrottle extension is enabled. This means that if
<code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> is set to a value higher than
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>, the crawler won’t reach this number
of concurrent requests.</p>
<p>At every given time point Scrapy can be sending more or less concurrent
requests than <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>; it is a suggested
value the crawler tries to approach, not a hard limit.</p>
</section>
<section id="autothrottle-debug">
<span id="std-setting-AUTOTHROTTLE_DEBUG"></span><h5>AUTOTHROTTLE_DEBUG<a class="headerlink" href="#autothrottle-debug" title="Permalink to this heading">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.</p>
</section>
</section>
</section>
<span id="document-topics/benchmarking"></span><section id="benchmarking">
<span id="id1"></span><h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this heading">¶</a></h3>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.</p>
<p>To run it use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">bench</span>
</pre></div>
</div>
<p>You should see an output like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">48</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">log</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Scrapy</span> <span class="mf">1.2.2</span> <span class="n">started</span> <span class="p">(</span><span class="n">bot</span><span class="p">:</span> <span class="n">quotesbot</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">48</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">log</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Overridden</span> <span class="n">settings</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;CLOSESPIDER_TIMEOUT&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;ROBOTSTXT_OBEY&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;SPIDER_MODULES&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;quotesbot.spiders&#39;</span><span class="p">],</span> <span class="s1">&#39;LOGSTATS_INTERVAL&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;BOT_NAME&#39;</span><span class="p">:</span> <span class="s1">&#39;quotesbot&#39;</span><span class="p">,</span> <span class="s1">&#39;LOG_LEVEL&#39;</span><span class="p">:</span> <span class="s1">&#39;INFO&#39;</span><span class="p">,</span> <span class="s1">&#39;NEWSPIDER_MODULE&#39;</span><span class="p">:</span> <span class="s1">&#39;quotesbot.spiders&#39;</span><span class="p">}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">extensions</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.extensions.closespider.CloseSpider&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.logstats.LogStats&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">downloader</span> <span class="n">middlewares</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.downloadermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">spider</span> <span class="n">middlewares</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">item</span> <span class="n">pipelines</span><span class="p">:</span>
<span class="p">[]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">0</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">50</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">70</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">4200</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">51</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">134</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3840</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">52</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">198</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3840</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">53</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">254</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3360</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">54</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">302</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">55</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">358</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3360</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">56</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">406</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">57</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">438</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">1920</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">58</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">470</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">1920</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">59</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Closing</span> <span class="n">spider</span> <span class="p">(</span><span class="n">closespider_timeout</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">59</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">518</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">19</span><span class="p">:</span><span class="mi">00</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">statscollectors</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Dumping</span> <span class="n">Scrapy</span> <span class="n">stats</span><span class="p">:</span>
<span class="p">{</span><span class="s1">&#39;downloader/request_bytes&#39;</span><span class="p">:</span> <span class="mi">229995</span><span class="p">,</span>
 <span class="s1">&#39;downloader/request_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/request_method_count/GET&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_bytes&#39;</span><span class="p">:</span> <span class="mi">1565504</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_status_count/200&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;finish_reason&#39;</span><span class="p">:</span> <span class="s1">&#39;closespider_timeout&#39;</span><span class="p">,</span>
 <span class="s1">&#39;finish_time&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2016</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">647725</span><span class="p">),</span>
 <span class="s1">&#39;log_count/INFO&#39;</span><span class="p">:</span> <span class="mi">17</span><span class="p">,</span>
 <span class="s1">&#39;request_depth_max&#39;</span><span class="p">:</span> <span class="mi">19</span><span class="p">,</span>
 <span class="s1">&#39;response_received_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/dequeued&#39;</span><span class="p">:</span> <span class="mi">533</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/dequeued/memory&#39;</span><span class="p">:</span> <span class="mi">533</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/enqueued&#39;</span><span class="p">:</span> <span class="mi">10661</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/enqueued/memory&#39;</span><span class="p">:</span> <span class="mi">10661</span><span class="p">,</span>
 <span class="s1">&#39;start_time&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2016</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">799869</span><span class="p">)}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">19</span><span class="p">:</span><span class="mi">00</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">closed</span> <span class="p">(</span><span class="n">closespider_timeout</span><span class="p">)</span>
</pre></div>
</div>
<p>That tells you that Scrapy is able to crawl about 3000 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it’s written.</p>
<p>Use <a class="reference external" href="https://github.com/scrapy/scrapy-bench">scrapy-bench</a> for more complex benchmarking.</p>
</section>
<span id="document-topics/jobs"></span><section id="jobs-pausing-and-resuming-crawls">
<span id="topics-jobs"></span><h3>Jobs: pausing and resuming crawls<a class="headerlink" href="#jobs-pausing-and-resuming-crawls" title="Permalink to this heading">¶</a></h3>
<p>Sometimes, for big sites, it’s desirable to pause crawls and be able to resume
them later.</p>
<p>Scrapy supports this functionality out of the box by providing the following
facilities:</p>
<ul class="simple">
<li><p>a scheduler that persists scheduled requests on disk</p></li>
<li><p>a duplicates filter that persists visited requests on disk</p></li>
<li><p>an extension that keeps some spider state (key/value pairs) persistent
between batches</p></li>
</ul>
<section id="job-directory">
<h4>Job directory<a class="headerlink" href="#job-directory" title="Permalink to this heading">¶</a></h4>
<p>To enable persistence support you just need to define a <em>job directory</em> through
the <code class="docutils literal notranslate"><span class="pre">JOBDIR</span></code> setting. This directory will be for storing all required data to
keep the state of a single job (i.e. a spider run).  It’s important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it’s meant to be used for storing the state of
a <em>single</em> job.</p>
</section>
<section id="how-to-use-it">
<h4>How to use it<a class="headerlink" href="#how-to-use-it" title="Permalink to this heading">¶</a></h4>
<p>To start a spider with persistence support enabled, run it like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">somespider</span> <span class="o">-</span><span class="n">s</span> <span class="n">JOBDIR</span><span class="o">=</span><span class="n">crawls</span><span class="o">/</span><span class="n">somespider</span><span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending
a signal), and resume it later by issuing the same command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">somespider</span> <span class="o">-</span><span class="n">s</span> <span class="n">JOBDIR</span><span class="o">=</span><span class="n">crawls</span><span class="o">/</span><span class="n">somespider</span><span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="keeping-persistent-state-between-batches">
<span id="topics-keeping-persistent-state-between-batches"></span><h4>Keeping persistent state between batches<a class="headerlink" href="#keeping-persistent-state-between-batches" title="Permalink to this heading">¶</a></h4>
<p>Sometimes you’ll want to keep some persistent spider state between pause/resume
batches. You can use the <code class="docutils literal notranslate"><span class="pre">spider.state</span></code> attribute for that, which should be a
dict. There’s <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions-ref-spiderstate"><span class="std std-ref">a built-in extension</span></a> that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.</p>
<p>Here’s an example of a callback that uses the spider state (other spider code
is omitted for brevity):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># parse item here</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;items_count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;items_count&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</section>
<section id="persistence-gotchas">
<h4>Persistence gotchas<a class="headerlink" href="#persistence-gotchas" title="Permalink to this heading">¶</a></h4>
<p>There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:</p>
<section id="cookies-expiration">
<h5>Cookies expiration<a class="headerlink" href="#cookies-expiration" title="Permalink to this heading">¶</a></h5>
<p>Cookies may expire. So, if you don’t resume your spider quickly the requests
scheduled may no longer work. This won’t be an issue if your spider doesn’t rely
on cookies.</p>
</section>
<section id="request-serialization">
<span id="id1"></span><h5>Request serialization<a class="headerlink" href="#request-serialization" title="Permalink to this heading">¶</a></h5>
<p>For persistence to work, <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects must be
serializable with <a class="reference external" href="https://docs.python.org/3/library/pickle.html#module-pickle" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pickle</span></code></a>, except for the <code class="docutils literal notranslate"><span class="pre">callback</span></code> and <code class="docutils literal notranslate"><span class="pre">errback</span></code>
values passed to their <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, which must be methods of the
running <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> class.</p>
<p>If you wish to log the requests that couldn’t be serialized, you can set the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting to <code class="docutils literal notranslate"><span class="pre">True</span></code> in the project’s settings page.
It is <code class="docutils literal notranslate"><span class="pre">False</span></code> by default.</p>
</section>
</section>
</section>
<span id="document-topics/coroutines"></span><section id="coroutines">
<span id="topics-coroutines"></span><h3>Coroutines<a class="headerlink" href="#coroutines" title="Permalink to this heading">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Scrapy has <a class="hxr-hoverxref hxr-tooltip reference internal" href="#coroutine-support"><span class="std std-ref">partial support</span></a> for the
<a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.13)"><span class="xref std std-ref">coroutine syntax</span></a>.</p>
<section id="supported-callables">
<span id="coroutine-support"></span><h4>Supported callables<a class="headerlink" href="#supported-callables" title="Permalink to this heading">¶</a></h4>
<p>The following callables may be defined as coroutines using <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">def</span></code>, and
hence use coroutine syntax (e.g. <code class="docutils literal notranslate"><span class="pre">await</span></code>, <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">for</span></code>, <code class="docutils literal notranslate"><span class="pre">async</span> <span class="pre">with</span></code>):</p>
<ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> callbacks.</p>
<p>If you are using any custom or third-party <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middleware</span></a>, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#sync-async-spider-middleware"><span class="std std-ref">Mixing synchronous and asynchronous spider middlewares</span></a>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.7: </span>Output of async callbacks is now processed asynchronously instead of
collecting all of it first.</p>
</div>
</li>
<li><p>The <a class="reference internal" href="index.html#process_item" title="process_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_item()</span></code></a> method of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a>.</p></li>
<li><p>The
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>,
and
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a>
methods of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware-custom"><span class="std std-ref">downloader middlewares</span></a>.</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#signal-deferred"><span class="std std-ref">Signal handlers that support deferreds</span></a>.</p></li>
<li><p>The
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a>.</p>
<p>It must be defined as an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a>. The input
<code class="docutils literal notranslate"><span class="pre">result</span></code> parameter is an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-iterable" title="(in Python v3.13)"><span class="xref std std-term">asynchronous iterable</span></a>.</p>
<p>See also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#sync-async-spider-middleware"><span class="std std-ref">Mixing synchronous and asynchronous spider middlewares</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#universal-spider-middleware"><span class="std std-ref">Universal spider middlewares</span></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.7.</span></p>
</div>
</li>
</ul>
</section>
<section id="general-usage">
<h4>General usage<a class="headerlink" href="#general-usage" title="Permalink to this heading">¶</a></h4>
<p>There are several use cases for coroutines in Scrapy.</p>
<p>Code that would return Deferreds when written for previous Scrapy versions,
such as downloader middlewares and signal handlers, can be rewritten to be
shorter and cleaner:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>


<span class="k">class</span> <span class="nc">DbPipeline</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_update_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;field&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>
        <span class="k">return</span> <span class="n">item</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">dfd</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">get_some_data</span><span class="p">(</span><span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">])</span>
        <span class="n">dfd</span><span class="o">.</span><span class="n">addCallback</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_update_item</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dfd</span>
</pre></div>
</div>
<p>becomes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>


<span class="k">class</span> <span class="nc">DbPipeline</span><span class="p">:</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;field&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="k">await</span> <span class="n">db</span><span class="o">.</span><span class="n">get_some_data</span><span class="p">(</span><span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Coroutines may be used to call asynchronous code. This includes other
coroutines, functions that return Deferreds and functions that return
<a class="reference external" href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.13)"><span class="xref std std-term">awaitable objects</span></a> such as <a class="reference external" href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Future</span></code></a>.
This means you can use many useful Python libraries providing such code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpiderDeferred</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">additional_response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">treq</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://additional.url&quot;</span><span class="p">)</span>
        <span class="n">additional_data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">treq</span><span class="o">.</span><span class="n">content</span><span class="p">(</span><span class="n">additional_response</span><span class="p">)</span>
        <span class="c1"># ... use response and additional_data to yield items and requests</span>


<span class="k">class</span> <span class="nc">MySpiderAsyncio</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">aiohttp</span><span class="o">.</span><span class="n">ClientSession</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
            <span class="k">async</span> <span class="k">with</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://additional.url&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">additional_response</span><span class="p">:</span>
                <span class="n">additional_data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">additional_response</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
        <span class="c1"># ... use response and additional_data to yield items and requests</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many libraries that use coroutines, such as <a class="reference external" href="https://github.com/aio-libs">aio-libs</a>, require the
<a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> loop and to use them you need to
<a class="reference internal" href="index.html#document-topics/asyncio"><span class="doc">enable asyncio support in Scrapy</span></a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to <code class="docutils literal notranslate"><span class="pre">await</span></code> on Deferreds while using the asyncio reactor,
you need to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#asyncio-await-dfd"><span class="std std-ref">wrap them</span></a>.</p>
</div>
<p>Common use cases for asynchronous code include:</p>
<ul class="simple">
<li><p>requesting data from websites, databases and other services (in callbacks,
pipelines and middlewares);</p></li>
<li><p>storing data in databases (in pipelines and middlewares);</p></li>
<li><p>delaying the spider initialization until some external event (in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a> handler);</p></li>
<li><p>calling asynchronous Scrapy methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">ExecutionEngine.download()</span></code>
(see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#screenshotpipeline"><span class="std std-ref">the screenshot pipeline example</span></a>).</p></li>
</ul>
</section>
<section id="inline-requests">
<span id="id1"></span><h4>Inline requests<a class="headerlink" href="#inline-requests" title="Permalink to this heading">¶</a></h4>
<p>The spider below shows how to send a request and await its response all from
within a spider callback:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span><span class="p">,</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.defer</span> <span class="kn">import</span> <span class="n">maybe_deferred_to_future</span>


<span class="k">class</span> <span class="nc">SingleRequestSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;single&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://example.org/product&quot;</span><span class="p">]</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">additional_request</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="s2">&quot;https://example.org/price&quot;</span><span class="p">)</span>
        <span class="n">deferred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">additional_request</span><span class="p">)</span>
        <span class="n">additional_response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">maybe_deferred_to_future</span><span class="p">(</span><span class="n">deferred</span><span class="p">)</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s2">&quot;h1&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="s2">&quot;price&quot;</span><span class="p">:</span> <span class="n">additional_response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;#price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>You can also send multiple requests in parallel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span><span class="p">,</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.defer</span> <span class="kn">import</span> <span class="n">maybe_deferred_to_future</span>
<span class="kn">from</span> <span class="nn">twisted.internet.defer</span> <span class="kn">import</span> <span class="n">DeferredList</span>


<span class="k">class</span> <span class="nc">MultipleRequestsSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;multiple&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://example.com/product&quot;</span><span class="p">]</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">additional_requests</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">Request</span><span class="p">(</span><span class="s2">&quot;https://example.com/price&quot;</span><span class="p">),</span>
            <span class="n">Request</span><span class="p">(</span><span class="s2">&quot;https://example.com/color&quot;</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="n">deferreds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">additional_requests</span><span class="p">:</span>
            <span class="n">deferred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="n">deferreds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">deferred</span><span class="p">)</span>
        <span class="n">responses</span> <span class="o">=</span> <span class="k">await</span> <span class="n">maybe_deferred_to_future</span><span class="p">(</span><span class="n">DeferredList</span><span class="p">(</span><span class="n">deferreds</span><span class="p">))</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s2">&quot;h1&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;h1::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="s2">&quot;price&quot;</span><span class="p">:</span> <span class="n">responses</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;.price::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="s2">&quot;price2&quot;</span><span class="p">:</span> <span class="n">responses</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;.color::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
        <span class="p">}</span>
</pre></div>
</div>
</section>
<section id="mixing-synchronous-and-asynchronous-spider-middlewares">
<span id="sync-async-spider-middleware"></span><h4>Mixing synchronous and asynchronous spider middlewares<a class="headerlink" href="#mixing-synchronous-and-asynchronous-spider-middlewares" title="Permalink to this heading">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.7.</span></p>
</div>
<p>The output of a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> callback is passed as the <code class="docutils literal notranslate"><span class="pre">result</span></code>
parameter to the
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> method
of the first <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middleware</span></a> from the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware-setting"><span class="std std-ref">list of active spider middlewares</span></a>.
Then the output of that <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method is passed to the
<code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method of the next spider middleware, and so on for
every active spider middleware.</p>
<p>Scrapy supports mixing <a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.13)"><span class="xref std std-ref">coroutine methods</span></a> and synchronous methods
in this chain of calls.</p>
<p>However, if any of the <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> methods is defined as a
synchronous method, and the previous <code class="docutils literal notranslate"><span class="pre">Request</span></code> callback or
<code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method is a coroutine, there are some drawbacks to
the asynchronous-to-synchronous conversion that Scrapy does so that the
synchronous <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method gets a synchronous iterable as its
<code class="docutils literal notranslate"><span class="pre">result</span></code> parameter:</p>
<ul>
<li><p>The whole output of the previous <code class="docutils literal notranslate"><span class="pre">Request</span></code> callback or
<code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method is awaited at this point.</p></li>
<li><p>If an exception raises while awaiting the output of the previous
<code class="docutils literal notranslate"><span class="pre">Request</span></code> callback or <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method, none of that
output will be processed.</p>
<p>This contrasts with the regular behavior, where all items yielded before
an exception raises are processed.</p>
</li>
</ul>
<p>Asynchronous-to-synchronous conversions are supported for backward
compatibility, but they are deprecated and will stop working in a future
version of Scrapy.</p>
<p>To avoid asynchronous-to-synchronous conversions, when defining <code class="docutils literal notranslate"><span class="pre">Request</span></code>
callbacks as coroutine methods or when using spider middlewares whose
<code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method is an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a>, all
active spider middlewares must either have their <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code>
method defined as an asynchronous generator or <a class="hxr-hoverxref hxr-tooltip reference internal" href="#universal-spider-middleware"><span class="std std-ref">define a
process_spider_output_async method</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using third-party spider middlewares that only define a
synchronous <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method, consider
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#universal-spider-middleware"><span class="std std-ref">making them universal</span></a> through
<a class="reference external" href="https://docs.python.org/3/tutorial/classes.html#tut-inheritance" title="(in Python v3.13)"><span class="xref std std-ref">subclassing</span></a>.</p>
</div>
</section>
<section id="universal-spider-middlewares">
<span id="universal-spider-middleware"></span><h4>Universal spider middlewares<a class="headerlink" href="#universal-spider-middlewares" title="Permalink to this heading">¶</a></h4>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.7.</span></p>
</div>
<p>To allow writing a spider middleware that supports asynchronous execution of
its <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method in Scrapy 2.7 and later (avoiding
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#sync-async-spider-middleware"><span class="std std-ref">asynchronous-to-synchronous conversions</span></a>)
while maintaining support for older Scrapy versions, you may define
<code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> as a synchronous method and define an
<a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a> version of that method with an alternative name:
<code class="docutils literal notranslate"><span class="pre">process_spider_output_async</span></code>.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">UniversalSpiderMiddleware</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">process_spider_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="c1"># ... do something with r</span>
            <span class="k">yield</span> <span class="n">r</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">process_spider_output_async</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="c1"># ... do something with r</span>
            <span class="k">yield</span> <span class="n">r</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an interim measure to allow, for a time, to write code that
works in Scrapy 2.7 and later without requiring
asynchronous-to-synchronous conversions, and works in earlier Scrapy
versions as well.</p>
<p>In some future version of Scrapy, however, this feature will be
deprecated and, eventually, in a later version of Scrapy, this
feature will be removed, and all spider middlewares will be expected
to define their <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> method as an asynchronous
generator.</p>
</div>
</section>
</section>
<span id="document-topics/asyncio"></span><section id="asyncio">
<span id="using-asyncio"></span><h3>asyncio<a class="headerlink" href="#asyncio" title="Permalink to this heading">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Scrapy has partial support for <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a>. After you <a class="hxr-hoverxref hxr-tooltip reference internal" href="#install-asyncio"><span class="std std-ref">install the
asyncio reactor</span></a>, you may use <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> and
<a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a>-powered libraries in any <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">coroutine</span></a>.</p>
<section id="installing-the-asyncio-reactor">
<span id="install-asyncio"></span><h4>Installing the asyncio reactor<a class="headerlink" href="#installing-the-asyncio-reactor" title="Permalink to this heading">¶</a></h4>
<p>To enable <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> support, set the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting to
<code class="docutils literal notranslate"><span class="pre">'twisted.internet.asyncioreactor.AsyncioSelectorReactor'</span></code>.</p>
<p>If you are using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>, you also need to
install the <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">AsyncioSelectorReactor</span></code></a>
reactor manually. You can do that using
<a class="reference internal" href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">install_reactor</span><span class="p">(</span><span class="s1">&#39;twisted.internet.asyncioreactor.AsyncioSelectorReactor&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="handling-a-pre-installed-reactor">
<span id="asyncio-preinstalled-reactor"></span><h4>Handling a pre-installed reactor<a class="headerlink" href="#handling-a-pre-installed-reactor" title="Permalink to this heading">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">twisted.internet.reactor</span></code> and some other Twisted imports install the default
Twisted reactor as a side effect. Once a Twisted reactor is installed, it is
not possible to switch to a different reactor at run time.</p>
<p>If you <a class="hxr-hoverxref hxr-tooltip reference internal" href="#install-asyncio"><span class="std std-ref">configure the asyncio Twisted reactor</span></a> and, at
run time, Scrapy complains that a different reactor is already installed,
chances are you have some such imports in your code.</p>
<p>You can usually fix the issue by moving those offending module-level Twisted
imports to the method or function definitions where they are used. For example,
if you have something like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>


<span class="k">def</span> <span class="nf">my_function</span><span class="p">():</span>
    <span class="n">reactor</span><span class="o">.</span><span class="n">callLater</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>Switch to something like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_function</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="kn">import</span> <span class="n">reactor</span>

    <span class="n">reactor</span><span class="o">.</span><span class="n">callLater</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can try to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#install-asyncio"><span class="std std-ref">manually install the asyncio reactor</span></a>, with <a class="reference internal" href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>, before
those imports happen.</p>
</section>
<section id="awaiting-on-deferreds">
<span id="asyncio-await-dfd"></span><h4>Awaiting on Deferreds<a class="headerlink" href="#awaiting-on-deferreds" title="Permalink to this heading">¶</a></h4>
<p>When the asyncio reactor isn’t installed, you can await on Deferreds in the
coroutines directly. When it is installed, this is not possible anymore, due to
specifics of the Scrapy coroutine integration (the coroutines are wrapped into
<a class="reference external" href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">asyncio.Future</span></code></a> objects, not into
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> directly), and you need to wrap them into
Futures. Scrapy provides two helpers for this:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.defer.deferred_to_future">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.defer.</span></span><span class="sig-name descname"><span class="pre">deferred_to_future</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Future</span><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.utils.defer.deferred_to_future" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
<p>Return an <a class="reference external" href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">asyncio.Future</span></code></a> object that wraps <em>d</em>.</p>
<p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="#install-asyncio"><span class="std std-ref">using the asyncio reactor</span></a>, you cannot await
on <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> objects from <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#coroutine-support"><span class="std std-ref">Scrapy
callables defined as coroutines</span></a>, you can only await on
<code class="docutils literal notranslate"><span class="pre">Future</span></code> objects. Wrapping <code class="docutils literal notranslate"><span class="pre">Deferred</span></code> objects into <code class="docutils literal notranslate"><span class="pre">Future</span></code> objects
allows you to wait on them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">additional_request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;https://example.org/price&#39;</span><span class="p">)</span>
        <span class="n">deferred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">additional_request</span><span class="p">)</span>
        <span class="n">additional_response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">deferred_to_future</span><span class="p">(</span><span class="n">deferred</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.defer.maybe_deferred_to_future">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.defer.</span></span><span class="sig-name descname"><span class="pre">maybe_deferred_to_future</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><span class="pre">Future</span><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.utils.defer.maybe_deferred_to_future" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.6.0.</span></p>
</div>
<p>Return <em>d</em> as an object that can be awaited from a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#coroutine-support"><span class="std std-ref">Scrapy callable
defined as a coroutine</span></a>.</p>
<p>What you can await in Scrapy callables defined as coroutines depends on the
value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a>:</p>
<ul class="simple">
<li><p>When not using the asyncio reactor, you can only await on
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> objects.</p></li>
<li><p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="#install-asyncio"><span class="std std-ref">using the asyncio reactor</span></a>, you can only
await on <a class="reference external" href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">asyncio.Future</span></code></a> objects.</p></li>
</ul>
<p>If you want to write code that uses <code class="docutils literal notranslate"><span class="pre">Deferred</span></code> objects but works with any
reactor, use this function on all <code class="docutils literal notranslate"><span class="pre">Deferred</span></code> objects:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">additional_request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;https://example.org/price&#39;</span><span class="p">)</span>
        <span class="n">deferred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">additional_request</span><span class="p">)</span>
        <span class="n">additional_response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">maybe_deferred_to_future</span><span class="p">(</span><span class="n">deferred</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you need to use these functions in code that aims to be compatible
with lower versions of Scrapy that do not provide these functions,
down to Scrapy 2.0 (earlier versions do not support
<a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a>), you can copy the implementation of these functions
into your own code.</p>
</div>
</section>
<section id="enforcing-asyncio-as-a-requirement">
<span id="enforce-asyncio-requirement"></span><h4>Enforcing asyncio as a requirement<a class="headerlink" href="#enforcing-asyncio-as-a-requirement" title="Permalink to this heading">¶</a></h4>
<p>If you are writing a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-components"><span class="std std-ref">component</span></a> that requires asyncio
to work, use <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.reactor.is_asyncio_reactor_installed()</span></code> to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#enforce-component-requirements"><span class="std std-ref">enforce it as a requirement</span></a>. For
example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.utils.reactor</span> <span class="kn">import</span> <span class="n">is_asyncio_reactor_installed</span>


<span class="k">class</span> <span class="nc">MyComponent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_asyncio_reactor_installed</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MyComponent</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> requires the asyncio Twisted &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;reactor. Make sure you have it configured in the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;TWISTED_REACTOR setting. See the asyncio documentation &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;of Scrapy for more information.&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="windows-specific-notes">
<span id="asyncio-windows"></span><h4>Windows-specific notes<a class="headerlink" href="#windows-specific-notes" title="Permalink to this heading">¶</a></h4>
<p>The Windows implementation of <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> can use two event loop
implementations, <a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProactorEventLoop</span></code></a> (default) and
<a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorEventLoop</span></code></a>. However, only
<a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorEventLoop</span></code></a> works with Twisted.</p>
<p>Scrapy changes the event loop class to <a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorEventLoop</span></code></a>
automatically when you change the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting or call
<a class="reference internal" href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor"><code class="xref py py-func docutils literal notranslate"><span class="pre">install_reactor()</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Other libraries you use may require
<a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProactorEventLoop</span></code></a>, e.g. because it supports
subprocesses (this is the case with <a class="reference external" href="https://github.com/microsoft/playwright-python">playwright</a>), so you cannot use
them together with Scrapy on Windows (but you should be able to use
them on WSL or native Linux).</p>
</div>
</section>
<section id="using-custom-asyncio-loops">
<span id="using-custom-loops"></span><h4>Using custom asyncio loops<a class="headerlink" href="#using-custom-asyncio-loops" title="Permalink to this heading">¶</a></h4>
<p>You can also use custom asyncio event loops with the asyncio reactor. Set the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ASYNCIO_EVENT_LOOP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ASYNCIO_EVENT_LOOP</span></code></a> setting to the import path of the desired event
loop class to use it instead of the default asyncio event loop.</p>
</section>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-faq"><span class="doc">Frequently Asked Questions</span></a></dt><dd><p>Get answers to most frequently asked questions.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/debug"><span class="doc">Debugging Spiders</span></a></dt><dd><p>Learn how to debug common problems of your Scrapy spider.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/contracts"><span class="doc">Spiders Contracts</span></a></dt><dd><p>Learn how to use contracts for testing your spiders.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/practices"><span class="doc">Common Practices</span></a></dt><dd><p>Get familiar with some Scrapy common practices.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/broad-crawls"><span class="doc">Broad Crawls</span></a></dt><dd><p>Tune Scrapy for crawling a lot domains in parallel.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/developer-tools"><span class="doc">Using your browser’s Developer Tools for scraping</span></a></dt><dd><p>Learn how to scrape with your browser’s developer tools.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/dynamic-content"><span class="doc">Selecting dynamically-loaded content</span></a></dt><dd><p>Read webpage data that is loaded dynamically.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/leaks"><span class="doc">Debugging memory leaks</span></a></dt><dd><p>Learn how to find and get rid of memory leaks in your crawler.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/media-pipeline"><span class="doc">Downloading and processing files and images</span></a></dt><dd><p>Download files and/or images associated with your scraped items.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/deploy"><span class="doc">Deploying Spiders</span></a></dt><dd><p>Deploying your Scrapy spiders and run them in a remote server.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/autothrottle"><span class="doc">AutoThrottle extension</span></a></dt><dd><p>Adjust crawl rate dynamically based on load.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/benchmarking"><span class="doc">Benchmarking</span></a></dt><dd><p>Check how Scrapy performs on your hardware.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/jobs"><span class="doc">Jobs: pausing and resuming crawls</span></a></dt><dd><p>Learn how to pause and resume crawls for large spiders.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">Coroutines</span></a></dt><dd><p>Use the <a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.13)"><span class="xref std std-ref">coroutine syntax</span></a>.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/asyncio"><span class="doc">asyncio</span></a></dt><dd><p>Use <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> and <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a>-powered libraries.</p>
</dd>
</dl>
</section>
<section id="extending-scrapy">
<span id="id2"></span><h2>Extending Scrapy<a class="headerlink" href="#extending-scrapy" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/architecture"></span><section id="architecture-overview">
<span id="topics-architecture"></span><h3>Architecture overview<a class="headerlink" href="#architecture-overview" title="Permalink to this heading">¶</a></h3>
<p>This document describes the architecture of Scrapy and how its components
interact.</p>
<section id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h4>
<p>The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the red arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.</p>
</section>
<section id="data-flow">
<span id="id1"></span><h4>Data flow<a class="headerlink" href="#data-flow" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="_images/scrapy_architecture_02.png"><img alt="Scrapy architecture" src="_images/scrapy_architecture_02.png" style="width: 700px; height: 470px;" /></a>
<p>The data flow in Scrapy is controlled by the execution engine, and goes like
this:</p>
<ol class="arabic simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> gets the initial Requests to crawl from the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a>.</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> schedules the Requests in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> and asks for the
next Requests to crawl.</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> returns the next Requests
to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a>.</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> sends the Requests to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a>, passing through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a> (see
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>).</p></li>
<li><p>Once the page finishes downloading the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a> generates a Response (with
that page) and sends it to the Engine, passing through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a> (see
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>).</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> receives the Response from the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a> and sends it to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a> for processing, passing
through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-spider-middleware"><span class="std std-ref">Spider Middleware</span></a> (see
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a>).</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a> processes the Response and returns
scraped items and new Requests (to follow) to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a>, passing through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-spider-middleware"><span class="std std-ref">Spider Middleware</span></a> (see
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>).</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> sends processed items to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-pipelines"><span class="std std-ref">Item Pipelines</span></a>, then send processed Requests to
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> and asks for possible next Requests
to crawl.</p></li>
<li><p>The process repeats (from step 3) until there are no more requests from the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a>.</p></li>
</ol>
</section>
<section id="components">
<h4>Components<a class="headerlink" href="#components" title="Permalink to this heading">¶</a></h4>
<section id="scrapy-engine">
<span id="component-engine"></span><h5>Scrapy Engine<a class="headerlink" href="#scrapy-engine" title="Permalink to this heading">¶</a></h5>
<p>The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#data-flow"><span class="std std-ref">Data Flow</span></a> section above for more details.</p>
</section>
<section id="scheduler">
<span id="component-scheduler"></span><h5>Scheduler<a class="headerlink" href="#scheduler" title="Permalink to this heading">¶</a></h5>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">scheduler</span></a> receives requests from the engine and
enqueues them for feeding them later (also to the engine) when the engine
requests them.</p>
</section>
<section id="downloader">
<span id="component-downloader"></span><h5>Downloader<a class="headerlink" href="#downloader" title="Permalink to this heading">¶</a></h5>
<p>The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.</p>
</section>
<section id="spiders">
<span id="component-spiders"></span><h5>Spiders<a class="headerlink" href="#spiders" title="Permalink to this heading">¶</a></h5>
<p>Spiders are custom classes written by Scrapy users to parse responses and
extract <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">items</span></a> from them or additional requests to
follow. For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a>.</p>
</section>
<section id="item-pipeline">
<span id="component-pipelines"></span><h5>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this heading">¶</a></h5>
<p>The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
</section>
<section id="downloader-middlewares">
<span id="component-downloader-middleware"></span><h5>Downloader middlewares<a class="headerlink" href="#downloader-middlewares" title="Permalink to this heading">¶</a></h5>
<p>Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine.</p>
<p>Use a Downloader middleware if you need to do one of the following:</p>
<ul class="simple">
<li><p>process a request just before it is sent to the Downloader
(i.e. right before Scrapy sends the request to the website);</p></li>
<li><p>change received response before passing it to a spider;</p></li>
<li><p>send a new Request instead of passing received response to a spider;</p></li>
<li><p>pass response to a spider without fetching a web page;</p></li>
<li><p>silently drop some requests.</p></li>
</ul>
<p>For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middleware</span></a>.</p>
</section>
<section id="spider-middlewares">
<span id="component-spider-middleware"></span><h5>Spider middlewares<a class="headerlink" href="#spider-middlewares" title="Permalink to this heading">¶</a></h5>
<p>Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests).</p>
<p>Use a Spider middleware if you need to</p>
<ul class="simple">
<li><p>post-process output of spider callbacks - change/add/remove requests or items;</p></li>
<li><p>post-process start_requests;</p></li>
<li><p>handle spider exceptions;</p></li>
<li><p>call errback instead of callback for some of the requests based on response
content.</p></li>
</ul>
<p>For more information see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">Spider Middleware</span></a>.</p>
</section>
</section>
<section id="event-driven-networking">
<h4>Event-driven networking<a class="headerlink" href="#event-driven-networking" title="Permalink to this heading">¶</a></h4>
<p>Scrapy is written with <a class="reference external" href="https://twisted.org/">Twisted</a>, a popular event-driven networking framework
for Python. Thus, it’s implemented using a non-blocking (aka asynchronous) code
for concurrency.</p>
<p>For more information about asynchronous programming and Twisted see these
links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v24.10)"><span>Introduction to Deferreds</span></a></p></li>
<li><p><a class="reference external" href="https://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/">Twisted Introduction - Krondo</a></p></li>
</ul>
</section>
</section>
<span id="document-topics/addons"></span><section id="add-ons">
<span id="topics-addons"></span><h3>Add-ons<a class="headerlink" href="#add-ons" title="Permalink to this heading">¶</a></h3>
<p>Scrapy’s add-on system is a framework which unifies managing and configuring
components that extend Scrapy’s core functionality, such as middlewares,
extensions, or pipelines. It provides users with a plug-and-play experience in
Scrapy extension management, and grants extensive configuration control to
developers.</p>
<section id="activating-and-configuring-add-ons">
<h4>Activating and configuring add-ons<a class="headerlink" href="#activating-and-configuring-add-ons" title="Permalink to this heading">¶</a></h4>
<p>During <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> initialization, the list of enabled
add-ons is read from your <code class="docutils literal notranslate"><span class="pre">ADDONS</span></code> setting.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ADDONS</span></code> setting is a dict in which every key is an add-on class or its
import path and the value is its priority.</p>
<p>This is an example where two add-ons are enabled in a project’s
<code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ADDONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;path.to.someaddon&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">SomeAddonClass</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="writing-your-own-add-ons">
<h4>Writing your own add-ons<a class="headerlink" href="#writing-your-own-add-ons" title="Permalink to this heading">¶</a></h4>
<p>Add-ons are Python classes that include the following method:</p>
<dl class="py method">
<dt class="sig sig-object py" id="update_settings">
<span class="sig-name descname"><span class="pre">update_settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#update_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called during the initialization of the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. Here, you should perform dependency checks
(e.g. for external Python libraries) and update the
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object as wished, e.g. enable components
for this add-on or set required configuration of other extensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>settings</strong> (<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a>) – The settings object storing Scrapy/component configuration</p>
</dd>
</dl>
</dd></dl>

<p>They can also have the following method:</p>
<dl class="py method">
<dt class="sig sig-object py">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span></dt>
<dd><p>If present, this class method is called to create an add-on instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the add-on. The crawler object provides access to all Scrapy core
components like settings and signals; it is a way for the add-on to access
them and hook its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>) – The crawler that uses this add-on</p>
</dd>
</dl>
</dd></dl>

<p>The settings set by the add-on should use the <code class="docutils literal notranslate"><span class="pre">addon</span></code> priority (see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#populating-settings"><span class="std std-ref">Populating the settings</span></a> and <a class="reference internal" href="index.html#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings.set()</span></code></a>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="n">settings</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;DNSCACHE_ENABLED&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;addon&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This allows users to override these settings in the project or spider
configuration. This is not possible with settings that are mutable objects,
such as the dict that is a value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a>. In these cases
you can provide an add-on-specific setting that governs whether the add-on will
modify <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s2">&quot;MYADDON_ENABLE_PIPELINE&quot;</span><span class="p">):</span>
            <span class="n">settings</span><span class="p">[</span><span class="s2">&quot;ITEM_PIPELINES&quot;</span><span class="p">][</span><span class="s2">&quot;path.to.mypipeline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
<p>If the <code class="docutils literal notranslate"><span class="pre">update_settings</span></code> method raises
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal notranslate"><span class="pre">scrapy.exceptions.NotConfigured</span></code></a>, the add-on will be skipped. This makes
it easy to enable an add-on only when some conditions are met.</p>
<section id="fallbacks">
<h5>Fallbacks<a class="headerlink" href="#fallbacks" title="Permalink to this heading">¶</a></h5>
<p>Some components provided by add-ons need to fall back to “default”
implementations, e.g. a custom download handler needs to send the request that
it doesn’t handle via the default download handler, or a stats collector that
includes some additional processing but otherwise uses the default stats
collector. And it’s possible that a project needs to use several custom
components of the same type, e.g. two custom download handlers that support
different kinds of custom requests and still need to use the default download
handler for other requests. To make such use cases easier to configure, we
recommend that such custom components should be written in the following way:</p>
<ol class="arabic simple">
<li><p>The custom component (e.g. <code class="docutils literal notranslate"><span class="pre">MyDownloadHandler</span></code>) shouldn’t inherit from the
default Scrapy one (e.g.
<code class="docutils literal notranslate"><span class="pre">scrapy.core.downloader.handlers.http.HTTPDownloadHandler</span></code>), but instead
be able to load the class of the fallback component from a special setting
(e.g. <code class="docutils literal notranslate"><span class="pre">MY_FALLBACK_DOWNLOAD_HANDLER</span></code>), create an instance of it and use
it.</p></li>
<li><p>The add-ons that include these components should read the current value of
the default setting (e.g. <code class="docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code>) in their
<code class="docutils literal notranslate"><span class="pre">update_settings()</span></code> methods, save that value into the fallback setting
(<code class="docutils literal notranslate"><span class="pre">MY_FALLBACK_DOWNLOAD_HANDLER</span></code> mentioned earlier) and set the default
setting to the component provided by the add-on (e.g.
<code class="docutils literal notranslate"><span class="pre">MyDownloadHandler</span></code>). If the fallback setting is already set by the user,
they shouldn’t change it.</p></li>
<li><p>This way, if there are several add-ons that want to modify the same setting,
all of them will fallback to the component from the previous one and then to
the Scrapy default. The order of that depends on the priority order in the
<code class="docutils literal notranslate"><span class="pre">ADDONS</span></code> setting.</p></li>
</ol>
</section>
</section>
<section id="add-on-examples">
<h4>Add-on examples<a class="headerlink" href="#add-on-examples" title="Permalink to this heading">¶</a></h4>
<p>Set some basic configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="n">settings</span><span class="p">[</span><span class="s2">&quot;ITEM_PIPELINES&quot;</span><span class="p">][</span><span class="s2">&quot;path.to.mypipeline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>
        <span class="n">settings</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;DNSCACHE_ENABLED&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;addon&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Check dependencies:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">boto</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">NotConfigured</span><span class="p">(</span><span class="s2">&quot;MyAddon requires the boto library&quot;</span><span class="p">)</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Access the crawler instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">crawler</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="n">crawler</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">crawler</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Use a fallback component:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.core.downloader.handlers.http</span> <span class="kn">import</span> <span class="n">HTTPDownloadHandler</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.misc</span> <span class="kn">import</span> <span class="n">build_from_crawler</span>


<span class="n">FALLBACK_SETTING</span> <span class="o">=</span> <span class="s2">&quot;MY_FALLBACK_DOWNLOAD_HANDLER&quot;</span>


<span class="k">class</span> <span class="nc">MyHandler</span><span class="p">:</span>
    <span class="n">lazy</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">dhcls</span> <span class="o">=</span> <span class="n">load_object</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FALLBACK_SETTING</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fallback_handler</span> <span class="o">=</span> <span class="n">build_from_crawler</span><span class="p">(</span><span class="n">dhcls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">download_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;my_params&quot;</span><span class="p">):</span>
            <span class="c1"># handle the request</span>
            <span class="o">...</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fallback_handler</span><span class="o">.</span><span class="n">download_request</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MyAddon</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">update_settings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">FALLBACK_SETTING</span><span class="p">):</span>
            <span class="n">settings</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
                <span class="n">FALLBACK_SETTING</span><span class="p">,</span>
                <span class="n">settings</span><span class="o">.</span><span class="n">getwithbase</span><span class="p">(</span><span class="s2">&quot;DOWNLOAD_HANDLERS&quot;</span><span class="p">)[</span><span class="s2">&quot;https&quot;</span><span class="p">],</span>
                <span class="s2">&quot;addon&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">settings</span><span class="p">[</span><span class="s2">&quot;DOWNLOAD_HANDLERS&quot;</span><span class="p">][</span><span class="s2">&quot;https&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">MyHandler</span>
</pre></div>
</div>
</section>
</section>
<span id="document-topics/downloader-middleware"></span><section id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h3>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this heading">¶</a></h3>
<p>The downloader middleware is a framework of hooks into Scrapy’s
request/response processing.  It’s a light, low-level system for globally
altering Scrapy’s requests and responses.</p>
<section id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h4>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this heading">¶</a></h4>
<p>To activate a downloader middleware component, add it to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;myproject.middlewares.CustomDownloaderMiddleware&quot;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader. In other words,
the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …) and the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project’s <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <code class="docutils literal notranslate"><span class="pre">None</span></code>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;myproject.middlewares.CustomDownloaderMiddleware&quot;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</section>
<section id="writing-your-own-downloader-middleware">
<span id="topics-downloader-middleware-custom"></span><h4>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this heading">¶</a></h4>
<p>Each downloader middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method, which receives a
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance. The <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object gives you access, for example, to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>.</p>
<span class="target" id="module-scrapy.downloadermiddlewares"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.</span></span><span class="sig-name descname"><span class="pre">DownloaderMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any of the downloader middleware methods may also return a deferred.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">
<span class="sig-name descname"><span class="pre">process_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> should either: return <code class="docutils literal notranslate"><span class="pre">None</span></code>, return a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code> object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
object, or raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a>.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, Scrapy won’t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> or <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods,
or the appropriate download function; it’ll return that response. The <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, Scrapy will stop calling
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request being processed</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">
<span class="sig-name descname"><span class="pre">process_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> should either: return a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object, return a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object or
raise a <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> of the next middleware in the chain.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the errback
function of the request (<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that originated the response</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this response is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">
<span class="sig-name descname"><span class="pre">process_exception</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception)</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> should return: either <code class="docutils literal notranslate"><span class="pre">None</span></code>,
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, or a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
method chain of installed middleware is started, and Scrapy won’t bother calling
any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of middleware.</p>
<p>If it returns a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of the middleware the same as returning a
response would.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (is a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the exception</p></li>
<li><p><strong>exception</strong> (an <code class="docutils literal notranslate"><span class="pre">Exception</span></code> object) – the raised exception</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler">
<span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a middleware instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this middleware</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h4>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this heading">¶</a></h4>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-downloader-middleware"><span class="std std-ref">downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<section id="module-scrapy.downloadermiddlewares.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h5>CookiesMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.cookies" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.cookies.</span></span><span class="sig-name descname"><span class="pre">CookiesMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
sends them back on subsequent requests (from that spider), just like web
browsers do.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>When non-UTF8 encoded byte sequences are passed to a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>, the <code class="docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code> will log
a warning. Refer to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging-advanced-customization"><span class="std std-ref">Advanced customization</span></a>
to customize the logging behaviour.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Cookies set via the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header are not considered by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code> parameter. This is a known
current limitation that is being worked on.</p>
</div>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a></p></li>
</ul>
<section id="multiple-cookie-sessions-per-spider">
<span id="std-reqmeta-cookiejar"></span><h6>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this heading">¶</a></h6>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cookiejar&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># do some processing</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span>
        <span class="s2">&quot;http://www.example.com/otherpage&quot;</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cookiejar&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s2">&quot;cookiejar&quot;</span><span class="p">]},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_other_page</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="cookies-enabled">
<span id="std-setting-COOKIES_ENABLED"></span><h6>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
<p>Notice that despite the value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a> setting if
<code class="docutils literal notranslate"><span class="pre">Request.</span></code><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">meta['dont_merge_cookies']</span></code></a>
evaluates to <code class="docutils literal notranslate"><span class="pre">True</span></code> the request cookies will <strong>not</strong> be sent to the
web server and received cookies in <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> will
<strong>not</strong> be merged with the existing cookies.</p>
<p>For more detailed information see the <code class="docutils literal notranslate"><span class="pre">cookies</span></code> parameter in
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>.</p>
</section>
<section id="cookies-debug">
<span id="std-setting-COOKIES_DEBUG"></span><h6>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (i.e. <code class="docutils literal notranslate"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (i.e. <code class="docutils literal notranslate"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here’s an example of a log with <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Sending</span> <span class="n">cookies</span> <span class="n">to</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">14</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Received</span> <span class="n">cookies</span> <span class="n">from</span><span class="p">:</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">JSESSIONID</span><span class="o">=</span><span class="n">B</span><span class="o">~</span><span class="n">FA4DC0C496C8762AE4F1A620EAB34F38</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">ip_isocode</span><span class="o">=</span><span class="n">US</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span><span class="p">;</span> <span class="n">Expires</span><span class="o">=</span><span class="n">Thu</span><span class="p">,</span> <span class="mi">07</span><span class="o">-</span><span class="n">Apr</span><span class="o">-</span><span class="mi">2011</span> <span class="mi">21</span><span class="p">:</span><span class="mi">21</span><span class="p">:</span><span class="mi">34</span> <span class="n">GMT</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">49</span><span class="p">:</span><span class="mi">50</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.defaultheaders">
<span id="defaultheadersmiddleware"></span><h5>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.defaultheaders" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.defaultheaders.</span></span><span class="sig-name descname"><span class="pre">DefaultHeadersMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h5>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.downloadtimeout" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.downloadtimeout.</span></span><span class="sig-name descname"><span class="pre">DownloadTimeoutMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_timeout</span></code>
spider attribute.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also set download timeout per-request using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a> Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.</p>
</div>
</section>
<section id="module-scrapy.downloadermiddlewares.httpauth">
<span id="httpauthmiddleware"></span><h5>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpauth" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpauth.</span></span><span class="sig-name descname"><span class="pre">HttpAuthMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication for a spider, set the <code class="docutils literal notranslate"><span class="pre">http_user</span></code> and
<code class="docutils literal notranslate"><span class="pre">http_pass</span></code> spider attributes to the authentication data and the
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute to the domain which requires this
authentication (its subdomains will be also handled in the same way).
You can set <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code> to enable the
authentication for all requests but you risk leaking your authentication
credentials to unrelated domains.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In previous Scrapy versions HttpAuthMiddleware sent the authentication
data with all requests, which is a security problem if the spider
makes requests to several different domains. Currently if the
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> attribute is not set, the middleware will use the
domain of the first request, which will work for some spiders but not
for others. In the future the middleware will produce an error instead.</p>
</div>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>


<span class="k">class</span> <span class="nc">SomeIntranetSiteSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">http_user</span> <span class="o">=</span> <span class="s2">&quot;someuser&quot;</span>
    <span class="n">http_pass</span> <span class="o">=</span> <span class="s2">&quot;somepass&quot;</span>
    <span class="n">http_auth_domain</span> <span class="o">=</span> <span class="s2">&quot;intranet.example.com&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;intranet.example.com&quot;</span>

    <span class="c1"># .. rest of the spider code omitted ...</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.httpcache">
<span id="httpcachemiddleware"></span><h5>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcache" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpcache.</span></span><span class="sig-name descname"><span class="pre">HttpCacheMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with the following HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-storage-fs"><span class="std std-ref">Filesystem storage backend (default)</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a></p></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-storage-custom"><span class="std std-ref">implement your own storage backend.</span></a></p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-policy-rfc2616"><span class="std std-ref">RFC2616 policy</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-policy-dummy"><span class="std std-ref">Dummy policy (default)</span></a></p></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
<p id="std-reqmeta-dont_cache">You can also avoid caching a response on every policy using <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_cache</span></code></a> meta key equals <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd></dl>

<section id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h6>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.DummyPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">DummyPolicy</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.DummyPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run <em>exactly as it ran before</em>.</p>
</dd></dl>

</section>
<section id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h6>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.RFC2616Policy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">RFC2616Policy</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.RFC2616Policy" title="Permalink to this definition">¶</a></dt>
<dd><p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up
crawls).</p>
<p>What is implemented:</p>
<ul class="simple">
<li><p>Do not attempt to store responses/requests with <code class="docutils literal notranslate"><span class="pre">no-store</span></code> cache-control directive set</p></li>
<li><p>Do not serve responses from cache if <code class="docutils literal notranslate"><span class="pre">no-cache</span></code> cache-control directive is set even for fresh responses</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">max-age</span></code> cache-control directive</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Expires</span></code> response header</p></li>
<li><p>Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header (heuristic used by Firefox)</p></li>
<li><p>Compute current age from <code class="docutils literal notranslate"><span class="pre">Age</span></code> response header</p></li>
<li><p>Compute current age from <code class="docutils literal notranslate"><span class="pre">Date</span></code> header</p></li>
<li><p>Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header</p></li>
<li><p>Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">ETag</span></code> response header</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">Date</span></code> header for any received response missing it</p></li>
<li><p>Support <code class="docutils literal notranslate"><span class="pre">max-stale</span></code> cache-control directive in requests</p></li>
</ul>
<p>This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.</p>
<p>Example:</p>
<p>Add <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale=600</span></code> to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.</p>
<p>See also: RFC2616, 14.9.3</p>
<p>What is missing:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Pragma:</span> <span class="pre">no-cache</span></code> support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1">https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vary</span></code> header support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></p></li>
<li><p>Invalidation after updates or deletes <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></p></li>
<li><p>… probably others ..</p></li>
</ul>
</dd></dl>

</section>
<section id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h6>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.FilesystemCacheStorage">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">FilesystemCacheStorage</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.FilesystemCacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><p>File system storage backend is available for the HTTP cache middleware.</p>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">request_body</span></code> - the plain request body</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">request_headers</span></code> - the request headers (in raw HTTP format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_body</span></code> - the plain response body</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_headers</span></code> - the request headers (in raw HTTP format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">meta</span></code> - some metadata of this cache resource in Python <code class="docutils literal notranslate"><span class="pre">repr()</span></code>
format (grep-friendly format)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pickled_meta</span></code> - the same metadata in <code class="docutils literal notranslate"><span class="pre">meta</span></code> but pickled for more
efficient deserialization</p></li>
</ul>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">cache</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="mi">72</span><span class="o">/</span><span class="mi">72811</span><span class="n">f648e718090f041317756c03adb0ada46c7</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h6>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.DbmCacheStorage">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">DbmCacheStorage</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.DbmCacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="https://docs.python.org/3/library/dbm.html#module-dbm" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dbm</span></code></a>, but you can change it with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
</dd></dl>

</section>
<section id="writing-your-own-storage-backend">
<span id="httpcache-storage-custom"></span><h6>Writing your own storage backend<a class="headerlink" href="#writing-your-own-storage-backend" title="Permalink to this heading">¶</a></h6>
<p>You can implement a cache storage backend by creating a Python class that
defines the methods described below.</p>
<span class="target" id="module-scrapy.extensions.httpcache"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.httpcache.</span></span><span class="sig-name descname"><span class="pre">CacheStorage</span></span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.open_spider">
<span class="sig-name descname"><span class="pre">open_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been opened for crawling. It handles
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">open_spider</span></code></a> signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been opened</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.close_spider">
<span class="sig-name descname"><span class="pre">close_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been closed. It handles
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">close_spider</span></code></a> signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been closed</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.retrieve_response">
<span class="sig-name descname"><span class="pre">retrieve_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.retrieve_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Return response if present in cache, or <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which generated the request</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request to find cached response for</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.extensions.httpcache.CacheStorage.store_response">
<span class="sig-name descname"><span class="pre">store_response</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.store_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the given response in the cache.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the corresponding request the spider generated</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response to store in the cache</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<p>In order to use your storage backend, set:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to the Python import path of your custom storage class.</p></li>
</ul>
</section>
<section id="httpcache-middleware-settings">
<h6>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this heading">¶</a></h6>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code> can be configured through the following
settings:</p>
<section id="httpcache-enabled">
<span id="std-setting-HTTPCACHE_ENABLED"></span><h6 aria-level="7">HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
</section>
<section id="httpcache-expiration-secs">
<span id="std-setting-HTTPCACHE_EXPIRATION_SECS"></span><h6 aria-level="7">HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
</section>
<section id="httpcache-dir">
<span id="std-setting-HTTPCACHE_DIR"></span><h6 aria-level="7">HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-project-structure"><span class="std std-ref">Default structure of Scrapy projects</span></a>.</p>
</section>
<section id="httpcache-ignore-http-codes">
<span id="std-setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h6 aria-level="7">HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Don’t cache response with these HTTP codes.</p>
</section>
<section id="httpcache-ignore-missing">
<span id="std-setting-HTTPCACHE_IGNORE_MISSING"></span><h6 aria-level="7">HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</section>
<section id="httpcache-ignore-schemes">
<span id="std-setting-HTTPCACHE_IGNORE_SCHEMES"></span><h6 aria-level="7">HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">['file']</span></code></p>
<p>Don’t cache responses with these URI schemes.</p>
</section>
<section id="httpcache-storage">
<span id="std-setting-HTTPCACHE_STORAGE"></span><h6 aria-level="7">HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
</section>
<section id="httpcache-dbm-module">
<span id="std-setting-HTTPCACHE_DBM_MODULE"></span><h6 aria-level="7">HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'dbm'</span></code></p>
<p>The database module to use in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
</section>
<section id="httpcache-policy">
<span id="std-setting-HTTPCACHE_POLICY"></span><h6 aria-level="7">HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
</section>
<section id="httpcache-gzip">
<span id="std-setting-HTTPCACHE_GZIP"></span><h6 aria-level="7">HTTPCACHE_GZIP<a class="headerlink" href="#httpcache-gzip" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.</p>
</section>
<section id="httpcache-always-store">
<span id="std-setting-HTTPCACHE_ALWAYS_STORE"></span><h6 aria-level="7">HTTPCACHE_ALWAYS_STORE<a class="headerlink" href="#httpcache-always-store" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale</span></code>, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">no-store</span></code> directives in responses.
If you don’t want that, filter <code class="docutils literal notranslate"><span class="pre">no-store</span></code> out of the Cache-Control headers in
responses you feed to the cache middleware.</p>
</section>
<section id="httpcache-ignore-response-cache-controls">
<span id="std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"></span><h6 aria-level="7">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS<a class="headerlink" href="#httpcache-ignore-response-cache-controls" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get
upset at the traffic a spider can generate if it actually respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.httpcompression">
<span id="httpcompressionmiddleware"></span><h5>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcompression" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpcompression.</span></span><span class="sig-name descname"><span class="pre">HttpCompressionMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
<p>This middleware also supports decoding <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a> as well as
<a class="reference external" href="https://www.ietf.org/rfc/rfc8478.txt">zstd-compressed</a> responses, provided that <a class="reference external" href="https://pypi.org/project/Brotli/">brotli</a> or <a class="reference external" href="https://pypi.org/project/zstandard/">zstandard</a> is
installed, respectively.</p>
</dd></dl>

<section id="httpcompressionmiddleware-settings">
<h6>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="compression-enabled">
<span id="std-setting-COMPRESSION_ENABLED"></span><h6 aria-level="7">COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.httpproxy">
<span id="httpproxymiddleware"></span><h5>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpproxy" title="Permalink to this heading">¶</a></h5>
<span class="target" id="std-reqmeta-proxy"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.httpproxy.</span></span><span class="sig-name descname"><span class="pre">HttpProxyMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal notranslate"><span class="pre">proxy</span></code> meta value for <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects.</p>
<p>Like the Python standard library module <a class="reference external" href="https://docs.python.org/3/library/urllib.request.html#module-urllib.request" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">urllib.request</span></code></a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">http_proxy</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">https_proxy</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">no_proxy</span></code></p></li>
</ul>
<p>You can also set the meta key <code class="docutils literal notranslate"><span class="pre">proxy</span></code> per-request, to a value like
<code class="docutils literal notranslate"><span class="pre">http://some_proxy_server:port</span></code> or <code class="docutils literal notranslate"><span class="pre">http://username:password&#64;some_proxy_server:port</span></code>.
Keep in mind this value will take precedence over <code class="docutils literal notranslate"><span class="pre">http_proxy</span></code>/<code class="docutils literal notranslate"><span class="pre">https_proxy</span></code>
environment variables, and it will also ignore <code class="docutils literal notranslate"><span class="pre">no_proxy</span></code> environment variable.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.offsite">
<span id="offsitemiddleware"></span><h5>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.offsite" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.offsite.OffsiteMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.offsite.</span></span><span class="sig-name descname"><span class="pre">OffsiteMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.11.2.</span></p>
</div>
<p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren’t in the
spider’s <a class="reference internal" href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule <code class="docutils literal notranslate"><span class="pre">www.example.org</span></code> will also allow <code class="docutils literal notranslate"><span class="pre">bob.www.example.org</span></code>
but not <code class="docutils literal notranslate"><span class="pre">www2.example.com</span></code> nor <code class="docutils literal notranslate"><span class="pre">example.com</span></code>.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEBUG</span><span class="p">:</span> <span class="n">Filtered</span> <span class="n">offsite</span> <span class="n">request</span> <span class="n">to</span> <span class="s1">&#39;offsite.example&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">offsite</span><span class="o">.</span><span class="n">example</span><span class="o">/</span><span class="n">some</span><span class="o">/</span><span class="n">page</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <code class="docutils literal notranslate"><span class="pre">offsite.example</span></code> is filtered, no log message will be
printed. But if a request for <code class="docutils literal notranslate"><span class="pre">other.example</span></code> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn’t define an
<a class="reference internal" href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <code class="xref py py-attr docutils literal notranslate"><span class="pre">dont_filter</span></code> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.redirect">
<span id="redirectmiddleware"></span><h5>RedirectMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.redirect" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.redirect.</span></span><span class="sig-name descname"><span class="pre">RedirectMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std-reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal notranslate"><span class="pre">redirect_urls</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> key.</p>
<p id="std-reqmeta-redirect_reasons">The reason behind each redirect in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> can be found in the
<code class="docutils literal notranslate"><span class="pre">redirect_reasons</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> key. For
example: <code class="docutils literal notranslate"><span class="pre">[301,</span> <span class="pre">302,</span> <span class="pre">307,</span> <span class="pre">'meta</span> <span class="pre">refresh']</span></code>.</p>
<p>The format of a reason depends on the middleware that handled the corresponding
redirect. For example, <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> indicates the triggering
response status code as an integer, while <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a>
always uses the <code class="docutils literal notranslate"><span class="pre">'meta</span> <span class="pre">refresh'</span></code> string as reason.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a></p></li>
</ul>
<p id="std-reqmeta-dont_redirect">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has <code class="docutils literal notranslate"><span class="pre">dont_redirect</span></code>
key set to True, the request will be ignored by this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">handle_httpstatus_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">301</span><span class="p">,</span> <span class="mi">302</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> key of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> if you want to allow any response code
for a request.</p>
<section id="redirectmiddleware-settings">
<h6>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="redirect-enabled">
<span id="std-setting-REDIRECT_ENABLED"></span><h6 aria-level="7">REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
</section>
<section id="redirect-max-times">
<span id="std-setting-REDIRECT_MAX_TIMES"></span><h6 aria-level="7">REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be followed for a single request.
If maximum redirections are exceeded, the request is aborted and ignored.</p>
</section>
</section>
</section>
<section id="metarefreshmiddleware">
<h5>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.redirect.</span></span><span class="sig-name descname"><span class="pre">MetaRefreshMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-METAREFRESH_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-METAREFRESH_MAXDELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_MAXDELAY</span></code></a></p></li>
</ul>
<p>This middleware obey <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_redirect</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a> request meta keys as described
for <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a></p>
<section id="metarefreshmiddleware-settings">
<h6>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="metarefresh-enabled">
<span id="std-setting-METAREFRESH_ENABLED"></span><h6 aria-level="7">METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</section>
<section id="metarefresh-ignore-tags">
<span id="std-setting-METAREFRESH_IGNORE_TAGS"></span><h6 aria-level="7">METAREFRESH_IGNORE_TAGS<a class="headerlink" href="#metarefresh-ignore-tags" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Meta tags within these tags are ignored.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.0: </span>The default value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> changed from
<code class="docutils literal notranslate"><span class="pre">[&quot;script&quot;,</span> <span class="pre">&quot;noscript&quot;]</span></code> to <code class="docutils literal notranslate"><span class="pre">[]</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.11.2: </span>The default value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> changed from
<code class="docutils literal notranslate"><span class="pre">[]</span></code> to <code class="docutils literal notranslate"><span class="pre">[&quot;noscript&quot;]</span></code>.</p>
</div>
</section>
<section id="metarefresh-maxdelay">
<span id="std-setting-METAREFRESH_MAXDELAY"></span><h6 aria-level="7">METAREFRESH_MAXDELAY<a class="headerlink" href="#metarefresh-maxdelay" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.</p>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.retry">
<span id="retrymiddleware"></span><h5>RetryMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.retry" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.retry.RetryMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.retry.</span></span><span class="sig-name descname"><span class="pre">RetryMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_EXCEPTIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_EXCEPTIONS</span></code></a></p></li>
</ul>
<p id="std-reqmeta-dont_retry">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> key
set to True, the request will be ignored by this middleware.</p>
<p>To retry requests from a spider callback, you can use the
<a class="reference internal" href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_retry_request()</span></code></a> function:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.retry.get_retry_request">
<span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.retry.</span></span><span class="sig-name descname"><span class="pre">get_retry_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request:</span> <span class="pre">Request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider:</span> <span class="pre">Spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reason:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">Exception</span> <span class="pre">|</span> <span class="pre">type[Exception]</span> <span class="pre">=</span> <span class="pre">'unspecified'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_retry_times:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority_adjust:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger:</span> <span class="pre">Logger</span> <span class="pre">=</span> <span class="pre">&lt;Logger</span> <span class="pre">scrapy.downloadermiddlewares.retry</span> <span class="pre">(WARNING)&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stats_base_key:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'retry'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object to retry the specified
request, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if retries of the specified request have been
exhausted.</p>
<p>For example, in a <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> callback, you could use it as
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
        <span class="n">new_request_or_none</span> <span class="o">=</span> <span class="n">get_retry_request</span><span class="p">(</span>
            <span class="n">response</span><span class="o">.</span><span class="n">request</span><span class="p">,</span>
            <span class="n">spider</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">reason</span><span class="o">=</span><span class="s1">&#39;empty&#39;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">new_request_or_none</span>
</pre></div>
</div>
<p><em>spider</em> is the <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> instance which is asking for the
retry request. It is used to access the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>
and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">stats</span></a>, and to provide extra logging context (see
<a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.debug" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">logging.debug()</span></code></a>).</p>
<p><em>reason</em> is a string or an <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Exception</span></code></a> object that indicates the
reason why the request needs to be retried. It is used to name retry stats.</p>
<p><em>max_retry_times</em> is a number that determines the maximum number of times
that <em>request</em> can be retried. If not specified or <code class="docutils literal notranslate"><span class="pre">None</span></code>, the number is
read from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key of the request. If the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key is not defined or <code class="docutils literal notranslate"><span class="pre">None</span></code>, the number
is read from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
<p><em>priority_adjust</em> is a number that determines how the priority of the new
request changes in relation to <em>request</em>. If not specified, the number is
read from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a> setting.</p>
<p><em>logger</em> is the logging.Logger object to be used when logging messages</p>
<p><em>stats_base_key</em> is a string to be used as the base key for the
retry-related job stats</p>
</dd></dl>

<section id="retrymiddleware-settings">
<h6>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="retry-enabled">
<span id="std-setting-RETRY_ENABLED"></span><h6 aria-level="7">RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
</section>
<section id="retry-times">
<span id="std-setting-RETRY_TIMES"></span><h6 aria-level="7">RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
<p>Maximum number of retries can also be specified per-request using
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> attribute of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code>.
When initialized, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key takes higher
precedence over the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
</section>
<section id="retry-http-codes">
<span id="std-setting-RETRY_HTTP_CODES"></span><h6 aria-level="7">RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">522,</span> <span class="pre">524,</span> <span class="pre">408,</span> <span class="pre">429]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.</p>
</section>
<section id="retry-exceptions">
<span id="std-setting-RETRY_EXCEPTIONS"></span><h6 aria-level="7">RETRY_EXCEPTIONS<a class="headerlink" href="#retry-exceptions" title="Permalink to this heading">¶</a></h6>
<p>Default:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="s1">&#39;twisted.internet.defer.TimeoutError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.TimeoutError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.DNSLookupError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.ConnectionRefusedError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.ConnectionDone&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.ConnectError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.ConnectionLost&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.internet.error.TCPTimedOutError&#39;</span><span class="p">,</span>
    <span class="s1">&#39;twisted.web.client.ResponseFailed&#39;</span><span class="p">,</span>
    <span class="ne">IOError</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.core.downloader.handlers.http11.TunnelError&#39;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>List of exceptions to retry.</p>
<p>Each list entry may be an exception type or its import path as a string.</p>
<p>An exception will not be caught when the exception type is not in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_EXCEPTIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_EXCEPTIONS</span></code></a> or when the maximum number of retries for a request
has been exceeded (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a>). To learn about uncaught
exception propagation, see
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a>.</p>
</section>
<section id="retry-priority-adjust">
<span id="std-setting-RETRY_PRIORITY_ADJUST"></span><h6 aria-level="7">RETRY_PRIORITY_ADJUST<a class="headerlink" href="#retry-priority-adjust" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<ul class="simple">
<li><p>a positive priority adjust means higher priority.</p></li>
<li><p><strong>a negative priority adjust (default) means lower priority.</strong></p></li>
</ul>
</section>
</section>
</section>
<section id="module-scrapy.downloadermiddlewares.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h5>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.robotstxt" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.robotstxt.</span></span><span class="sig-name descname"><span class="pre">RobotsTxtMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_USER_AGENT</span></code></a> setting can be used to specify the
user agent string to use for matching in the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file. If it
is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the User-Agent header you are sending with the request or the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> setting (in that order) will be used for determining
the user agent to use in the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file.</p>
<p>This middleware has to be combined with a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser.</p>
<p>Scrapy ships with support for the following <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parsers:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#protego-parser"><span class="std std-ref">Protego</span></a> (default)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#python-robotfileparser"><span class="std std-ref">RobotFileParser</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#rerp-parser"><span class="std std-ref">Robotexclusionrulesparser</span></a></p></li>
</ul>
<p>You can change the <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a>
setting. Or you can also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#support-for-new-robots-parser"><span class="std std-ref">implement support for a new parser</span></a>.</p>
</dd></dl>

<p id="std-reqmeta-dont_obey_robotstxt">If <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> has
<code class="docutils literal notranslate"><span class="pre">dont_obey_robotstxt</span></code> key set to True
the request will be ignored by this middleware even if
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>
<p>Parsers vary in several aspects:</p>
<ul class="simple">
<li><p>Language of implementation</p></li>
<li><p>Supported specification</p></li>
<li><p>Support for wildcard matching</p></li>
<li><p>Usage of <a class="reference external" href="https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#order-of-precedence-for-rules">length based rule</a>:
in particular for <code class="docutils literal notranslate"><span class="pre">Allow</span></code> and <code class="docutils literal notranslate"><span class="pre">Disallow</span></code> directives, where the most
specific rule based on the length of the path trumps the less specific
(shorter) rule</p></li>
</ul>
<p>Performance comparison of different parsers is available at <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3969">the following link</a>.</p>
<section id="protego-parser">
<span id="id1"></span><h6>Protego parser<a class="headerlink" href="#protego-parser" title="Permalink to this heading">¶</a></h6>
<p>Based on <a class="reference external" href="https://github.com/scrapy/protego">Protego</a>:</p>
<ul class="simple">
<li><p>implemented in Python</p></li>
<li><p>is compliant with <a class="reference external" href="https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt">Google’s Robots.txt Specification</a></p></li>
<li><p>supports wildcard matching</p></li>
<li><p>uses the length based rule</p></li>
</ul>
<p>Scrapy uses this parser by default.</p>
</section>
<section id="robotfileparser">
<span id="python-robotfileparser"></span><h6>RobotFileParser<a class="headerlink" href="#robotfileparser" title="Permalink to this heading">¶</a></h6>
<p>Based on <a class="reference external" href="https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotFileParser</span></code></a>:</p>
<ul class="simple">
<li><p>is Python’s built-in <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser</p></li>
<li><p>is compliant with <a class="reference external" href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster’s 1996 draft specification</a></p></li>
<li><p>lacks support for wildcard matching</p></li>
<li><p>doesn’t use the length based rule</p></li>
</ul>
<p>It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0.</p>
<p>In order to use this parser, set:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.robotstxt.PythonRobotParser</span></code></p></li>
</ul>
</section>
<section id="robotexclusionrulesparser">
<span id="rerp-parser"></span><h6>Robotexclusionrulesparser<a class="headerlink" href="#robotexclusionrulesparser" title="Permalink to this heading">¶</a></h6>
<p>Based on <a class="reference external" href="https://pypi.org/project/robotexclusionrulesparser/">Robotexclusionrulesparser</a>:</p>
<ul class="simple">
<li><p>implemented in Python</p></li>
<li><p>is compliant with <a class="reference external" href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster’s 1996 draft specification</a></p></li>
<li><p>supports wildcard matching</p></li>
<li><p>doesn’t use the length based rule</p></li>
</ul>
<p>In order to use this parser:</p>
<ul class="simple">
<li><p>Install <code class="docutils literal notranslate"><span class="pre">Robotexclusionrulesparser</span></code> by running
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">robotexclusionrulesparser</span></code></p></li>
<li><p>Set <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> setting to
<code class="docutils literal notranslate"><span class="pre">scrapy.robotstxt.RerpRobotParser</span></code></p></li>
</ul>
</section>
<section id="implementing-support-for-a-new-parser">
<span id="support-for-new-robots-parser"></span><h6>Implementing support for a new parser<a class="headerlink" href="#implementing-support-for-a-new-parser" title="Permalink to this heading">¶</a></h6>
<p>You can implement support for a new <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parser by subclassing
the abstract base class <a class="reference internal" href="#scrapy.robotstxt.RobotParser" title="scrapy.robotstxt.RobotParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotParser</span></code></a> and
implementing the methods described below.</p>
<span class="target" id="module-scrapy.robotstxt"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.robotstxt.</span></span><span class="sig-name descname"><span class="pre">RobotParser</span></span><a class="headerlink" href="#scrapy.robotstxt.RobotParser" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser.allowed">
<em class="property"><span class="pre">abstract</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">allowed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">url</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_agent</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.robotstxt.RobotParser.allowed" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if  <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> is allowed to crawl <code class="docutils literal notranslate"><span class="pre">url</span></code>, otherwise return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – Absolute URL</p></li>
<li><p><strong>user_agent</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – User agent</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.robotstxt.RobotParser.from_crawler">
<em class="property"><span class="pre">abstract</span><span class="w">  </span><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">robotstxt_body</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><span class="pre">bytes</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.robotstxt.RobotParser.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse the content of a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file as bytes. This must be a class method.
It must return a new instance of the parser backend.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance) – crawler which made the request</p></li>
<li><p><strong>robotstxt_body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><em>bytes</em></a>) – content of a <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="module-scrapy.downloadermiddlewares.stats">
<span id="downloaderstats"></span><h5>DownloaderStats<a class="headerlink" href="#module-scrapy.downloadermiddlewares.stats" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.stats.DownloaderStats">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.stats.</span></span><span class="sig-name descname"><span class="pre">DownloaderStats</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.useragent">
<span id="useragentmiddleware"></span><h5>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.useragent" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.useragent.</span></span><span class="sig-name descname"><span class="pre">UserAgentMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <code class="docutils literal notranslate"><span class="pre">user_agent</span></code>
attribute must be set.</p>
</dd></dl>

</section>
<section id="module-scrapy.downloadermiddlewares.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h5>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.ajaxcrawl" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.downloadermiddlewares.ajaxcrawl.</span></span><span class="sig-name descname"><span class="pre">AjaxCrawlMiddleware</span></span><a class="headerlink" href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds ‘AJAX crawlable’ page variants based
on meta-fragment html tag.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scrapy finds ‘AJAX crawlable’ pages for URLs like
<code class="docutils literal notranslate"><span class="pre">'http://example.com/!#foo=bar'</span></code> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn’t contain <code class="docutils literal notranslate"><span class="pre">'!#'</span></code>.
This is often a case for ‘index’ or ‘main’ website pages.</p>
</div>
</dd></dl>

<section id="ajaxcrawlmiddleware-settings">
<h6>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="ajaxcrawl-enabled">
<span id="std-setting-AJAXCRAWL_ENABLED"></span><h6 aria-level="7">AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-broad-crawls"><span class="std std-ref">broad crawls</span></a>.</p>
</section>
</section>
<section id="httpproxymiddleware-settings">
<h6>HttpProxyMiddleware settings<a class="headerlink" href="#httpproxymiddleware-settings" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-setting-HTTPPROXY_ENABLED"></span><section id="httpproxy-enabled">
<span id="std-setting-HTTPPROXY_AUTH_ENCODING"></span><h6 aria-level="7">HTTPPROXY_ENABLED<a class="headerlink" href="#httpproxy-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to enable the <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</section>
<section id="httpproxy-auth-encoding">
<h6 aria-level="7">HTTPPROXY_AUTH_ENCODING<a class="headerlink" href="#httpproxy-auth-encoding" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;latin-1&quot;</span></code></p>
<p>The default encoding for proxy authentication on <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</section>
</section>
</section>
</section>
</section>
<span id="document-topics/spider-middleware"></span><section id="spider-middleware">
<span id="topics-spider-middleware"></span><h3>Spider Middleware<a class="headerlink" href="#spider-middleware" title="Permalink to this heading">¶</a></h3>
<p>The spider middleware is a framework of hooks into Scrapy’s spider processing
mechanism where you can plug custom functionality to process the responses that
are sent to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a> for processing and to process the requests
and items that are generated from spiders.</p>
<section id="activating-a-spider-middleware">
<span id="topics-spider-middleware-setting"></span><h4>Activating a spider middleware<a class="headerlink" href="#activating-a-spider-middleware" title="Permalink to this heading">¶</a></h4>
<p>To activate a spider middleware component, add it to the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;myproject.middlewares.CustomSpiderMiddleware&quot;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider. In other words,
the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …), and the
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a>, and enabled by default) you must define it
in your project <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting and assign <code class="docutils literal notranslate"><span class="pre">None</span></code> as its
value.  For example, if you want to disable the off-site middleware:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scrapy.spidermiddlewares.referer.RefererMiddleware&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;myproject.middlewares.CustomRefererSpiderMiddleware&quot;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</section>
<section id="writing-your-own-spider-middleware">
<span id="custom-spider-middleware"></span><h4>Writing your own spider middleware<a class="headerlink" href="#writing-your-own-spider-middleware" title="Permalink to this heading">¶</a></h4>
<p>Each spider middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method, which receives a
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance. The <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object gives you access, for example, to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>.</p>
<span class="target" id="module-scrapy.spidermiddlewares"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.</span></span><span class="sig-name descname"><span class="pre">SpiderMiddleware</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input">
<span class="sig-name descname"><span class="pre">process_spider_input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each response that goes through the spider
middleware and into the spider, for processing.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a> should return <code class="docutils literal notranslate"><span class="pre">None</span></code> or raise an
exception.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.</p>
<p>If it raises an exception, Scrapy won’t bother calling any other spider
middleware <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a> and will call the request
errback if there is one, otherwise it will start the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a>
chain. The output of the errback is chained back in the other
direction for <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> to process it, or
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> if it raised an exception.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this response is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">
<span class="sig-name descname"><span class="pre">process_spider_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called with the results returned from the Spider, after
it has processed the response.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> must return an iterable of
<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 2.7: </span>This method may be defined as an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a>, in
which case <code class="docutils literal notranslate"><span class="pre">result</span></code> is an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-iterable" title="(in Python v3.13)"><span class="xref std std-term">asynchronous iterable</span></a>.</p>
</div>
<p>Consider defining this method as an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a>,
which will be a requirement in a future version of Scrapy. However, if
you plan on sharing your spider middleware with other people, consider
either <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#enforce-component-requirements"><span class="std std-ref">enforcing Scrapy 2.7</span></a>
as a minimum requirement of your spider middleware, or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#universal-spider-middleware"><span class="std std-ref">making
your spider middleware universal</span></a> so that
it works with Scrapy versions earlier than Scrapy 2.7.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response which generated this output from the
spider</p></li>
<li><p><strong>result</strong> (an iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a>) – the result returned by the spider</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider whose result is being processed</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output_async">
<span class="sig-name descname"><span class="pre">process_spider_output_async</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">result</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output_async" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.7.</span></p>
</div>
<p>If defined, this method must be an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a>,
which will be called instead of <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> if
<code class="docutils literal notranslate"><span class="pre">result</span></code> is an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-iterable" title="(in Python v3.13)"><span class="xref std std-term">asynchronous iterable</span></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception">
<span class="sig-name descname"><span class="pre">process_spider_exception</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when a spider or <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method (from a previous spider middleware) raises an exception.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> should return either <code class="docutils literal notranslate"><span class="pre">None</span></code> or an
iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a>
objects.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it’s logged and discarded).</p>
<p>If it returns an iterable the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> pipeline
kicks in, starting from the next spider middleware, and no other
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> will be called.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed when the exception was
raised</p></li>
<li><p><strong>exception</strong> (<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">Exception</span></code></a> object) – the exception raised</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests">
<span class="sig-name descname"><span class="pre">process_start_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_requests</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called with the start requests of the spider, and works
similarly to the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> method, except that it
doesn’t have a response associated and must return only requests (not
items).</p>
<p>It receives an iterable (in the <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> parameter) and must
return another iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects and/or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item objects</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_requests</strong> (an iterable of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>) – the start requests</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider to whom the start requests belong</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.SpiderMiddleware.from_crawler">
<span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a middleware instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this middleware</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="built-in-spider-middleware-reference">
<span id="topics-spider-middleware-ref"></span><h4>Built-in spider middleware reference<a class="headerlink" href="#built-in-spider-middleware-reference" title="Permalink to this heading">¶</a></h4>
<p>This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-spider-middleware"><span class="std std-ref">spider middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<section id="module-scrapy.spidermiddlewares.depth">
<span id="depthmiddleware"></span><h5>DepthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.depth" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.depth.DepthMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.depth.</span></span><span class="sig-name descname"><span class="pre">DepthMiddleware</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>DepthMiddleware is used for tracking the depth of each Request inside the
site being scraped. It works by setting <code class="docutils literal notranslate"><span class="pre">request.meta['depth']</span> <span class="pre">=</span> <span class="pre">0</span></code> whenever
there is no value previously set (usually just the first Request) and
incrementing it by 1 otherwise.</p>
<p>It can be used to limit the maximum depth to scrape, control Request
priority based on their depth, and things like that.</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="scrapy.spidermiddlewares.depth.DepthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">DepthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DEPTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_LIMIT</span></code></a> - The maximum depth that will be allowed to
crawl for any site. If zero, no limit will be imposed.</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DEPTH_STATS_VERBOSE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_STATS_VERBOSE</span></code></a> - Whether to collect the number of
requests for each depth.</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DEPTH_PRIORITY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_PRIORITY</span></code></a> - Whether to prioritize the requests based on
their depth.</p></li>
</ul>
</div></blockquote>
</dd></dl>

</section>
<section id="module-scrapy.spidermiddlewares.httperror">
<span id="httperrormiddleware"></span><h5>HttpErrorMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.httperror" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.httperror.</span></span><span class="sig-name descname"><span class="pre">HttpErrorMiddleware</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.</p>
</dd></dl>

<p>According to the <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP standard</a>, successful responses are those whose
status codes are in the 200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> spider attribute or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-HTTPERROR_ALLOWED_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPERROR_ALLOWED_CODES</span></code></a> setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="kn">import</span> <span class="n">CrawlSpider</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">handle_httpstatus_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">404</span><span class="p">]</span>
</pre></div>
</div>
<span class="target" id="std-reqmeta-handle_httpstatus_list"></span><p id="std-reqmeta-handle_httpstatus_all">The <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> key of <code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code>
to <code class="docutils literal notranslate"><span class="pre">True</span></code> if you want to allow any response code for a request, and <code class="docutils literal notranslate"><span class="pre">False</span></code> to
disable the effects of the <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code> key.</p>
<p>Keep in mind, however, that it’s usually a bad idea to handle non-200
responses, unless you really know what you’re doing.</p>
<p>For more information see: <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code Definitions</a>.</p>
<section id="httperrormiddleware-settings">
<h6>HttpErrorMiddleware settings<a class="headerlink" href="#httperrormiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="httperror-allowed-codes">
<span id="std-setting-HTTPERROR_ALLOWED_CODES"></span><h6 aria-level="7">HTTPERROR_ALLOWED_CODES<a class="headerlink" href="#httperror-allowed-codes" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Pass all responses with non-200 status codes contained in this list.</p>
</section>
<section id="httperror-allow-all">
<span id="std-setting-HTTPERROR_ALLOW_ALL"></span><h6 aria-level="7">HTTPERROR_ALLOW_ALL<a class="headerlink" href="#httperror-allow-all" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Pass all responses, regardless of its status code.</p>
</section>
</section>
</section>
<section id="module-scrapy.spidermiddlewares.referer">
<span id="referermiddleware"></span><h5>RefererMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.referer" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.RefererMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">RefererMiddleware</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.RefererMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates Request <code class="docutils literal notranslate"><span class="pre">Referer</span></code> header, based on the URL of the Response which
generated it.</p>
</dd></dl>

<section id="referermiddleware-settings">
<h6>RefererMiddleware settings<a class="headerlink" href="#referermiddleware-settings" title="Permalink to this heading">¶</a></h6>
<section id="referer-enabled">
<span id="std-setting-REFERER_ENABLED"></span><h6 aria-level="7">REFERER_ENABLED<a class="headerlink" href="#referer-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable referer middleware.</p>
</section>
<section id="referrer-policy">
<span id="std-setting-REFERRER_POLICY"></span><h6 aria-level="7">REFERRER_POLICY<a class="headerlink" href="#referrer-policy" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'</span></code></p>
<p id="std-reqmeta-referrer_policy"><a class="reference external" href="https://www.w3.org/TR/referrer-policy">Referrer Policy</a> to apply when populating Request “Referer” header.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also set the Referrer Policy per request,
using the special <code class="docutils literal notranslate"><span class="pre">&quot;referrer_policy&quot;</span></code> <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-meta"><span class="std std-ref">Request.meta</span></a> key,
with the same acceptable values as for the <code class="docutils literal notranslate"><span class="pre">REFERRER_POLICY</span></code> setting.</p>
</div>
<section id="acceptable-values-for-referrer-policy">
<h6 aria-level="8">Acceptable values for REFERRER_POLICY<a class="headerlink" href="#acceptable-values-for-referrer-policy" title="Permalink to this heading">¶</a></h6>
<ul class="simple">
<li><p>either a path to a <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.ReferrerPolicy</span></code>
subclass — a custom policy or one of the built-in ones (see classes below),</p></li>
<li><p>or one or more comma-separated standard W3C-defined string values,</p></li>
<li><p>or the special <code class="docutils literal notranslate"><span class="pre">&quot;scrapy-default&quot;</span></code>.</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>String value</p></th>
<th class="head"><p>Class name (as a string)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">&quot;scrapy-default&quot;</span></code> (default)</p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.DefaultReferrerPolicy</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">“no-referrer”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.NoReferrerPolicy" title="scrapy.spidermiddlewares.referer.NoReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.NoReferrerPolicy</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">“no-referrer-when-downgrade”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy" title="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">“same-origin”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.SameOriginPolicy" title="scrapy.spidermiddlewares.referer.SameOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.SameOriginPolicy</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">“origin”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.OriginPolicy" title="scrapy.spidermiddlewares.referer.OriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.OriginPolicy</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">“strict-origin”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.StrictOriginPolicy" title="scrapy.spidermiddlewares.referer.StrictOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.StrictOriginPolicy</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">“origin-when-cross-origin”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy" title="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">“strict-origin-when-cross-origin”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy" title="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">“unsafe-url”</a></p></td>
<td><p><a class="reference internal" href="#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy" title="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.UnsafeUrlPolicy</span></code></a></p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">DefaultReferrerPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>A variant of “no-referrer-when-downgrade”,
with the addition that “Referer” is not sent if the parent request was
using <code class="docutils literal notranslate"><span class="pre">file://</span></code> or <code class="docutils literal notranslate"><span class="pre">s3://</span></code> scheme.</p>
</dd></dl>

<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Scrapy’s default referrer policy — just like <a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">“no-referrer-when-downgrade”</a>,
the W3C-recommended value for browsers — will send a non-empty
“Referer” header from any <code class="docutils literal notranslate"><span class="pre">http(s)://</span></code> to any <code class="docutils literal notranslate"><span class="pre">https://</span></code> URL,
even if the domain is different.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">“same-origin”</a> may be a better choice if you want to remove referrer
information for cross-domain requests.</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.NoReferrerPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">NoReferrerPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.NoReferrerPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer</a></p>
<p>The simplest policy is “no-referrer”, which specifies that no referrer information
is to be sent along with requests made from a particular request client to any origin.
The header will be omitted entirely.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">NoReferrerWhenDowngradePolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade</a></p>
<p>The “no-referrer-when-downgrade” policy sends a full URL along with requests
from a TLS-protected environment settings object to a potentially trustworthy URL,
and requests from clients which are not TLS-protected to any origin.</p>
<p>Requests from TLS-protected clients to non-potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p>This is a user agent’s default behavior, if no policy is otherwise specified.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“no-referrer-when-downgrade” policy is the W3C-recommended default,
and is used by major web browsers.</p>
<p>However, it is NOT Scrapy’s default referrer policy (see <a class="reference internal" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultReferrerPolicy</span></code></a>).</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.SameOriginPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">SameOriginPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.SameOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin</a></p>
<p>The “same-origin” policy specifies that a full URL, stripped for use as a referrer,
is sent as referrer information when making same-origin requests from a particular request client.</p>
<p>Cross-origin requests, on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.OriginPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">OriginPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.OriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin</a></p>
<p>The “origin” policy specifies that only the ASCII serialization
of the origin of the request client is sent as referrer information
when making both same-origin requests and cross-origin requests
from a particular request client.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.StrictOriginPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">StrictOriginPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.StrictOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin</a></p>
<p>The “strict-origin” policy sends the ASCII serialization
of the origin of the request client when making requests:
- from a TLS-protected environment settings object to a potentially trustworthy URL, and
- from non-TLS-protected environment settings objects to any origin.</p>
<p>Requests from TLS-protected request clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">OriginWhenCrossOriginPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin</a></p>
<p>The “origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
is sent as referrer information when making cross-origin requests
from a particular request client.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">StrictOriginWhenCrossOriginPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin</a></p>
<p>The “strict-origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
when making cross-origin requests:</p>
<ul class="simple">
<li><p>from a TLS-protected environment settings object to a potentially trustworthy URL, and</p></li>
<li><p>from non-TLS-protected environment settings objects to any origin.</p></li>
</ul>
<p>Requests from TLS-protected clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.referer.</span></span><span class="sig-name descname"><span class="pre">UnsafeUrlPolicy</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url</a></p>
<p>The “unsafe-url” policy specifies that a full URL, stripped for use as a referrer,
is sent along with both cross-origin requests
and same-origin requests made from a particular request client.</p>
<p>Note: The policy’s name doesn’t lie; it is unsafe.
This policy will leak origins and paths from TLS-protected resources
to insecure origins.
Carefully consider the impact of setting such a policy for potentially sensitive documents.</p>
</dd></dl>

<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>“unsafe-url” policy is NOT recommended.</p>
</div>
</section>
</section>
</section>
</section>
<section id="module-scrapy.spidermiddlewares.urllength">
<span id="urllengthmiddleware"></span><h5>UrlLengthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.urllength" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spidermiddlewares.urllength.</span></span><span class="sig-name descname"><span class="pre">UrlLengthMiddleware</span></span><a class="headerlink" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UrlLengthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-URLLENGTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">URLLENGTH_LIMIT</span></code></a> - The maximum URL length to allow for crawled URLs.</p></li>
</ul>
</div></blockquote>
</dd></dl>

</section>
</section>
</section>
<span id="document-topics/extensions"></span><section id="extensions">
<span id="topics-extensions"></span><h3>Extensions<a class="headerlink" href="#extensions" title="Permalink to this heading">¶</a></h3>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes.</p>
<section id="extension-settings">
<h4>Extension settings<a class="headerlink" href="#extension-settings" title="Permalink to this heading">¶</a></h4>
<p>Extensions use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Scrapy settings</span></a> to manage their
settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, a
hypothetical extension to handle <a class="reference external" href="https://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a> would use settings like
<code class="docutils literal notranslate"><span class="pre">GOOGLESITEMAP_ENABLED</span></code>, <code class="docutils literal notranslate"><span class="pre">GOOGLESITEMAP_DEPTH</span></code>, and so on.</p>
</section>
<section id="loading-activating-extensions">
<h4>Loading &amp; activating extensions<a class="headerlink" href="#loading-activating-extensions" title="Permalink to this heading">¶</a></h4>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class per spider being run. All the extension
initialization code must be performed in the class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>To make an extension available, add it to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting in
your Scrapy settings. In <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a>, each extension is represented
by a string: the full Python path to the extension’s class name. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EXTENSIONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scrapy.extensions.corestats.CoreStats&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s2">&quot;scrapy.extensions.telnet.TelnetConsole&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>As you can see, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension <em>loading</em> order. The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting is merged with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting defined in Scrapy (and not meant to be
overridden) and then sorted by order to get the final sorted list of enabled
extensions.</p>
<p>As extensions typically do not depend on each other, their loading order is
irrelevant in most cases. This is why the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting
defines all extensions with the same order (<code class="docutils literal notranslate"><span class="pre">0</span></code>). However, this feature can
be exploited if you need to add an extension which depends on other extensions
already loaded.</p>
</section>
<section id="available-enabled-and-disabled-extensions">
<h4>Available, enabled and disabled extensions<a class="headerlink" href="#available-enabled-and-disabled-extensions" title="Permalink to this heading">¶</a></h4>
<p>Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_ENABLED</span></code></a> setting is set.</p>
</section>
<section id="disabling-an-extension">
<h4>Disabling an extension<a class="headerlink" href="#disabling-an-extension" title="Permalink to this heading">¶</a></h4>
<p>In order to disable an extension that comes enabled by default (i.e. those
included in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting) you must set its order to
<code class="docutils literal notranslate"><span class="pre">None</span></code>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EXTENSIONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scrapy.extensions.corestats.CoreStats&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="writing-your-own-extension">
<h4>Writing your own extension<a class="headerlink" href="#writing-your-own-extension" title="Permalink to this heading">¶</a></h4>
<p>Each extension is a Python class. The main entry point for a Scrapy extension
(this also includes middlewares and pipelines) is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code>
class method which receives a <code class="docutils literal notranslate"><span class="pre">Crawler</span></code> instance. Through the Crawler object
you can access settings, signals, stats, and also control the crawling behaviour.</p>
<p>Typically, extensions connect to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">signals</span></a> and perform
tasks triggered by them.</p>
<p>Finally, if the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> method raises the
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal notranslate"><span class="pre">NotConfigured</span></code></a> exception, the extension will be
disabled. Otherwise, the extension will be enabled.</p>
<section id="sample-extension">
<h5>Sample extension<a class="headerlink" href="#sample-extension" title="Permalink to this heading">¶</a></h5>
<p>Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:</p>
<ul class="simple">
<li><p>a spider is opened</p></li>
<li><p>a spider is closed</p></li>
<li><p>a specific number of items are scraped</p></li>
</ul>
<p>The extension will be enabled through the <code class="docutils literal notranslate"><span class="pre">MYEXT_ENABLED</span></code> setting and the
number of items will be specified through the <code class="docutils literal notranslate"><span class="pre">MYEXT_ITEMCOUNT</span></code> setting.</p>
<p>Here is the code of such extension:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">signals</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">NotConfigured</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SpiderOpenCloseLogging</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item_count</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">item_count</span> <span class="o">=</span> <span class="n">item_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="c1"># first check if the extension should be enabled and raise</span>
        <span class="c1"># NotConfigured otherwise</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s2">&quot;MYEXT_ENABLED&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">NotConfigured</span>

        <span class="c1"># get the number of items from settings</span>
        <span class="n">item_count</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">getint</span><span class="p">(</span><span class="s2">&quot;MYEXT_ITEMCOUNT&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="c1"># instantiate the extension object</span>
        <span class="n">ext</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">item_count</span><span class="p">)</span>

        <span class="c1"># connect the extension object to signals</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">spider_opened</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_opened</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">)</span>

        <span class="c1"># return the extension object</span>
        <span class="k">return</span> <span class="n">ext</span>

    <span class="k">def</span> <span class="nf">spider_opened</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;opened spider </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">spider_closed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;closed spider </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">item_scraped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;scraped </span><span class="si">%d</span><span class="s2"> items&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="built-in-extensions-reference">
<span id="topics-extensions-ref"></span><h4>Built-in extensions reference<a class="headerlink" href="#built-in-extensions-reference" title="Permalink to this heading">¶</a></h4>
<section id="general-purpose-extensions">
<h5>General purpose extensions<a class="headerlink" href="#general-purpose-extensions" title="Permalink to this heading">¶</a></h5>
<section id="module-scrapy.extensions.logstats">
<span id="log-stats-extension"></span><h6>Log Stats extension<a class="headerlink" href="#module-scrapy.extensions.logstats" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.logstats.LogStats">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.logstats.</span></span><span class="sig-name descname"><span class="pre">LogStats</span></span><a class="headerlink" href="#scrapy.extensions.logstats.LogStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Log basic stats like crawled pages and scraped items.</p>
</section>
<section id="module-scrapy.extensions.corestats">
<span id="core-stats-extension"></span><h6>Core Stats extension<a class="headerlink" href="#module-scrapy.extensions.corestats" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.corestats.CoreStats">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.corestats.</span></span><span class="sig-name descname"><span class="pre">CoreStats</span></span><a class="headerlink" href="#scrapy.extensions.corestats.CoreStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Enable the collection of core statistics, provided the stats collection is
enabled (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>).</p>
</section>
<section id="module-scrapy.extensions.telnet">
<span id="telnet-console-extension"></span><span id="topics-extensions-ref-telnetconsole"></span><h6>Telnet console extension<a class="headerlink" href="#module-scrapy.extensions.telnet" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.telnet.TelnetConsole">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.telnet.</span></span><span class="sig-name descname"><span class="pre">TelnetConsole</span></span><a class="headerlink" href="#scrapy.extensions.telnet.TelnetConsole" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.</p>
<p>The telnet console must be enabled by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a>
setting, and the server will listen in the port specified in
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PORT</span></code></a>.</p>
</section>
<section id="module-scrapy.extensions.memusage">
<span id="memory-usage-extension"></span><span id="topics-extensions-ref-memusage"></span><h6>Memory usage extension<a class="headerlink" href="#module-scrapy.extensions.memusage" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.memusage.MemoryUsage">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.memusage.</span></span><span class="sig-name descname"><span class="pre">MemoryUsage</span></span><a class="headerlink" href="#scrapy.extensions.memusage.MemoryUsage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This extension does not work in Windows.</p>
</div>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<ol class="arabic simple">
<li><p>sends a notification e-mail when it exceeds a certain value</p></li>
<li><p>closes the spider when it exceeds a certain value</p></li>
</ol>
<p>The notification e-mails can be triggered when a certain warning value is
reached (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>) and when the maximum value is reached
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>) which will also cause the spider to be closed
and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_ENABLED</span></code></a> setting and
can be configured with the following settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a></p></li>
</ul>
</section>
<section id="module-scrapy.extensions.memdebug">
<span id="memory-debugger-extension"></span><h6>Memory debugger extension<a class="headerlink" href="#module-scrapy.extensions.memdebug" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.memdebug.MemoryDebugger">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.memdebug.</span></span><span class="sig-name descname"><span class="pre">MemoryDebugger</span></span><a class="headerlink" href="#scrapy.extensions.memdebug.MemoryDebugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>An extension for debugging memory usage. It collects information about:</p>
<ul class="simple">
<li><p>objects uncollected by the Python garbage collector</p></li>
<li><p>objects left alive that shouldn’t. For more info, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a></p></li>
</ul>
<p>To enable this extension, turn on the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMDEBUG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMDEBUG_ENABLED</span></code></a> setting. The
info will be stored in the stats.</p>
</section>
<section id="module-scrapy.extensions.spiderstate">
<span id="spider-state-extension"></span><span id="topics-extensions-ref-spiderstate"></span><h6>Spider state extension<a class="headerlink" href="#module-scrapy.extensions.spiderstate" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.spiderstate.SpiderState">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.spiderstate.</span></span><span class="sig-name descname"><span class="pre">SpiderState</span></span><a class="headerlink" href="#scrapy.extensions.spiderstate.SpiderState" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Manages spider state data by loading it before a crawl and saving it after.</p>
<p>Give a value to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> setting to enable this extension.
When enabled, this extension manages the <a class="reference internal" href="index.html#scrapy.Spider.state" title="scrapy.Spider.state"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state</span></code></a>
attribute of your <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> instance:</p>
<ul class="simple">
<li><p>When your spider closes (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a>), the contents of its
<a class="reference internal" href="index.html#scrapy.Spider.state" title="scrapy.Spider.state"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state</span></code></a> attribute are serialized into a file named
<code class="docutils literal notranslate"><span class="pre">spider.state</span></code> in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> folder.</p></li>
<li><p>When your spider opens (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>), if a previously-generated
<code class="docutils literal notranslate"><span class="pre">spider.state</span></code> file exists in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> folder, it is loaded
into the <a class="reference internal" href="index.html#scrapy.Spider.state" title="scrapy.Spider.state"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state</span></code></a> attribute.</p></li>
</ul>
<p>For an example, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-keeping-persistent-state-between-batches"><span class="std std-ref">Keeping persistent state between batches</span></a>.</p>
</section>
<section id="module-scrapy.extensions.closespider">
<span id="close-spider-extension"></span><h6>Close spider extension<a class="headerlink" href="#module-scrapy.extensions.closespider" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.closespider.CloseSpider">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.closespider.</span></span><span class="sig-name descname"><span class="pre">CloseSpider</span></span><a class="headerlink" href="#scrapy.extensions.closespider.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the following
settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CLOSESPIDER_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT_NO_ITEM</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When a certain closing condition is met, requests which are
currently in the downloader queue (up to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>
requests) are still processed.</p>
</div>
<section id="closespider-timeout">
<span id="std-setting-CLOSESPIDER_TIMEOUT"></span><h6 aria-level="7">CLOSESPIDER_TIMEOUT<a class="headerlink" href="#closespider-timeout" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason <code class="docutils literal notranslate"><span class="pre">closespider_timeout</span></code>. If zero (or non set), spiders won’t be closed by
timeout.</p>
</section>
<section id="closespider-timeout-no-item">
<span id="std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM"></span><h6 aria-level="7">CLOSESPIDER_TIMEOUT_NO_ITEM<a class="headerlink" href="#closespider-timeout-no-item" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of seconds. If the spider has not produced
any items in the last number of seconds, it will be closed with the reason
<code class="docutils literal notranslate"><span class="pre">closespider_timeout_no_item</span></code>. If zero (or non set), spiders won’t be closed
regardless if it hasn’t produced any items.</p>
</section>
<section id="closespider-itemcount">
<span id="std-setting-CLOSESPIDER_ITEMCOUNT"></span><h6 aria-level="7">CLOSESPIDER_ITEMCOUNT<a class="headerlink" href="#closespider-itemcount" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of items. If the spider scrapes more than
that amount and those items are passed by the item pipeline, the
spider will be closed with the reason <code class="docutils literal notranslate"><span class="pre">closespider_itemcount</span></code>.
If zero (or non set), spiders won’t be closed by number of passed items.</p>
</section>
<section id="closespider-pagecount">
<span id="std-setting-CLOSESPIDER_PAGECOUNT"></span><h6 aria-level="7">CLOSESPIDER_PAGECOUNT<a class="headerlink" href="#closespider-pagecount" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
<code class="docutils literal notranslate"><span class="pre">closespider_pagecount</span></code>. If zero (or non set), spiders won’t be closed by
number of crawled responses.</p>
</section>
<section id="closespider-pagecount-no-item">
<span id="std-setting-CLOSESPIDER_PAGECOUNT_NO_ITEM"></span><h6 aria-level="7">CLOSESPIDER_PAGECOUNT_NO_ITEM<a class="headerlink" href="#closespider-pagecount-no-item" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of consecutive responses to crawl
without items scraped. If the spider crawls more consecutive responses than that
and no items are scraped in the meantime, the spider will be closed with the
reason <code class="docutils literal notranslate"><span class="pre">closespider_pagecount_no_item</span></code>. If zero (or not set), spiders won’t be
closed by number of crawled responses with no items.</p>
</section>
<section id="closespider-errorcount">
<span id="std-setting-CLOSESPIDER_ERRORCOUNT"></span><h6 aria-level="7">CLOSESPIDER_ERRORCOUNT<a class="headerlink" href="#closespider-errorcount" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason <code class="docutils literal notranslate"><span class="pre">closespider_errorcount</span></code>. If zero (or non
set), spiders won’t be closed by number of errors.</p>
</section>
</section>
<section id="module-scrapy.extensions.statsmailer">
<span id="statsmailer-extension"></span><h6>StatsMailer extension<a class="headerlink" href="#module-scrapy.extensions.statsmailer" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.statsmailer.StatsMailer">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.statsmailer.</span></span><span class="sig-name descname"><span class="pre">StatsMailer</span></span><a class="headerlink" href="#scrapy.extensions.statsmailer.StatsMailer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This simple extension can be used to send a notification e-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-STATSMAILER_RCPTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATSMAILER_RCPTS</span></code></a>
setting.</p>
<p>Emails can be sent using the <a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a> class. To see a
full list of parameters, including examples on how to instantiate
<a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a> and use mail settings, see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-email"><span class="std std-ref">Sending e-mail</span></a>.</p>
<span class="target" id="module-scrapy.extensions.debug"></span><span class="target" id="module-scrapy.extensions.periodic_log"></span></section>
<section id="periodic-log-extension">
<h6>Periodic log extension<a class="headerlink" href="#periodic-log-extension" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.periodic_log.PeriodicLog">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.periodic_log.</span></span><span class="sig-name descname"><span class="pre">PeriodicLog</span></span><a class="headerlink" href="#scrapy.extensions.periodic_log.PeriodicLog" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This extension periodically logs rich stat data as a JSON object:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2023</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">04</span> <span class="mi">02</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mi">57</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">976</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">162</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">925</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">161</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">04</span> <span class="mi">02</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mi">57</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">periodic_log</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;delta&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;downloader/request_bytes&quot;</span><span class="p">:</span> <span class="mi">55582</span><span class="p">,</span>
        <span class="s2">&quot;downloader/request_count&quot;</span><span class="p">:</span> <span class="mi">162</span><span class="p">,</span>
        <span class="s2">&quot;downloader/request_method_count/GET&quot;</span><span class="p">:</span> <span class="mi">162</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_bytes&quot;</span><span class="p">:</span> <span class="mi">618133</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_count&quot;</span><span class="p">:</span> <span class="mi">162</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_status_count/200&quot;</span><span class="p">:</span> <span class="mi">162</span><span class="p">,</span>
        <span class="s2">&quot;item_scraped_count&quot;</span><span class="p">:</span> <span class="mi">161</span>
    <span class="p">},</span>
    <span class="s2">&quot;stats&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;downloader/request_bytes&quot;</span><span class="p">:</span> <span class="mi">338243</span><span class="p">,</span>
        <span class="s2">&quot;downloader/request_count&quot;</span><span class="p">:</span> <span class="mi">992</span><span class="p">,</span>
        <span class="s2">&quot;downloader/request_method_count/GET&quot;</span><span class="p">:</span> <span class="mi">992</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_bytes&quot;</span><span class="p">:</span> <span class="mi">3836736</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_count&quot;</span><span class="p">:</span> <span class="mi">976</span><span class="p">,</span>
        <span class="s2">&quot;downloader/response_status_count/200&quot;</span><span class="p">:</span> <span class="mi">976</span><span class="p">,</span>
        <span class="s2">&quot;item_scraped_count&quot;</span><span class="p">:</span> <span class="mi">925</span><span class="p">,</span>
        <span class="s2">&quot;log_count/INFO&quot;</span><span class="p">:</span> <span class="mi">21</span><span class="p">,</span>
        <span class="s2">&quot;log_count/WARNING&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;scheduler/dequeued&quot;</span><span class="p">:</span> <span class="mi">992</span><span class="p">,</span>
        <span class="s2">&quot;scheduler/dequeued/memory&quot;</span><span class="p">:</span> <span class="mi">992</span><span class="p">,</span>
        <span class="s2">&quot;scheduler/enqueued&quot;</span><span class="p">:</span> <span class="mi">1050</span><span class="p">,</span>
        <span class="s2">&quot;scheduler/enqueued/memory&quot;</span><span class="p">:</span> <span class="mi">1050</span>
    <span class="p">},</span>
    <span class="s2">&quot;time&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;elapsed&quot;</span><span class="p">:</span> <span class="mf">360.008903</span><span class="p">,</span>
        <span class="s2">&quot;log_interval&quot;</span><span class="p">:</span> <span class="mf">60.0</span><span class="p">,</span>
        <span class="s2">&quot;log_interval_real&quot;</span><span class="p">:</span> <span class="mf">60.006694</span><span class="p">,</span>
        <span class="s2">&quot;start_time&quot;</span><span class="p">:</span> <span class="s2">&quot;2023-08-03 23:24:57&quot;</span><span class="p">,</span>
        <span class="s2">&quot;utcnow&quot;</span><span class="p">:</span> <span class="s2">&quot;2023-08-03 23:30:57&quot;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This extension logs the following configurable sections:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;delta&quot;</span></code> shows how some numeric stats have changed since the last stats
log message.</p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-PERIODIC_LOG_DELTA"><code class="xref std std-setting docutils literal notranslate"><span class="pre">PERIODIC_LOG_DELTA</span></code></a> setting determines the target stats. They
must have <code class="docutils literal notranslate"><span class="pre">int</span></code> or <code class="docutils literal notranslate"><span class="pre">float</span></code> values.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;stats&quot;</span></code> shows the current value of some stats.</p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-PERIODIC_LOG_STATS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">PERIODIC_LOG_STATS</span></code></a> setting determines the target stats.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;time&quot;</span></code> shows detailed timing data.</p>
<p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-setting-PERIODIC_LOG_TIMING_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">PERIODIC_LOG_TIMING_ENABLED</span></code></a> setting determines whether or
not to show this section.</p>
</li>
</ul>
<p>This extension logs data at the start, then on a fixed time interval
configurable through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOGSTATS_INTERVAL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOGSTATS_INTERVAL</span></code></a> setting, and finally
right before the crawl ends.</p>
<p>Example extension configuration:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">custom_settings</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;LOG_LEVEL&quot;</span><span class="p">:</span> <span class="s2">&quot;INFO&quot;</span><span class="p">,</span>
    <span class="s2">&quot;PERIODIC_LOG_STATS&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;include&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;downloader/&quot;</span><span class="p">,</span> <span class="s2">&quot;scheduler/&quot;</span><span class="p">,</span> <span class="s2">&quot;log_count/&quot;</span><span class="p">,</span> <span class="s2">&quot;item_scraped_count/&quot;</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="s2">&quot;PERIODIC_LOG_DELTA&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;include&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;downloader/&quot;</span><span class="p">]},</span>
    <span class="s2">&quot;PERIODIC_LOG_TIMING_ENABLED&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;EXTENSIONS&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;scrapy.extensions.periodic_log.PeriodicLog&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<section id="periodic-log-delta">
<span id="std-setting-PERIODIC_LOG_DELTA"></span><h6 aria-level="7">PERIODIC_LOG_DELTA<a class="headerlink" href="#periodic-log-delta" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_DELTA&quot;:</span> <span class="pre">True</span></code> - show deltas for all <code class="docutils literal notranslate"><span class="pre">int</span></code> and <code class="docutils literal notranslate"><span class="pre">float</span></code> stat values.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_DELTA&quot;:</span> <span class="pre">{&quot;include&quot;:</span> <span class="pre">[&quot;downloader/&quot;,</span> <span class="pre">&quot;scheduler/&quot;]}</span></code> - show deltas for stats with names containing any configured substring.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_DELTA&quot;:</span> <span class="pre">{&quot;exclude&quot;:</span> <span class="pre">[&quot;downloader/&quot;]}</span></code> - show deltas for all stats with names not containing any configured substring.</p></li>
</ul>
</section>
<section id="periodic-log-stats">
<span id="std-setting-PERIODIC_LOG_STATS"></span><h6 aria-level="7">PERIODIC_LOG_STATS<a class="headerlink" href="#periodic-log-stats" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_STATS&quot;:</span> <span class="pre">True</span></code> - show the current value of all stats.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_STATS&quot;:</span> <span class="pre">{&quot;include&quot;:</span> <span class="pre">[&quot;downloader/&quot;,</span> <span class="pre">&quot;scheduler/&quot;]}</span></code> - show current values for stats with names containing any configured substring.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;PERIODIC_LOG_STATS&quot;:</span> <span class="pre">{&quot;exclude&quot;:</span> <span class="pre">[&quot;downloader/&quot;]}</span></code> - show current values for all stats with names not containing any configured substring.</p></li>
</ul>
</section>
<section id="periodic-log-timing-enabled">
<span id="std-setting-PERIODIC_LOG_TIMING_ENABLED"></span><h6 aria-level="7">PERIODIC_LOG_TIMING_ENABLED<a class="headerlink" href="#periodic-log-timing-enabled" title="Permalink to this heading">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">True</span></code> enables logging of timing data (i.e. the <code class="docutils literal notranslate"><span class="pre">&quot;time&quot;</span></code> section).</p>
</section>
</section>
</section>
<section id="debugging-extensions">
<h5>Debugging extensions<a class="headerlink" href="#debugging-extensions" title="Permalink to this heading">¶</a></h5>
<section id="stack-trace-dump-extension">
<h6>Stack trace dump extension<a class="headerlink" href="#stack-trace-dump-extension" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.periodic_log.StackTraceDump">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.periodic_log.</span></span><span class="sig-name descname"><span class="pre">StackTraceDump</span></span><a class="headerlink" href="#scrapy.extensions.periodic_log.StackTraceDump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Dumps information about the running process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. The information dumped is the following:</p>
<ol class="arabic simple">
<li><p>engine status (using <code class="docutils literal notranslate"><span class="pre">scrapy.utils.engine.get_engine_status()</span></code>)</p></li>
<li><p>live references (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a>)</p></li>
<li><p>stack trace of all threads</p></li>
</ol>
<p>After the stack trace and engine status is dumped, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (i.e. not Windows),
because the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a> signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> signal:</p>
<ol class="arabic">
<li><p>By pressing Ctrl-while a Scrapy process is running (Linux only?)</p></li>
<li><p>By running this command (assuming <code class="docutils literal notranslate"><span class="pre">&lt;pid&gt;</span></code> is the process id of the Scrapy
process):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kill</span> <span class="o">-</span><span class="n">QUIT</span> <span class="o">&lt;</span><span class="n">pid</span><span class="o">&gt;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="debugger-extension">
<h6>Debugger extension<a class="headerlink" href="#debugger-extension" title="Permalink to this heading">¶</a></h6>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.extensions.periodic_log.Debugger">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.extensions.periodic_log.</span></span><span class="sig-name descname"><span class="pre">Debugger</span></span><a class="headerlink" href="#scrapy.extensions.periodic_log.Debugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Invokes a <a class="reference external" href="https://docs.python.org/3/library/pdb.html" title="(in Python v3.13)"><span class="xref std std-doc">Python debugger</span></a> inside a running Scrapy process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. After the debugger is exited, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (i.e. not Windows).</p>
</section>
</section>
</section>
</section>
<span id="document-topics/signals"></span><section id="signals">
<span id="topics-signals"></span><h3>Signals<a class="headerlink" href="#signals" title="Permalink to this heading">¶</a></h3>
<p>Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extension</span></a>, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch them
don’t need to accept all of them - the signal dispatching mechanism will only
deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api-signals"><span class="std std-ref">Signals API</span></a>.</p>
<p>Here is a simple example showing how you can catch signals and perform some action:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">signals</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>


<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">spider</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DmozSpider</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">spider</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spider</span>

    <span class="k">def</span> <span class="nf">spider_closed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">spider</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Spider closed: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
<section id="deferred-signal-handlers">
<span id="signal-deferred"></span><h4>Deferred signal handlers<a class="headerlink" href="#deferred-signal-handlers" title="Permalink to this heading">¶</a></h4>
<p>Some signals support returning <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a>
or <a class="reference external" href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.13)"><span class="xref std std-term">awaitable objects</span></a> from their handlers, allowing
you to run asynchronous code that does not block Scrapy. If a signal
handler returns one of these objects, Scrapy waits for that asynchronous
operation to finish.</p>
<p>Let’s take an example using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">coroutines</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">SignalSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;signals&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://quotes.toscrape.com/page/1/&quot;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">spider</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">SignalSpider</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">spider</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spider</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">item_scraped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="c1"># Send the scraped item to the server</span>
        <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">treq</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="s2">&quot;http://example.com/post&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;ascii&quot;</span><span class="p">),</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="sa">b</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="p">[</span><span class="sa">b</span><span class="s2">&quot;application/json&quot;</span><span class="p">]},</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">response</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s2">&quot;tags&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>
</pre></div>
</div>
<p>See the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> below to know which signals support
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> and <a class="reference external" href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.13)"><span class="xref std std-term">awaitable objects</span></a>.</p>
</section>
<section id="module-scrapy.signals">
<span id="built-in-signals-reference"></span><span id="topics-signals-ref"></span><h4>Built-in signals reference<a class="headerlink" href="#module-scrapy.signals" title="Permalink to this heading">¶</a></h4>
<p>Here’s the list of Scrapy built-in signals and their meaning.</p>
<section id="engine-signals">
<h5>Engine signals<a class="headerlink" href="#engine-signals" title="Permalink to this heading">¶</a></h5>
<section id="engine-started">
<h6>engine_started<a class="headerlink" href="#engine-started" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-engine_started"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.engine_started">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">engine_started</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_started" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine has started crawling.</p>
<p>This signal supports returning deferreds from its handlers.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This signal may be fired <em>after</em> the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a> signal,
depending on how the spider was started. So <strong>don’t</strong> rely on this signal
getting fired before <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</div>
</section>
<section id="engine-stopped">
<h6>engine_stopped<a class="headerlink" href="#engine-stopped" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-engine_stopped"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.engine_stopped">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">engine_stopped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_stopped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<p>This signal supports returning deferreds from its handlers.</p>
</dd></dl>

</section>
</section>
<section id="item-signals">
<h5>Item signals<a class="headerlink" href="#item-signals" title="Permalink to this heading">¶</a></h5>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As at max <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_ITEMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_ITEMS</span></code></a> items are processed in
parallel, many deferreds are fired together using
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeferredList</span></code></a>. Hence the next
batch waits for the <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DeferredList</span></code></a>
to fire and then runs the respective item signal handler for
the next batch of scraped items.</p>
</div>
<section id="item-scraped">
<h6>item_scraped<a class="headerlink" href="#item-scraped" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-item_scraped"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.item_scraped">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">item_scraped</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_scraped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when an item has been scraped, after it has passed all the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a> stages (without being dropped).</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item</strong> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>) – the scraped item</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> | <code class="docutils literal notranslate"><span class="pre">None</span></code>) – the response from where the item was scraped, or <code class="docutils literal notranslate"><span class="pre">None</span></code>
if it was yielded from <a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="item-dropped">
<h6>item_dropped<a class="headerlink" href="#item-dropped" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-item_dropped"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.item_dropped">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">item_dropped</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exception</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after an item has been dropped from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>
when some stage raised a <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item</strong> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>) – the item dropped from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a></p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> | <code class="docutils literal notranslate"><span class="pre">None</span></code>) – the response from where the item was dropped, or <code class="docutils literal notranslate"><span class="pre">None</span></code>
if it was yielded from <a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a>.</p></li>
<li><p><strong>exception</strong> (<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception) – the exception (which must be a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> subclass) which caused the item
to be dropped</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="item-error">
<h6>item_error<a class="headerlink" href="#item-error" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-item_error"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.item_error">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">item_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">failure</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a> generates an error (i.e. raises
an exception), except <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item</strong> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a>) – the item that caused the error in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a></p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> | <code class="docutils literal notranslate"><span class="pre">None</span></code>) – the response being processed when the exception was
raised, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if it was yielded from
<a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a>.</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</p></li>
<li><p><strong>failure</strong> (<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><em>twisted.python.failure.Failure</em></a>) – the exception raised</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="spider-signals">
<h5>Spider signals<a class="headerlink" href="#spider-signals" title="Permalink to this heading">¶</a></h5>
<section id="spider-closed">
<h6>spider_closed<a class="headerlink" href="#spider-closed" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-spider_closed"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.spider_closed">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">spider_closed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reason</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been closed. This can be used to release per-spider
resources reserved on <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been closed</p></li>
<li><p><strong>reason</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – a string which describes the reason why the spider was closed. If
it was closed because the spider has completed scraping, the reason
is <code class="docutils literal notranslate"><span class="pre">'finished'</span></code>. Otherwise, if the spider was manually closed by
calling the <code class="docutils literal notranslate"><span class="pre">close_spider</span></code> engine method, then the reason is the one
passed in the <code class="docutils literal notranslate"><span class="pre">reason</span></code> argument of that method (which defaults to
<code class="docutils literal notranslate"><span class="pre">'cancelled'</span></code>). If the engine was shutdown (for example, by hitting
Ctrl-C to stop it) the reason will be <code class="docutils literal notranslate"><span class="pre">'shutdown'</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="spider-opened">
<h6>spider_opened<a class="headerlink" href="#spider-opened" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-spider_opened"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.spider_opened">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">spider_opened</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been opened for crawling. This is typically used to
reserve per-spider resources, but can be used for any task that needs to be
performed when a spider is opened.</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been opened</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="spider-idle">
<h6>spider_idle<a class="headerlink" href="#spider-idle" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-spider_idle"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.spider_idle">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">spider_idle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider has gone idle, which means the spider has no further:</p>
<blockquote>
<div><ul class="simple">
<li><p>requests waiting to be downloaded</p></li>
<li><p>requests scheduled</p></li>
<li><p>items being processed in the item pipeline</p></li>
</ul>
</div></blockquote>
<p>If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> signal is sent.</p>
<p>You may raise a <a class="reference internal" href="index.html#scrapy.exceptions.DontCloseSpider" title="scrapy.exceptions.DontCloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DontCloseSpider</span></code></a> exception to
prevent the spider from being closed.</p>
<p>Alternatively, you may raise a <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a>
exception to provide a custom spider closing reason. An
idle handler is the perfect place to put some code that assesses
the final spider results and update the final closing reason
accordingly (e.g. setting it to ‘too_few_results’ instead of
‘finished’).</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has gone idle</p>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scheduling some requests in your <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_idle"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_idle</span></code></a> handler does
<strong>not</strong> guarantee that it can prevent the spider from being closed,
although it sometimes can. That’s because the spider may still remain idle
if all the scheduled requests are rejected by the scheduler (e.g. filtered
due to duplication).</p>
</div>
</section>
<section id="spider-error">
<h6>spider_error<a class="headerlink" href="#spider-error" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-spider_error"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.spider_error">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">spider_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">failure</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider callback generates an error (i.e. raises an exception).</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>failure</strong> (<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)"><em>twisted.python.failure.Failure</em></a>) – the exception raised</p></li>
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed when the exception was raised</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="feed-slot-closed">
<h6>feed_slot_closed<a class="headerlink" href="#feed-slot-closed" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-feed_slot_closed"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.feed_slot_closed">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">feed_slot_closed</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">slot</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.feed_slot_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> slot is closed.</p>
<p>This signal supports returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>slot</strong> (<em>scrapy.extensions.feedexport.FeedSlot</em>) – the slot closed</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="feed-exporter-closed">
<h6>feed_exporter_closed<a class="headerlink" href="#feed-exporter-closed" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-feed_exporter_closed"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.feed_exporter_closed">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">feed_exporter_closed</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.feed_exporter_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> extension is closed,
during the handling of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="#std-signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> signal by the extension,
after all feed exporting has been handled.</p>
<p>This signal supports returning deferreds from its handlers.</p>
</dd></dl>

</section>
</section>
<section id="request-signals">
<h5>Request signals<a class="headerlink" href="#request-signals" title="Permalink to this heading">¶</a></h5>
<section id="request-scheduled">
<h6>request_scheduled<a class="headerlink" href="#request-scheduled" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-request_scheduled"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.request_scheduled">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">request_scheduled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine is asked to schedule a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>, to be
downloaded later, before the request reaches the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">scheduler</span></a>.</p>
<p>Raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> to drop a request before it
reaches the scheduler.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.11.2: </span>Allow dropping requests with <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that reached the scheduler</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="request-dropped">
<h6>request_dropped<a class="headerlink" href="#request-dropped" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-request_dropped"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.request_dropped">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">request_dropped</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code>, scheduled by the engine to be
downloaded later, is rejected by the scheduler.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that reached the scheduler</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="request-reached-downloader">
<h6>request_reached_downloader<a class="headerlink" href="#request-reached-downloader" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-request_reached_downloader"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.request_reached_downloader">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">request_reached_downloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_reached_downloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> reached downloader.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that reached downloader</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="request-left-downloader">
<h6>request_left_downloader<a class="headerlink" href="#request-left-downloader" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-request_left_downloader"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.request_left_downloader">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">request_left_downloader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_left_downloader" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified added">New in version 2.0.</span></p>
</div>
<p>Sent when a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> leaves the downloader, even in case of
failure.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that reached the downloader</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="bytes-received">
<h6>bytes_received<a class="headerlink" href="#bytes-received" title="Permalink to this heading">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.2.</span></p>
</div>
<span class="target" id="std-signal-bytes_received"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.bytes_received">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">bytes_received</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.bytes_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the HTTP 1.1 and S3 download handlers when a group of bytes is
received for a specific request. This signal might be fired multiple
times for the same request, with partial data each time. For instance,
a possible scenario for a 25 kb response would be two signals fired
with 10 kb of data, and a final one with 5 kb of data.</p>
<p>Handlers for this signal can stop the download of a response while it
is in progress by raising the <a class="reference internal" href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload"><code class="xref py py-exc docutils literal notranslate"><span class="pre">StopDownload</span></code></a>
exception. Please refer to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">Stopping the download of a Response</span></a> topic
for additional information and examples.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes</span></code></a> object) – the data received by the download handler</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the download</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider associated with the response</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="headers-received">
<h6>headers_received<a class="headerlink" href="#headers-received" title="Permalink to this heading">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.5.</span></p>
</div>
<span class="target" id="std-signal-headers_received"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.headers_received">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">headers_received</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">headers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">body_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.headers_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the HTTP 1.1 and S3 download handlers when the response headers are
available for a given request, before downloading any additional content.</p>
<p>Handlers for this signal can stop the download of a response while it
is in progress by raising the <a class="reference internal" href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload"><code class="xref py py-exc docutils literal notranslate"><span class="pre">StopDownload</span></code></a>
exception. Please refer to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">Stopping the download of a Response</span></a> topic
for additional information and examples.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>headers</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.headers.Headers</span></code> object) – the headers received by the download handler</p></li>
<li><p><strong>body_length</strong> (<cite>int</cite>) – expected size of the response body, in bytes</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the download</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider associated with the response</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="response-signals">
<h5>Response signals<a class="headerlink" href="#response-signals" title="Permalink to this heading">¶</a></h5>
<section id="response-received">
<h6>response_received<a class="headerlink" href="#response-received" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-response_received"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.response_received">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">response_received</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine receives a new <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> from the
downloader.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response received</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the response</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">request</span></code> argument might not contain the original request that
reached the downloader, if a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middleware</span></a> modifies
the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object and sets a specific <code class="docutils literal notranslate"><span class="pre">request</span></code>
attribute.</p>
</div>
</section>
<section id="response-downloaded">
<h6>response_downloaded<a class="headerlink" href="#response-downloaded" title="Permalink to this heading">¶</a></h6>
<span class="target" id="std-signal-response_downloaded"></span><dl class="py function">
<dt class="sig sig-object py" id="scrapy.signals.response_downloaded">
<span class="sig-prename descclassname"><span class="pre">scrapy.signals.</span></span><span class="sig-name descname"><span class="pre">response_downloaded</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">response</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">request</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the downloader right after a <code class="docutils literal notranslate"><span class="pre">HTTPResponse</span></code> is downloaded.</p>
<p>This signal does not support returning deferreds from its handlers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response downloaded</p></li>
<li><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object) – the request that generated the response</p></li>
<li><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
</section>
<span id="document-topics/scheduler"></span><section id="module-scrapy.core.scheduler">
<span id="scheduler"></span><span id="topics-scheduler"></span><h3>Scheduler<a class="headerlink" href="#module-scrapy.core.scheduler" title="Permalink to this heading">¶</a></h3>
<p>The scheduler component receives requests from the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#component-engine"><span class="std std-ref">engine</span></a>
and stores them into persistent and/or non-persistent data structures.
It also gets those requests and feeds them back to the engine when it
asks for a next request to be downloaded.</p>
<section id="overriding-the-default-scheduler">
<h4>Overriding the default scheduler<a class="headerlink" href="#overriding-the-default-scheduler" title="Permalink to this heading">¶</a></h4>
<p>You can use your own custom scheduler class by supplying its full
Python path in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER</span></code></a> setting.</p>
</section>
<section id="minimal-scheduler-interface">
<h4>Minimal scheduler interface<a class="headerlink" href="#minimal-scheduler-interface" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.core.scheduler.</span></span><span class="sig-name descname"><span class="pre">BaseScheduler</span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler" title="Permalink to this definition">¶</a></dt>
<dd><p>The scheduler component is responsible for storing requests received from
the engine, and feeding them back upon request (also to the engine).</p>
<p>The original sources of said requests are:</p>
<ul class="simple">
<li><p>Spider: <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> method, requests created for URLs in the <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> attribute, request callbacks</p></li>
<li><p>Spider middleware: <code class="docutils literal notranslate"><span class="pre">process_spider_output</span></code> and <code class="docutils literal notranslate"><span class="pre">process_spider_exception</span></code> methods</p></li>
<li><p>Downloader middleware: <code class="docutils literal notranslate"><span class="pre">process_request</span></code>, <code class="docutils literal notranslate"><span class="pre">process_response</span></code> and <code class="docutils literal notranslate"><span class="pre">process_exception</span></code> methods</p></li>
</ul>
<p>The order in which the scheduler returns its stored requests (via the <code class="docutils literal notranslate"><span class="pre">next_request</span></code> method)
plays a great part in determining the order in which those requests are downloaded.</p>
<p>The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.close">
<span class="sig-name descname"><span class="pre">close</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reason</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.close" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider is closed by the engine. It receives the reason why the crawl
finished as argument and it’s useful to execute cleaning code.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>reason</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>) – a string which describes the reason why the spider was closed</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.enqueue_request">
<em class="property"><span class="pre">abstract</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">enqueue_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.enqueue_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Process a request received by the engine.</p>
<p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the request is stored correctly, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the engine will fire a <code class="docutils literal notranslate"><span class="pre">request_dropped</span></code> signal, and
will not make further attempts to schedule the request at a later time.
For reference, the default Scrapy scheduler returns <code class="docutils literal notranslate"><span class="pre">False</span></code> when the
request is rejected by the dupefilter.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.from_crawler">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Factory method which receives the current <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object as argument.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.has_pending_requests">
<em class="property"><span class="pre">abstract</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">has_pending_requests</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.has_pending_requests" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if the scheduler has enqueued requests, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.next_request">
<em class="property"><span class="pre">abstract</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">next_request</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.next_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the next <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> to be processed, or <code class="docutils literal notranslate"><span class="pre">None</span></code>
to indicate that there are no requests to be considered ready at the moment.</p>
<p>Returning <code class="docutils literal notranslate"><span class="pre">None</span></code> implies that no request from the scheduler will be sent
to the downloader in the current reactor cycle. The engine will continue
calling <code class="docutils literal notranslate"><span class="pre">next_request</span></code> until <code class="docutils literal notranslate"><span class="pre">has_pending_requests</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.BaseScheduler.open">
<span class="sig-name descname"><span class="pre">open</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.BaseScheduler.open" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider is opened by the engine. It receives the spider
instance as argument and it’s useful to execute initialization code.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>) – the spider object for the current crawl</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="default-scrapy-scheduler">
<h4>Default Scrapy scheduler<a class="headerlink" href="#default-scrapy-scheduler" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.core.scheduler.</span></span><span class="sig-name descname"><span class="pre">Scheduler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dupefilter</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">BaseDupeFilter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jobdir</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dqclass</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">BaseQueue</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mqclass</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">BaseQueue</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logunser</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stats</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><span class="pre">StatsCollector</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pqclass</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">ScrapyPriorityQueue</span><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler" title="Permalink to this definition">¶</a></dt>
<dd><p>Default Scrapy scheduler. This implementation also handles duplication
filtering via the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">dupefilter</span></code></a>.</p>
<p>This scheduler stores requests into several priority queues (defined by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting). In turn, said priority queues
are backed by either memory or disk based queues (respectively defined by the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_MEMORY_QUEUE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DISK_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DISK_QUEUE</span></code></a> settings).</p>
<p>Request prioritization is almost entirely delegated to the priority queue. The only
prioritization performed by this scheduler is using the disk-based queue if present
(i.e. if the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> setting is defined) and falling back to the memory-based
queue if a serialization error occurs. If the disk queue is not present, the memory one
is used directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dupefilter</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.dupefilters.BaseDupeFilter</span></code> instance or similar:
any class that implements the <cite>BaseDupeFilter</cite> interface) – An object responsible for checking and filtering duplicate requests.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> setting is used by default.</p></li>
<li><p><strong>jobdir</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a> or <code class="docutils literal notranslate"><span class="pre">None</span></code>) – The path of a directory to be used for persisting the crawl’s state.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> setting is used by default.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">Jobs: pausing and resuming crawls</span></a>.</p></li>
<li><p><strong>dqclass</strong> (<em>class</em>) – A class to be used as persistent request queue.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DISK_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DISK_QUEUE</span></code></a> setting is used by default.</p></li>
<li><p><strong>mqclass</strong> (<em>class</em>) – A class to be used as non-persistent request queue.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_MEMORY_QUEUE</span></code></a> setting is used by default.</p></li>
<li><p><strong>logunser</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – A boolean that indicates whether or not unserializable requests should be logged.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting is used by default.</p></li>
<li><p><strong>stats</strong> (<a class="reference internal" href="index.html#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.statscollectors.StatsCollector</span></code></a> instance or similar:
any class that implements the <cite>StatsCollector</cite> interface) – A stats collector object to record stats about the request scheduling process.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-STATS_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_CLASS</span></code></a> setting is used by default.</p></li>
<li><p><strong>pqclass</strong> (<em>class</em>) – A class to be used as priority queue for requests.
The value for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting is used by default.</p></li>
<li><p><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></a>) – The crawler object corresponding to the current crawl.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.__len__">
<span class="sig-name descname"><span class="pre">__len__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.__len__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the total amount of enqueued requests</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.close">
<span class="sig-name descname"><span class="pre">close</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reason</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.close" title="Permalink to this definition">¶</a></dt>
<dd><ol class="arabic simple">
<li><p>dump pending requests to disk if there is a disk queue</p></li>
<li><p>return the result of the dupefilter’s <code class="docutils literal notranslate"><span class="pre">close</span></code> method</p></li>
</ol>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.enqueue_request">
<span class="sig-name descname"><span class="pre">enqueue_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.enqueue_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Unless the received request is filtered out by the Dupefilter, attempt to push
it into the disk queue, falling back to pushing it into the memory queue.</p>
<p>Increment the appropriate stats, such as: <code class="docutils literal notranslate"><span class="pre">scheduler/enqueued</span></code>,
<code class="docutils literal notranslate"><span class="pre">scheduler/enqueued/disk</span></code>, <code class="docutils literal notranslate"><span class="pre">scheduler/enqueued/memory</span></code>.</p>
<p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if the request was stored successfully, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.from_crawler">
<em class="property"><span class="pre">classmethod</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Factory method, initializes the scheduler with arguments taken from the crawl settings</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.has_pending_requests">
<span class="sig-name descname"><span class="pre">has_pending_requests</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.has_pending_requests" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">True</span></code> if the scheduler has enqueued requests, <code class="docutils literal notranslate"><span class="pre">False</span></code> otherwise</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.next_request">
<span class="sig-name descname"><span class="pre">next_request</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><span class="pre">Request</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.next_request" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object from the memory queue,
falling back to the disk queue if the memory queue is empty.
Return <code class="docutils literal notranslate"><span class="pre">None</span></code> if there are no more enqueued requests.</p>
<p>Increment the appropriate stats, such as: <code class="docutils literal notranslate"><span class="pre">scheduler/dequeued</span></code>,
<code class="docutils literal notranslate"><span class="pre">scheduler/dequeued/disk</span></code>, <code class="docutils literal notranslate"><span class="pre">scheduler/dequeued/memory</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.core.scheduler.Scheduler.open">
<span class="sig-name descname"><span class="pre">open</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">Spider</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.core.scheduler.Scheduler.open" title="Permalink to this definition">¶</a></dt>
<dd><ol class="arabic simple">
<li><p>initialize the memory queue</p></li>
<li><p>initialize the disk queue if the <code class="docutils literal notranslate"><span class="pre">jobdir</span></code> attribute is a valid directory</p></li>
<li><p>return the result of the dupefilter’s <code class="docutils literal notranslate"><span class="pre">open</span></code> method</p></li>
</ol>
</dd></dl>

</dd></dl>

</section>
</section>
<span id="document-topics/exporters"></span><section id="module-scrapy.exporters">
<span id="item-exporters"></span><span id="topics-exporters"></span><h3>Item Exporters<a class="headerlink" href="#module-scrapy.exporters" title="Permalink to this heading">¶</a></h3>
<p>Once you have scraped your items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.</p>
<section id="using-item-exporters">
<h4>Using Item Exporters<a class="headerlink" href="#using-item-exporters" title="Permalink to this heading">¶</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.</p>
<p>In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-exporters-reference"><span class="std std-ref">Built-in Item Exporters reference</span></a>. After you have
instantiated your exporter, you have to:</p>
<p>1. call the method <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_exporting()</span></code></a> in order to
signal the beginning of the exporting process</p>
<p>2. call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">export_item()</span></code></a> method for each item you want
to export</p>
<p>3. and finally call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finish_exporting()</span></code></a> to signal
the end of the exporting process</p>
<p>Here you can see an <a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">Item Pipeline</span></a> which uses multiple
Item Exporters to group scraped items to different files according to the
value of one of their fields:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exporters</span> <span class="kn">import</span> <span class="n">XmlItemExporter</span>


<span class="k">class</span> <span class="nc">PerYearXmlExportPipeline</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distribute items across multiple XML files according to their &#39;year&#39; field&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">exporter</span><span class="p">,</span> <span class="n">xml_file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">exporter</span><span class="o">.</span><span class="n">finish_exporting</span><span class="p">()</span>
            <span class="n">xml_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_exporter_for_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">adapter</span><span class="p">[</span><span class="s2">&quot;year&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">year</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">:</span>
            <span class="n">xml_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s2">.xml&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span>
            <span class="n">exporter</span> <span class="o">=</span> <span class="n">XmlItemExporter</span><span class="p">(</span><span class="n">xml_file</span><span class="p">)</span>
            <span class="n">exporter</span><span class="o">.</span><span class="n">start_exporting</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">[</span><span class="n">year</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">exporter</span><span class="p">,</span> <span class="n">xml_file</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">[</span><span class="n">year</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">exporter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exporter_for_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">exporter</span><span class="o">.</span><span class="n">export_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</section>
<section id="serialization-of-item-fields">
<span id="topics-exporters-field-serialization"></span><h4>Serialization of item fields<a class="headerlink" href="#serialization-of-item-fields" title="Permalink to this heading">¶</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it is
passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which are
described next.</p>
<section id="declaring-a-serializer-in-the-field">
<span id="topics-exporters-serializers"></span><h5>1. Declaring a serializer in the field<a class="headerlink" href="#declaring-a-serializer-in-the-field" title="Permalink to this heading">¶</a></h5>
<p>If you use <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> you can declare a serializer in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items-fields"><span class="std std-ref">field metadata</span></a>. The serializer must be
a callable which receives a value and returns its serialized form.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">def</span> <span class="nf">serialize_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;$ </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>


<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="n">serialize_price</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="overriding-the-serialize-field-method">
<h5>2. Overriding the serialize_field() method<a class="headerlink" href="#overriding-the-serialize-field-method" title="Permalink to this heading">¶</a></h5>
<p>You can also override the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code></a> method to
customize how your field value will be exported.</p>
<p>Make sure you call the base class <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code></a> method
after your custom code.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exporters</span> <span class="kn">import</span> <span class="n">XmlItemExporter</span>


<span class="k">class</span> <span class="nc">ProductXmlExporter</span><span class="p">(</span><span class="n">XmlItemExporter</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">serialize_field</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;price&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;$ </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">serialize_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="built-in-item-exporters-reference">
<span id="topics-exporters-reference"></span><h4>Built-in Item Exporters reference<a class="headerlink" href="#built-in-item-exporters-reference" title="Permalink to this heading">¶</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you’re exporting these two items:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;Color TV&quot;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s2">&quot;1200&quot;</span><span class="p">)</span>
<span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;DVD player&quot;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s2">&quot;200&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="baseitemexporter">
<h5>BaseItemExporter<a class="headerlink" href="#baseitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">BaseItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fields_to_export</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export_empty_fields</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'utf-8'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_fail</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.</p>
<p>These features can be configured through the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method arguments which
populate their respective instance attributes: <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal notranslate"><span class="pre">fields_to_export</span></code></a>,
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="scrapy.exporters.BaseItemExporter.export_empty_fields"><code class="xref py py-attr docutils literal notranslate"><span class="pre">export_empty_fields</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoding</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.indent" title="scrapy.exporters.BaseItemExporter.indent"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indent</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.0: </span>The <em>dont_fail</em> parameter.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.export_item">
<span class="sig-name descname"><span class="pre">export_item</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the given item. This method must be implemented in subclasses.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.serialize_field">
<span class="sig-name descname"><span class="pre">serialize_field</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">field</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.</p>
<p>By default, this method looks for a serializer <a class="hxr-hoverxref hxr-tooltip reference internal" href="#topics-exporters-serializers"><span class="std std-ref">declared in the item
field</span></a> and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>field</strong> (<a class="reference internal" href="index.html#scrapy.Field" title="scrapy.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> object or a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> instance) – the field being serialized. If the source <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item object</span></a> does not define field metadata, <em>field</em> is an empty
<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the name of the field being serialized</p></li>
<li><p><strong>value</strong> – the value being serialized</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.start_exporting">
<span class="sig-name descname"><span class="pre">start_exporting</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>). You must call this method before exporting any
items.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.finish_exporting">
<span class="sig-name descname"><span class="pre">finish_exporting</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>). You must always call this method after you
have no more items to export.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.fields_to_export">
<span class="sig-name descname"><span class="pre">fields_to_export</span></span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="Permalink to this definition">¶</a></dt>
<dd><p>Fields to export, their order <a class="footnote-reference brackets" href="#id3" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> and their output names.</p>
<p>Possible values are:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> (all fields <a class="footnote-reference brackets" href="#id4" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, default)</p></li>
<li><p>A list of fields:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;field1&#39;</span><span class="p">,</span> <span class="s1">&#39;field2&#39;</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>A dict where keys are fields and values are output names:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;field1&#39;</span><span class="p">:</span> <span class="s1">&#39;Field 1&#39;</span><span class="p">,</span> <span class="s1">&#39;field2&#39;</span><span class="p">:</span> <span class="s1">&#39;Field 2&#39;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id3" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Not all exporters respect the specified field order.</p>
</aside>
<aside class="footnote brackets" id="id4" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>When using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item objects</span></a> that do not expose
all their possible fields, exporters that do not support exporting
a different subset of fields per item will only export the fields
found in the first item exported.</p>
</aside>
</aside>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.export_empty_fields">
<span class="sig-name descname"><span class="pre">export_empty_fields</span></span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether to include empty/unpopulated item fields in the exported data.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a>)
ignore this attribute and always export all empty fields.</p>
<p>This option is ignored for dict items.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.encoding">
<span class="sig-name descname"><span class="pre">encoding</span></span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>The output character encoding.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.exporters.BaseItemExporter.indent">
<span class="sig-name descname"><span class="pre">indent</span></span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.indent" title="Permalink to this definition">¶</a></dt>
<dd><p>Amount of spaces used to indent the output on each level. Defaults to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">indent=None</span></code> selects the most compact representation,
all items in the same line with no indentation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">indent&lt;=0</span></code> each item on its own line, no indentation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">indent&gt;0</span></code> each item on its own line, indented with the provided numeric value</p></li>
</ul>
</dd></dl>

</dd></dl>

</section>
<section id="pythonitemexporter">
<h5>PythonItemExporter<a class="headerlink" href="#pythonitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.PythonItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">PythonItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dont_fail</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PythonItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a base class for item exporters that extends
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> with support for nested items.</p>
<p>It serializes items to built-in Python types, so that any serialization
library (e.g. <a class="reference external" href="https://docs.python.org/3/library/json.html#module-json" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">json</span></code></a> or <a class="reference external" href="https://pypi.org/project/msgpack/">msgpack</a>) can be used on top of it.</p>
</dd></dl>

</section>
<section id="xmlitemexporter">
<h5>XmlItemExporter<a class="headerlink" href="#xmlitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.XmlItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">XmlItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">item_element</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'item'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">root_element</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'items'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.XmlItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in XML format to the specified file object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p></li>
<li><p><strong>root_element</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The name of root element in the exported XML.</p></li>
<li><p><strong>item_element</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The name of each item element in the exported XML.</p></li>
</ul>
</dd>
</dl>
<p>The additional keyword arguments of this <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;Color TV&lt;/name&gt;
    &lt;price&gt;1200&lt;/price&gt;
 &lt;/item&gt;
  &lt;item&gt;
    &lt;name&gt;DVD player&lt;/name&gt;
    &lt;price&gt;200&lt;/price&gt;
 &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
<p>Unless overridden in the <code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code> method, multi-valued fields are
exported by serializing each value inside a <code class="docutils literal notranslate"><span class="pre">&lt;value&gt;</span></code> element. This is for
convenience, as multi-valued fields are very common.</p>
<p>For example, the item:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Item(name=[&#39;John&#39;, &#39;Doe&#39;], age=&#39;23&#39;)
</pre></div>
</div>
<p>Would be serialized as:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;
      &lt;value&gt;John&lt;/value&gt;
      &lt;value&gt;Doe&lt;/value&gt;
    &lt;/name&gt;
    &lt;age&gt;23&lt;/age&gt;
  &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
</dd></dl>

</section>
<section id="csvitemexporter">
<h5>CsvItemExporter<a class="headerlink" href="#csvitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.CsvItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">CsvItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_headers_line</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">join_multivalued</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">','</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">errors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.CsvItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in CSV format to the given file-like object. If the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">fields_to_export</span></code> attribute is set, it will be used to define the
CSV columns, their order and their column names. The
<code class="xref py py-attr docutils literal notranslate"><span class="pre">export_empty_fields</span></code> attribute has no effect on this exporter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p></li>
<li><p><strong>include_headers_line</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – If enabled, makes the exporter output a header
line with the field names taken from
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BaseItemExporter.fields_to_export</span></code></a> or the first exported item fields.</p></li>
<li><p><strong>join_multivalued</strong> – The char (or chars) that will be used for joining
multi-valued fields, if found.</p></li>
<li><p><strong>errors</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The optional string that specifies how encoding and decoding
errors are to be handled. For more information see
<a class="reference external" href="https://docs.python.org/3/library/io.html#io.TextIOWrapper" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">io.TextIOWrapper</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>The additional keyword arguments of this <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, and the leftover arguments to the
<a class="reference external" href="https://docs.python.org/3/library/csv.html#csv.writer" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">csv.writer()</span></code></a> function, so you can use any <a class="reference external" href="https://docs.python.org/3/library/csv.html#csv.writer" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">csv.writer()</span></code></a> function
argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>product,price
Color TV,1200
DVD player,200
</pre></div>
</div>
</dd></dl>

</section>
<section id="pickleitemexporter">
<h5>PickleItemExporter<a class="headerlink" href="#pickleitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.PickleItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">PickleItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">protocol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PickleItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in pickle format to the given file-like object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p></li>
<li><p><strong>protocol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The pickle protocol to use.</p></li>
</ul>
</dd>
</dl>
<p>For more information, see <a class="reference external" href="https://docs.python.org/3/library/pickle.html#module-pickle" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pickle</span></code></a>.</p>
<p>The additional keyword arguments of this <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>Pickle isn’t a human readable format, so no output examples are provided.</p>
</dd></dl>

</section>
<section id="pprintitemexporter">
<h5>PprintItemExporter<a class="headerlink" href="#pprintitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.PprintItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">PprintItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PprintItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in pretty print format to the specified file object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p>
</dd>
</dl>
<p>The additional keyword arguments of this <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;name&#39;: &#39;Color TV&#39;, &#39;price&#39;: &#39;1200&#39;}
{&#39;name&#39;: &#39;DVD player&#39;, &#39;price&#39;: &#39;200&#39;}
</pre></div>
</div>
<p>Longer lines (when present) are pretty-formatted.</p>
</dd></dl>

</section>
<section id="jsonitemexporter">
<h5>JsonItemExporter<a class="headerlink" href="#jsonitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.JsonItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">JsonItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in JSON format to the specified file-like object, writing all
objects as a list of objects. The additional <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method arguments are
passed to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, and the leftover
arguments to the <a class="reference external" href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONEncoder</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, so you can use any
<a class="reference external" href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONEncoder</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method argument to customize this exporter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p>
</dd>
</dl>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
</pre></div>
</div>
<div class="admonition warning" id="json-with-large-data">
<p class="admonition-title">Warning</p>
<p>JSON is very simple and flexible serialization format, but it
doesn’t scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream-friendly format, consider using <a class="reference internal" href="#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonLinesItemExporter</span></code></a>
instead, or splitting the output in multiple chunks.</p>
</div>
</dd></dl>

</section>
<section id="jsonlinesitemexporter">
<h5>JsonLinesItemExporter<a class="headerlink" href="#jsonlinesitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.JsonLinesItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">JsonLinesItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonLinesItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in JSON format to the specified file-like object, writing one
JSON-encoded item per line. The additional <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method arguments are passed
to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, and the leftover arguments to
the <a class="reference external" href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONEncoder</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, so you can use any
<a class="reference external" href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONEncoder</span></code></a> <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method argument to customize this exporter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</p>
</dd>
</dl>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
</pre></div>
</div>
<p>Unlike the one produced by <a class="reference internal" href="#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>, the format produced by
this exporter is well suited for serializing large amounts of data.</p>
</dd></dl>

</section>
<section id="marshalitemexporter">
<h5>MarshalItemExporter<a class="headerlink" href="#marshalitemexporter" title="Permalink to this heading">¶</a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.exporters.MarshalItemExporter">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.exporters.</span></span><span class="sig-name descname"><span class="pre">MarshalItemExporter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">BytesIO</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.MarshalItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports items in a Python-specific binary format (see
<a class="reference external" href="https://docs.python.org/3/library/marshal.html#module-marshal" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">marshal</span></code></a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>file</strong> – The file-like object to use for exporting the data. Its
<code class="docutils literal notranslate"><span class="pre">write</span></code> method should accept <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes</span></code></a> (a disk file
opened in binary mode, a <a class="reference external" href="https://docs.python.org/3/library/io.html#io.BytesIO" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">BytesIO</span></code></a> object, etc)</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<span id="document-topics/components"></span><section id="components">
<span id="topics-components"></span><h3>Components<a class="headerlink" href="#components" title="Permalink to this heading">¶</a></h3>
<p>A Scrapy component is any class whose objects are built using
<a class="reference internal" href="#scrapy.utils.misc.build_from_crawler" title="scrapy.utils.misc.build_from_crawler"><code class="xref py py-func docutils literal notranslate"><span class="pre">build_from_crawler()</span></code></a>.</p>
<p>That includes the classes that you may assign to the following settings:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_RESOLVER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_RESOLVER</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENTCONTEXTFACTORY</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORAGES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DISK_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DISK_QUEUE</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_MEMORY_QUEUE</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a></p></li>
</ul>
<p>Third-party Scrapy components may also let you define additional Scrapy
components, usually configurable through <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>, to
modify their behavior.</p>
<section id="enforcing-component-requirements">
<span id="enforce-component-requirements"></span><h4>Enforcing component requirements<a class="headerlink" href="#enforcing-component-requirements" title="Permalink to this heading">¶</a></h4>
<p>Sometimes, your components may only be intended to work under certain
conditions. For example, they may require a minimum version of Scrapy to work as
intended, or they may require certain settings to have specific values.</p>
<p>In addition to describing those conditions in the documentation of your
component, it is a good practice to raise an exception from the <code class="docutils literal notranslate"><span class="pre">__init__</span></code>
method of your component if those conditions are not met at run time.</p>
<p>In the case of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middlewares</span></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a>, and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a>, you should raise
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal notranslate"><span class="pre">scrapy.exceptions.NotConfigured</span></code></a>, passing a description of the issue as a
parameter to the exception so that it is printed in the logs, for the user to
see. For other components, feel free to raise whatever other exception feels
right to you; for example, <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">RuntimeError</span></code></a> would make sense for a Scrapy
version mismatch, while <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code></a> may be better if the issue is the
value of a setting.</p>
<p>If your requirement is a minimum Scrapy version, you may use
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scrapy.__version__</span></code> to enforce your requirement. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">packaging.version</span> <span class="kn">import</span> <span class="n">parse</span> <span class="k">as</span> <span class="n">parse_version</span>

<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MyComponent</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">parse_version</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">parse_version</span><span class="p">(</span><span class="s2">&quot;2.7&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">MyComponent</span><span class="o">.</span><span class="vm">__qualname__</span><span class="si">}</span><span class="s2"> requires Scrapy 2.7 or &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;later, which allow defining the process_spider_output &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;method of spider middlewares as an asynchronous &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;generator.&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="api-reference">
<h4>API reference<a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h4>
<p>The following function can be used to create an instance of a component class:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.misc.build_from_crawler">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.misc.</span></span><span class="sig-name descname"><span class="pre">build_from_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objcls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crawler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">/</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#scrapy.utils.misc.build_from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a class instance using its <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> or <code class="docutils literal notranslate"><span class="pre">from_settings</span></code> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">*args</span></code> and <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> are forwarded to the constructor.</p>
<p>Raises <code class="docutils literal notranslate"><span class="pre">TypeError</span></code> if the resulting instance is <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd></dl>

<p>The following function can also be useful when implementing a component, to
report the import path of the component class, e.g. when reporting problems:</p>
<dl class="py function">
<dt class="sig sig-object py" id="scrapy.utils.python.global_object_name">
<span class="sig-prename descclassname"><span class="pre">scrapy.utils.python.</span></span><span class="sig-name descname"><span class="pre">global_object_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></span><a class="headerlink" href="#scrapy.utils.python.global_object_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the full import path of the given class.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Request</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">global_object_name</span><span class="p">(</span><span class="n">Request</span><span class="p">)</span>
<span class="go">&#39;scrapy.http.request.Request&#39;</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>
<span id="document-topics/api"></span><section id="core-api">
<span id="topics-api"></span><h3>Core API<a class="headerlink" href="#core-api" title="Permalink to this heading">¶</a></h3>
<p>This section documents the Scrapy core API, and it’s intended for developers of
extensions and middlewares.</p>
<section id="crawler-api">
<span id="topics-api-crawler"></span><h4>Crawler API<a class="headerlink" href="#crawler-api" title="Permalink to this heading">¶</a></h4>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object, passed to extensions through the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method. This
object provides access to all Scrapy core components, and it’s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<span class="target" id="module-scrapy.crawler"></span><p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it’s configured through the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">configure the downloader middlewares</span></a>.</p>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.crawler.</span></span><span class="sig-name descname"><span class="pre">Crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spidercls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">scrapy.spiders.Spider</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">settings</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><span class="pre">Settings</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_reactor</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>The Crawler object must be instantiated with a
<a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Spider</span></code></a> subclass and a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.Settings</span></code></a> object.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.request_fingerprinter">
<span class="sig-name descname"><span class="pre">request_fingerprinter</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.request_fingerprinter" title="Permalink to this definition">¶</a></dt>
<dd><p>The request fingerprint builder of this crawler.</p>
<p>This is used from extensions and middlewares to build short, unique
identifiers for requests. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#request-fingerprints"><span class="std std-ref">Request fingerprints</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.settings">
<span class="sig-name descname"><span class="pre">settings</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.signals">
<span class="sig-name descname"><span class="pre">signals</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="Permalink to this definition">¶</a></dt>
<dd><p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">Signals</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalManager</span></code></a> class.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.stats">
<span class="sig-name descname"><span class="pre">stats</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a> class.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.extensions">
<span class="sig-name descname"><span class="pre">extensions</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="Permalink to this definition">¶</a></dt>
<dd><p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won’t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">Extensions</span></a>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.engine">
<span class="sig-name descname"><span class="pre">engine</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="Permalink to this definition">¶</a></dt>
<dd><p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to inspect  or
modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.spider">
<span class="sig-name descname"><span class="pre">spider</span></span><a class="headerlink" href="#scrapy.crawler.Crawler.spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Spider currently being crawled. This is an instance of the spider class
provided while constructing the crawler, and it is created after the
arguments given in the <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.crawl">
<span class="sig-name descname"><span class="pre">crawl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts the crawler by instantiating its spider class with the given
<code class="docutils literal notranslate"><span class="pre">args</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> arguments, while setting the execution engine in
motion. Should be called only once.</p>
<p>Returns a deferred that is fired when the crawl is finished.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.stop">
<span class="sig-name descname"><span class="pre">stop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Generator</span><span class="p"><span class="pre">[</span></span><span class="pre">Deferred</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts a graceful stop of the crawler and returns a deferred that is
fired when the crawler is stopped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.get_addon">
<span class="sig-name descname"><span class="pre">get_addon</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_T</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.get_addon" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the run-time instance of an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-addons"><span class="std std-ref">add-on</span></a> of
the specified class or a subclass, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.get_downloader_middleware">
<span class="sig-name descname"><span class="pre">get_downloader_middleware</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_T</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.get_downloader_middleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the run-time instance of a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middleware</span></a> of the specified class or a subclass,
or <code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
<p>This method can only be called after the crawl engine has been created,
e.g. at signals <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.get_extension">
<span class="sig-name descname"><span class="pre">get_extension</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_T</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.get_extension" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the run-time instance of an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extension</span></a> of the specified class or a subclass,
or <code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
<p>This method can only be called after the extension manager has been
created, e.g. at signals <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.get_item_pipeline">
<span class="sig-name descname"><span class="pre">get_item_pipeline</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_T</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.get_item_pipeline" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the run-time instance of a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipeline</span></a> of the specified class or a subclass, or
<code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
<p>This method can only be called after the crawl engine has been created,
e.g. at signals <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.Crawler.get_spider_middleware">
<span class="sig-name descname"><span class="pre">get_spider_middleware</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_T</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">_T</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.Crawler.get_spider_middleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the run-time instance of a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middleware</span></a> of the specified class or a subclass, or
<code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 2.12.</span></p>
</div>
<p>This method can only be called after the crawl engine has been created,
e.g. at signals <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.crawler.</span></span><span class="sig-name descname"><span class="pre">CrawlerRunner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><span class="pre">Settings</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a convenient helper class that keeps track of, manages and runs
crawlers inside an already setup <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a>.</p>
<p>The CrawlerRunner object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner.crawl">
<span class="sig-name descname"><span class="pre">crawl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler_or_spidercls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">scrapy.spiders.Spider</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</p></li>
<li><p><strong>args</strong> – arguments to initialize the spider</p></li>
<li><p><strong>kwargs</strong> – keyword arguments to initialize the spider</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner.crawlers">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">crawlers</span></span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner.create_crawler">
<span class="sig-name descname"><span class="pre">create_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler_or_spidercls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">scrapy.spiders.Spider</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler
is constructed for it.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner.join">
<span class="sig-name descname"><span class="pre">join</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawlers" title="scrapy.crawler.CrawlerRunner.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerRunner.stop">
<span class="sig-name descname"><span class="pre">stop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.crawler.</span></span><span class="sig-name descname"><span class="pre">CrawlerProcess</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><span class="pre">Settings</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">install_root_handler</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a></p>
<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>
<p>This class extends <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> by adding support
for starting a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> and handling shutdown
signals, like the keyboard interrupt command Ctrl-C. It also configures
top-level logging.</p>
<p>This utility should be a better fit than
<a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> if you aren’t running another
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> within your application.</p>
<p>The CrawlerProcess object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>install_root_handler</strong> – whether to install root logging handler
(default: True)</p>
</dd>
</dl>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.crawl">
<span class="sig-name descname"><span class="pre">crawl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler_or_spidercls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">scrapy.spiders.Spider</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</p></li>
<li><p><strong>args</strong> – arguments to initialize the spider</p></li>
<li><p><strong>kwargs</strong> – keyword arguments to initialize the spider</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.crawlers">
<em class="property"><span class="pre">property</span><span class="w">  </span></em><span class="sig-name descname"><span class="pre">crawlers</span></span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.create_crawler">
<span class="sig-name descname"><span class="pre">create_crawler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">crawler_or_spidercls</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><span class="pre">type</span></a><span class="p"><span class="pre">[</span></span><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><span class="pre">scrapy.spiders.Spider</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><span class="pre">Crawler</span></a></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler
is constructed for it.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.join">
<span class="sig-name descname"><span class="pre">join</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawlers" title="scrapy.crawler.CrawlerProcess.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.start">
<span class="sig-name descname"><span class="pre">start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stop_after_crawl</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">install_signal_handlers</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.start" title="Permalink to this definition">¶</a></dt>
<dd><p>This method starts a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a>, adjusts its pool
size to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache
based on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_SIZE</span></code></a>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">stop_after_crawl</span></code> is True, the reactor will be stopped after all
crawlers have finished, using <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.join" title="scrapy.crawler.CrawlerProcess.join"><code class="xref py py-meth docutils literal notranslate"><span class="pre">join()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stop_after_crawl</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – stop or not the reactor when all
crawlers have finished</p></li>
<li><p><strong>install_signal_handlers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – whether to install the OS signal
handlers from Twisted and Scrapy (default: True)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.crawler.CrawlerProcess.stop">
<span class="sig-name descname"><span class="pre">stop</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><span class="pre">Deferred</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h4>Settings API<a class="headerlink" href="#module-scrapy.settings" title="Permalink to this heading">¶</a></h4>
<dl class="py attribute">
<dt class="sig sig-object py" id="scrapy.settings.SETTINGS_PRIORITIES">
<span class="sig-prename descclassname"><span class="pre">scrapy.settings.</span></span><span class="sig-name descname"><span class="pre">SETTINGS_PRIORITIES</span></span><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;command&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;addon&quot;</span><span class="p">:</span> <span class="mi">15</span><span class="p">,</span>
    <span class="s2">&quot;project&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s2">&quot;spider&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;cmdline&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="scrapy.settings.get_settings_priority">
<span class="sig-prename descclassname"><span class="pre">scrapy.settings.</span></span><span class="sig-name descname"><span class="pre">get_settings_priority</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Small helper function that looks up a given string priority in the
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its
numerical value, or directly returns a given numerical priority.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.settings.Settings">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.settings.</span></span><span class="sig-name descname"><span class="pre">Settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">_SettingsInputT</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'project'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a></p>
<p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>It is a direct subclass and supports all methods of
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>. Additionally, after instantiation
of this class, the new object will have the global default settings
described on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> already populated.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.settings.</span></span><span class="sig-name descname"><span class="pre">BaseSettings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">_SettingsInputT</span></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'project'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instances of this class behave like dictionaries, but store priorities
along with their <code class="docutils literal notranslate"><span class="pre">(key,</span> <span class="pre">value)</span></code> pairs, and can be frozen (i.e. marked
immutable).</p>
<p>Key-value entries can be passed on initialization with the <code class="docutils literal notranslate"><span class="pre">values</span></code>
argument, and they would take the <code class="docutils literal notranslate"><span class="pre">priority</span></code> level (unless <code class="docutils literal notranslate"><span class="pre">values</span></code> is
already an instance of <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>, in which
case the existing priority levels will be kept).  If the <code class="docutils literal notranslate"><span class="pre">priority</span></code>
argument is a string, the priority name will be looked up in
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer
should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method, and can be accessed with
the square bracket notation of dictionaries, or with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a> method of the instance and its
value conversion variants. When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.copy">
<span class="sig-name descname"><span class="pre">copy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a deep copy of current settings.</p>
<p>This method returns a new instance of the <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class,
populated with the same values and their priorities.</p>
<p>Modifications to the new object won’t be reflected on the original
settings.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.copy_to_dict">
<span class="sig-name descname"><span class="pre">copy_to_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.13)"><span class="pre">Union</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">NoneType</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy_to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a copy of current settings and convert to a dict.</p>
<p>This method returns a new dict populated with the same values
and their priorities as the current settings.</p>
<p>Modifications to the returned dict won’t be reflected on the original
settings.</p>
<p>This method can be useful for example for printing settings
in Scrapy shell.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable further changes to the current settings.</p>
<p>After calling this method, the present state of the settings will become
immutable. Trying to change values through the <a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method and
its variants won’t be possible and will be alerted.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.frozencopy">
<span class="sig-name descname"><span class="pre">frozencopy</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Self</span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.frozencopy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an immutable copy of the current settings.</p>
<p>Alias for a <a class="reference internal" href="#scrapy.settings.BaseSettings.freeze" title="scrapy.settings.BaseSettings.freeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">freeze()</span></code></a> call in the object returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.copy" title="scrapy.settings.BaseSettings.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value without affecting its original type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getbool">
<span class="sig-name descname"><span class="pre">getbool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getbool" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a boolean.</p>
<p><code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">'1'</span></code>, <cite>True`</cite> and <code class="docutils literal notranslate"><span class="pre">'True'</span></code> return <code class="docutils literal notranslate"><span class="pre">True</span></code>,
while <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">'0'</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">'False'</span></code> and <code class="docutils literal notranslate"><span class="pre">None</span></code> return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'0'</span></code> will return <code class="docutils literal notranslate"><span class="pre">False</span></code> when using this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getdict">
<span class="sig-name descname"><span class="pre">getdict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a dictionary. If the setting original type is a
dictionary, a copy of it will be returned. If it is a string it will be
evaluated as a JSON dictionary. In the case that it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance itself, it will be
converted to a dictionary, containing all its current settings values
as they would be returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a>,
and losing all information about priority and mutability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getdictorlist">
<span class="sig-name descname"><span class="pre">getdictorlist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getdictorlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as either a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> or a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a>.</p>
<p>If the setting is already a dict or a list, a copy of it will be
returned.</p>
<p>If it is a string it will be evaluated as JSON, or as a comma-separated
list of strings as a fallback.</p>
<p>For example, settings populated from the command line will return:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">{'key1':</span> <span class="pre">'value1',</span> <span class="pre">'key2':</span> <span class="pre">'value2'}</span></code> if set to
<code class="docutils literal notranslate"><span class="pre">'{&quot;key1&quot;:</span> <span class="pre">&quot;value1&quot;,</span> <span class="pre">&quot;key2&quot;:</span> <span class="pre">&quot;value2&quot;}'</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">['one',</span> <span class="pre">'two']</span></code> if set to <code class="docutils literal notranslate"><span class="pre">'[&quot;one&quot;,</span> <span class="pre">&quot;two&quot;]'</span></code> or <code class="docutils literal notranslate"><span class="pre">'one,two'</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – the setting name</p></li>
<li><p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getfloat">
<span class="sig-name descname"><span class="pre">getfloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getfloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a float.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getint">
<span class="sig-name descname"><span class="pre">getint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getint" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as an int.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getlist">
<span class="sig-name descname"><span class="pre">getlist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a list. If the setting original type is a list, a
copy of it will be returned. If it’s a string it will be split by “,”.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'one,two'</span></code> will return a list [‘one’, ‘two’] when using this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>default</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to return if no setting is found</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getpriority">
<span class="sig-name descname"><span class="pre">getpriority</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the current numerical priority value of a setting, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
the given <code class="docutils literal notranslate"><span class="pre">name</span></code> does not exist.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.getwithbase">
<span class="sig-name descname"><span class="pre">getwithbase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="index.html#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><span class="pre">BaseSettings</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.getwithbase" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>
counterpart.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – name of the dictionary-like setting</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.maxpriority">
<span class="sig-name descname"><span class="pre">maxpriority</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.maxpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the numerical value of the highest priority present throughout
all settings, or the numerical value for <code class="docutils literal notranslate"><span class="pre">default</span></code> from
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> if there are no settings
stored.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.pop">
<span class="sig-name descname"><span class="pre">pop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="optional">]</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">v,</span> <span class="pre">remove</span> <span class="pre">specified</span> <span class="pre">key</span> <span class="pre">and</span> <span class="pre">return</span> <span class="pre">the</span> <span class="pre">corresponding</span> <span class="pre">value.</span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.pop" title="Permalink to this definition">¶</a></dt>
<dd><p>If key is not found, d is returned if given, otherwise KeyError is raised.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.set">
<span class="sig-name descname"><span class="pre">set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'project'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure()</span></code> method),
otherwise they won’t have any effect.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the setting name</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the value to associate with the setting</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the priority of the setting. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.setdefault">
<span class="sig-name descname"><span class="pre">setdefault</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em><span class="optional">[</span>, <em class="sig-param"><span class="n"><span class="pre">d</span></span></em><span class="optional">]</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">D.get(k,d),</span> <span class="pre">also</span> <span class="pre">set</span> <span class="pre">D[k]=d</span> <span class="pre">if</span> <span class="pre">k</span> <span class="pre">not</span> <span class="pre">in</span> <span class="pre">D</span></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.setdefault" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.setmodule">
<span class="sig-name descname"><span class="pre">setmodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">ModuleType</span><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'project'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.setmodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every globally declared
uppercase variable of <code class="docutils literal notranslate"><span class="pre">module</span></code> with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference external" href="https://docs.python.org/3/library/types.html#types.ModuleType" title="(in Python v3.13)"><em>types.ModuleType</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – the module or the path of the module</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.settings.BaseSettings.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">_SettingsInputT</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priority</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w">  </span><span class="p"><span class="pre">|</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">'project'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.settings.BaseSettings.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every item of <code class="docutils literal notranslate"><span class="pre">values</span></code>
with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">values</span></code> is a string, it is assumed to be JSON-encoded and parsed
into a dict with <code class="docutils literal notranslate"><span class="pre">json.loads()</span></code> first. If it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance, the per-key priorities
will be used and the <code class="docutils literal notranslate"><span class="pre">priority</span></code> parameter ignored. This allows
inserting/updating settings with different priorities with a single
command.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>values</strong> (dict or string or <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>) – the settings names and values</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-scrapy.spiderloader">
<span id="spiderloader-api"></span><span id="topics-api-spiderloader"></span><h4>SpiderLoader API<a class="headerlink" href="#module-scrapy.spiderloader" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.spiderloader.SpiderLoader">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.spiderloader.</span></span><span class="sig-name descname"><span class="pre">SpiderLoader</span></span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is in charge of retrieving and handling the spider classes
defined across the project.</p>
<p>Custom spider loaders can be employed by specifying their path in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement
the <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an
errorless execution.</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiderloader.SpiderLoader.from_settings">
<span class="sig-name descname"><span class="pre">from_settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">settings</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This class method is used by Scrapy to create an instance of the class.
It’s called with the current project settings, and it loads the spiders
found recursively in the modules of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>
setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>settings</strong> (<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> instance) – project settings</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiderloader.SpiderLoader.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider_name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Spider class with the given name. It’ll look into the previously
loaded spiders for a spider class with name <code class="docutils literal notranslate"><span class="pre">spider_name</span></code> and will raise
a KeyError if not found.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spider_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – spider class name</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiderloader.SpiderLoader.list">
<span class="sig-name descname"><span class="pre">list</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the names of the available spiders in the project.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.spiderloader.SpiderLoader.find_by_request">
<span class="sig-name descname"><span class="pre">find_by_request</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.find_by_request" title="Permalink to this definition">¶</a></dt>
<dd><p>List the spiders’ names that can handle the given request. Will try to
match the request’s url against the domains of the spiders.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>request</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> instance) – queried request</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h4>Signals API<a class="headerlink" href="#module-scrapy.signalmanager" title="Permalink to this heading">¶</a></h4>
<dl class="py class">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.signalmanager.</span></span><span class="sig-name descname"><span class="pre">SignalManager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sender</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span><span class="w">  </span><span class="o"><span class="pre">=</span></span><span class="w">  </span><span class="default_value"><span class="pre">_Anonymous</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager.connect">
<span class="sig-name descname"><span class="pre">connect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">receiver</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">Signals</span></a>
section.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>receiver</strong> (<a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable" title="(in Python v3.13)"><em>collections.abc.Callable</em></a>) – the function to be connected</p></li>
<li><p><strong>signal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the signal to connect to</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager.disconnect">
<span class="sig-name descname"><span class="pre">disconnect</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">receiver</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">signal</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect a receiver function from a signal. This has the
opposite effect of the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method, and the arguments
are the same.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager.disconnect_all">
<span class="sig-name descname"><span class="pre">disconnect_all</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">signal</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect all receivers from the given signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>signal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><em>object</em></a>) – the signal to disconnect from</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager.send_catch_log">
<span class="sig-name descname"><span class="pre">send_catch_log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">signal</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w">  </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<span class="sig-name descname"><span class="pre">send_catch_log_deferred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">signal</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w">  </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Deferred</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span></span><span class="w">  </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="Permalink to this definition">¶</a></dt>
<dd><p>Like <a class="reference internal" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">send_catch_log()</span></code></a> but supports returning
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> objects from signal handlers.</p>
<p>Returns a Deferred that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

</dd></dl>

</section>
<section id="stats-collector-api">
<span id="topics-api-stats"></span><h4>Stats Collector API<a class="headerlink" href="#stats-collector-api" title="Permalink to this heading">¶</a></h4>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.statscollectors</span></code></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a>
class (which they all inherit from).</p>
<span class="target" id="module-scrapy.statscollectors"></span><dl class="py class">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector">
<em class="property"><span class="pre">class</span><span class="w">  </span></em><span class="sig-prename descclassname"><span class="pre">scrapy.statscollectors.</span></span><span class="sig-name descname"><span class="pre">StatsCollector</span></span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.get_value">
<span class="sig-name descname"><span class="pre">get_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the value for the given stats key or default if it doesn’t exist.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.get_stats">
<span class="sig-name descname"><span class="pre">get_stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all stats from the currently running spider as a dict.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.set_value">
<span class="sig-name descname"><span class="pre">set_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given stats key.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.set_stats">
<span class="sig-name descname"><span class="pre">set_stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stats</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Override the current stats with the dict passed in <code class="docutils literal notranslate"><span class="pre">stats</span></code> argument.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.inc_value">
<span class="sig-name descname"><span class="pre">inc_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.inc_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it’s not set).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.max_value">
<span class="sig-name descname"><span class="pre">max_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.max_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.min_value">
<span class="sig-name descname"><span class="pre">min_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.min_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.clear_stats">
<span class="sig-name descname"><span class="pre">clear_stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.clear_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all stats.</p>
</dd></dl>

<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.open_spider">
<span class="sig-name descname"><span class="pre">open_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Open the given spider for stats collection.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="scrapy.statscollectors.StatsCollector.close_spider">
<span class="sig-name descname"><span class="pre">close_spider</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">spider</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
</dd></dl>

</dd></dl>

</section>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-topics/architecture"><span class="doc">Architecture overview</span></a></dt><dd><p>Understand the Scrapy architecture.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/addons"><span class="doc">Add-ons</span></a></dt><dd><p>Enable and configure third-party extensions.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/downloader-middleware"><span class="doc">Downloader Middleware</span></a></dt><dd><p>Customize how pages get requested and downloaded.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/spider-middleware"><span class="doc">Spider Middleware</span></a></dt><dd><p>Customize the input and output of your spiders.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/extensions"><span class="doc">Extensions</span></a></dt><dd><p>Extend Scrapy with your custom functionality</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/signals"><span class="doc">Signals</span></a></dt><dd><p>See all available signals and how to work with them.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/scheduler"><span class="doc">Scheduler</span></a></dt><dd><p>Understand the scheduler component.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/exporters"><span class="doc">Item Exporters</span></a></dt><dd><p>Quickly export your scraped items to a file (XML, CSV, etc).</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/components"><span class="doc">Components</span></a></dt><dd><p>Learn the common API and some good practices when building custom Scrapy
components.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-topics/api"><span class="doc">Core API</span></a></dt><dd><p>Use it on extensions and middlewares to extend Scrapy functionality.</p>
</dd>
</dl>
</section>
<section id="all-the-rest">
<h2>All the rest<a class="headerlink" href="#all-the-rest" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-news"></span><section id="release-notes">
<span id="news"></span><h3>Release notes<a class="headerlink" href="#release-notes" title="Permalink to this heading">¶</a></h3>
<section id="scrapy-2-12-0-2024-11-18">
<span id="release-2-12-0"></span><h4>Scrapy 2.12.0 (2024-11-18)<a class="headerlink" href="#scrapy-2-12-0-2024-11-18" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Dropped support for Python 3.8, added support for Python 3.13</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> can now yield items</p></li>
<li><p>Added <a class="reference internal" href="index.html#scrapy.http.JsonResponse" title="scrapy.http.JsonResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonResponse</span></code></a></p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_PAGECOUNT_NO_ITEM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT_NO_ITEM</span></code></a></p></li>
</ul>
<section id="modified-requirements">
<h5>Modified requirements<a class="headerlink" href="#modified-requirements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Dropped support for Python 3.8.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6466">issue 6466</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6472">issue 6472</a>)</p></li>
<li><p>Added support for Python 3.13.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6166">issue 6166</a>)</p></li>
<li><p>Minimum versions increased for these dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://twisted.org/">Twisted</a>: 18.9.0 → 21.7.0</p></li>
<li><p><a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a>: 36.0.0 → 37.0.0</p></li>
<li><p><a class="reference external" href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>: 21.0.0 → 22.0.0</p></li>
<li><p><a class="reference external" href="https://lxml.de/">lxml</a>: 4.4.1 → 4.6.0</p></li>
</ul>
</li>
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">setuptools</span></code> from the dependency list.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6487">issue 6487</a>)</p></li>
</ul>
</section>
<section id="backward-incompatible-changes">
<h5>Backward-incompatible changes<a class="headerlink" href="#backward-incompatible-changes" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>User-defined cookies for HTTPS requests will have the <code class="docutils literal notranslate"><span class="pre">secure</span></code> flag set
to <code class="docutils literal notranslate"><span class="pre">True</span></code> unless it’s set to <code class="docutils literal notranslate"><span class="pre">False</span></code> explictly. This is important when
these cookies are reused in HTTP requests, e.g. after a redirect to an HTTP
URL.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6357">issue 6357</a>)</p></li>
<li><p>The Reppy-based <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> parser,
<code class="docutils literal notranslate"><span class="pre">scrapy.robotstxt.ReppyRobotParser</span></code>, was removed, as it doesn’t support
Python 3.9+.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5230">issue 5230</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6099">issue 6099</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6499">issue 6499</a>)</p></li>
<li><p>The initialization API of <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.media.MediaPipeline</span></code> and
its subclasses was improved and it’s possible that some previously working
usage scenarios will no longer work. It can only affect you if you define
custom subclasses of <code class="docutils literal notranslate"><span class="pre">MediaPipeline</span></code> or create instances of these
pipelines via <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> or <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> calls instead of
<code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> calls.</p>
<p>Previously, <code class="docutils literal notranslate"><span class="pre">MediaPipeline.from_crawler()</span></code> called the <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code>
method if it existed or the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method otherwise, and then did
some additional initialization using the <code class="docutils literal notranslate"><span class="pre">crawler</span></code> instance. If the
<code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> method existed (like in <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code>) it called
<code class="docutils literal notranslate"><span class="pre">__init__()</span></code> to create the instance. It wasn’t possible to override
<code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> without calling <code class="docutils literal notranslate"><span class="pre">MediaPipeline.from_crawler()</span></code> from it
which, in turn, couldn’t be called in some cases (including subclasses of
<code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code>).</p>
<p>Now, in line with the general usage of <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> and
<code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> and the deprecation of the latter the recommended
initialization order is the following one:</p>
<ul class="simple">
<li><p>All <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> methods should take a <code class="docutils literal notranslate"><span class="pre">crawler</span></code> argument. If they
also take a <code class="docutils literal notranslate"><span class="pre">settings</span></code> argument they should ignore it, using
<code class="docutils literal notranslate"><span class="pre">crawler.settings</span></code> instead. When they call <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> of the base
class they should pass the <code class="docutils literal notranslate"><span class="pre">crawler</span></code> argument to it too.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> method shouldn’t be defined. Class-specific
initialization code should go into either an overriden <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code>
method or into <code class="docutils literal notranslate"><span class="pre">__init__()</span></code>.</p></li>
<li><p>It’s now possible to override <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> and it’s not necessary
to call <code class="docutils literal notranslate"><span class="pre">MediaPipeline.from_crawler()</span></code> in it if other recommendations
were followed.</p></li>
<li><p>If pipeline instances were created with <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> or
<code class="docutils literal notranslate"><span class="pre">__init__()</span></code> calls (which wasn’t supported even before, as it missed
important initialization code), they should now be created with
<code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> calls.</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">response_body</span></code> argument of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.convert_image</span></code> is now
positional-only, as it was changed from optional to required.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">convert</span></code> argument of <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.conf.build_component_list()</span></code>
is now positional-only, as the preceding argument (<code class="docutils literal notranslate"><span class="pre">custom</span></code>) was removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">overwrite_output</span></code> argument of
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.conf.feed_process_params_from_cli()</span></code> is now
positional-only, as the preceding argument (<code class="docutils literal notranslate"><span class="pre">output_format</span></code>) was removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
</ul>
</section>
<section id="deprecation-removals">
<h5>Deprecation removals<a class="headerlink" href="#deprecation-removals" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_fingerprint()</span></code> function,
deprecated in Scrapy 2.7.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6212">issue 6212</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6213">issue 6213</a>)</p></li>
<li><p>Removed support for value <code class="docutils literal notranslate"><span class="pre">&quot;2.6&quot;</span></code> of setting
<code class="docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_IMPLEMENTATION</span></code>, deprecated in Scrapy 2.7.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6212">issue 6212</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6213">issue 6213</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> subclasses now require
supporting the <code class="docutils literal notranslate"><span class="pre">fingerprinter</span></code> parameter in their <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method,
introduced in Scrapy 2.7.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6102">issue 6102</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.decompression</span></code> module,
deprecated in Scrapy 2.7.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6100">issue 6100</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.utils.response.response_httprepr()</span></code> function,
deprecated in Scrapy 2.6.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6111">issue 6111</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6116">issue 6116</a>)</p></li>
<li><p>Spiders with spider-level HTTP authentication, i.e. with the <code class="docutils literal notranslate"><span class="pre">http_user</span></code>
or <code class="docutils literal notranslate"><span class="pre">http_pass</span></code> attributes, must now define <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> as well,
which was introduced in Scrapy 2.5.1.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6103">issue 6103</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">Media pipelines</span></a> methods <code class="docutils literal notranslate"><span class="pre">file_path()</span></code>,
<code class="docutils literal notranslate"><span class="pre">file_downloaded()</span></code>, <code class="docutils literal notranslate"><span class="pre">get_images()</span></code>, <code class="docutils literal notranslate"><span class="pre">image_downloaded()</span></code>,
<code class="docutils literal notranslate"><span class="pre">media_downloaded()</span></code>, <code class="docutils literal notranslate"><span class="pre">media_to_download()</span></code>, and <code class="docutils literal notranslate"><span class="pre">thumb_path()</span></code> must
now support an <code class="docutils literal notranslate"><span class="pre">item</span></code> parameter, added in Scrapy 2.4.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6107">issue 6107</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> and <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> methods of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage"><span class="std std-ref">feed storage
backend classes</span></a> must now support the keyword-only
<code class="docutils literal notranslate"><span class="pre">feed_options</span></code> parameter, introduced in Scrapy 2.4.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6105">issue 6105</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.loader.common</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.loader.processors</span></code>
modules, deprecated in Scrapy 2.3.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6106">issue 6106</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.utils.misc.extract_regex()</span></code> function, deprecated in
Scrapy 2.3.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6106">issue 6106</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.http.JSONRequest</span></code> class, replaced with
<code class="docutils literal notranslate"><span class="pre">JsonRequest</span></code> in Scrapy 1.8.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6110">issue 6110</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.log.logformatter_adapter</span></code> no longer supports missing
<code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">level</span></code>, or <code class="docutils literal notranslate"><span class="pre">msg</span></code> parameters, and no longer supports a
<code class="docutils literal notranslate"><span class="pre">format</span></code> parameter, all scenarios that were deprecated in Scrapy 1.0.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6109">issue 6109</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6116">issue 6116</a>)</p></li>
<li><p>A custom class assigned to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> setting that
does not implement the <code class="xref py py-class docutils literal notranslate"><span class="pre">ISpiderLoader</span></code> interface
will now raise a <code class="xref py py-exc docutils literal notranslate"><span class="pre">zope.interface.verify.DoesNotImplement</span></code> exception at
run time. Non-compliant classes have been triggering a deprecation warning
since Scrapy 1.0.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6101">issue 6101</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6113">issue 6113</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">--output-format</span></code>/<code class="docutils literal notranslate"><span class="pre">-t</span></code> command line option, deprecated in
Scrapy 2.1.0. <code class="docutils literal notranslate"><span class="pre">-O</span> <span class="pre">&lt;URI&gt;:&lt;FORMAT&gt;</span></code> should be used instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Running <a class="reference internal" href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> more than once on the same
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance, deprecated in Scrapy 2.11.0, now
raises an exception.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Subclassing
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a>
without support for the <code class="docutils literal notranslate"><span class="pre">crawler</span></code> argument in <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> and without
a custom <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> method, deprecated in Scrapy 2.5.0, is no
longer allowed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">EXCEPTIONS_TO_RETRY</span></code> attribute of
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a>, deprecated in
Scrapy 2.10.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed support for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed exports</span></a> without
the <a class="reference external" href="https://github.com/boto/boto3">boto3</a> package installed, deprecated in Scrapy 2.10.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.feedexport._FeedSlot</span></code> class, deprecated in
Scrapy 2.10.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.pipelines.images.NoimagesDrop</span></code> exception, deprecated
in Scrapy 2.8.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">response_body</span></code> argument of <code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.convert_image</span></code> is now required,
not passing it was deprecated in Scrapy 2.8.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">custom</span></code> argument of
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.conf.build_component_list()</span></code>, deprecated in Scrapy
2.10.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">scrapy.utils.reactor.get_asyncio_event_loop_policy()</span></code>
function, deprecated in Scrapy 2.9.0. Use <a class="reference external" href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_event_loop" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">asyncio.get_event_loop()</span></code></a>
and related standard library functions instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6500">issue 6500</a>)</p></li>
</ul>
</section>
<section id="deprecations">
<h5>Deprecations<a class="headerlink" href="#deprecations" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> methods of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-components"><span class="std std-ref">Scrapy components</span></a> that have them are now deprecated. <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code>
should now be used instead. Affected components:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.dupefilters.RFPDupeFilter</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.mail.MailSender</span></code></a></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.middleware.MiddlewareManager</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.files.FilesPipeline</span></code></a></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.images.ImagesPipeline</span></code></a></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.urllength.UrlLengthMiddleware</span></code></a></p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p>
</li>
<li><p>It’s now deprecated to have a <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> method but no
<code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> method in 3rd-party <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-components"><span class="std std-ref">Scrapy components</span></a>. You can define a simple <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code> method
that calls <code class="docutils literal notranslate"><span class="pre">cls.from_settings(crawler.settings)</span></code> to fix this if you don’t
want to refactor the code. Note that if you have a <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code>
method Scrapy will not call the <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> method so the latter
can be removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p></li>
<li><p>The initialization API of <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.media.MediaPipeline</span></code> and
its subclasses was improved and some old usage scenarios are now deprecated
(see also the “Backward-incompatible changes” section). Specifically:</p>
<ul class="simple">
<li><p>It’s deprecated to define an <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method that doesn’t take a
<code class="docutils literal notranslate"><span class="pre">crawler</span></code> argument.</p></li>
<li><p>It’s deprecated to call an <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method without passing a
<code class="docutils literal notranslate"><span class="pre">crawler</span></code> argument. If it’s passed, it’s also deprecated to pass a
<code class="docutils literal notranslate"><span class="pre">settings</span></code> argument, which will be ignored anyway.</p></li>
<li><p>Calling <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> is deprecated, use <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code>
instead.</p></li>
<li><p>Overriding <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> is deprecated, override <code class="docutils literal notranslate"><span class="pre">from_crawler()</span></code>
instead.</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_IMPLEMENTATION</span></code> setting is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6212">issue 6212</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6213">issue 6213</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.misc.create_instance()</span></code> function is now deprecated, use
<a class="reference internal" href="index.html#scrapy.utils.misc.build_from_crawler" title="scrapy.utils.misc.build_from_crawler"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.misc.build_from_crawler()</span></code></a> instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5523">issue 5523</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5884">issue 5884</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6162">issue 6162</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6169">issue 6169</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.core.downloader.Downloader._get_slot_key()</span></code> is deprecated, use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.core.downloader.Downloader.get_slot_key()</span></code> instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6340">issue 6340</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6352">issue 6352</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.defer.process_chain_both()</span></code> is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6397">issue 6397</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.twisted_version</span></code> is now deprecated, you should instead use
<code class="xref py py-attr docutils literal notranslate"><span class="pre">twisted.version</span></code> directly (but note that it’s an
<code class="docutils literal notranslate"><span class="pre">incremental.Version</span></code> object, not a tuple).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6509">issue 6509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6512">issue 6512</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.flatten()</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.iflatten()</span></code>
are now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6517">issue 6517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6519">issue 6519</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.equal_attributes()</span></code> is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6517">issue 6517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6519">issue 6519</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_authenticate()</span></code> is now deprecated, you
should instead just set the <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header directly.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6517">issue 6517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6519">issue 6519</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.serialize.ScrapyJSONDecoder</span></code> is now deprecated, it didn’t
contain any code since Scrapy 1.0.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6517">issue 6517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6519">issue 6519</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.test.assert_samelines()</span></code> is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6517">issue 6517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6519">issue 6519</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.extensions.feedexport.build_storage()</span></code> is now deprecated. You can
instead call the builder callable directly.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6540">issue 6540</a>)</p></li>
</ul>
</section>
<section id="new-features">
<h5>New features<a class="headerlink" href="#new-features" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> can now yield items.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5289">issue 5289</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6417">issue 6417</a>)</p></li>
<li><p>Added a new <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> subclass,
<a class="reference internal" href="index.html#scrapy.http.JsonResponse" title="scrapy.http.JsonResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonResponse</span></code></a>, for responses with a <a class="reference external" href="https://mimesniff.spec.whatwg.org/#json-mime-type">JSON MIME type</a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6069">issue 6069</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6171">issue 6171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6174">issue 6174</a>)</p></li>
<li><p>The <a class="reference internal" href="index.html#scrapy.extensions.logstats.LogStats" title="scrapy.extensions.logstats.LogStats"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogStats</span></code></a> extension now adds
<code class="docutils literal notranslate"><span class="pre">items_per_minute</span></code> and <code class="docutils literal notranslate"><span class="pre">responses_per_minute</span></code> to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">stats</span></a> when the spider closes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4110">issue 4110</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4111">issue 4111</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_PAGECOUNT_NO_ITEM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT_NO_ITEM</span></code></a> which allows closing the
spider if no items were scraped in a set amount of time.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6434">issue 6434</a>)</p></li>
<li><p>User-defined cookies can now include the <code class="docutils literal notranslate"><span class="pre">secure</span></code> field.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6357">issue 6357</a>)</p></li>
<li><p>Added component getters to <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>:
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.get_addon" title="scrapy.crawler.Crawler.get_addon"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_addon()</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.get_downloader_middleware" title="scrapy.crawler.Crawler.get_downloader_middleware"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_downloader_middleware()</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.get_extension" title="scrapy.crawler.Crawler.get_extension"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_extension()</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.get_item_pipeline" title="scrapy.crawler.Crawler.get_item_pipeline"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_item_pipeline()</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.get_spider_middleware" title="scrapy.crawler.Crawler.get_spider_middleware"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_spider_middleware()</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6181">issue 6181</a>)</p></li>
<li><p>Slot delay updates by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a> based on response latencies can now be disabled for
specific requests via the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-autothrottle_dont_adjust_delay"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">autothrottle_dont_adjust_delay</span></code></a> meta
key.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6246">issue 6246</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6527">issue 6527</a>)</p></li>
<li><p>If <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span></code></a> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
<a class="reference internal" href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpiderLoader</span></code></a> does not raise
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#SyntaxError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">SyntaxError</span></code></a> but emits a warning instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6483">issue 6483</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6484">issue 6484</a>)</p></li>
<li><p>Added support for multiple-compressed responses (ones with several
encodings in the <code class="docutils literal notranslate"><span class="pre">Content-Encoding</span></code> header).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5143">issue 5143</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5964">issue 5964</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6063">issue 6063</a>)</p></li>
<li><p>Added support for multiple standard values in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REFERRER_POLICY</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6381">issue 6381</a>)</p></li>
<li><p>Added support for <a class="reference external" href="https://github.com/python-hyper/brotlicffi">brotlicffi</a> (previously named <a class="reference external" href="https://github.com/python-hyper/brotlipy/">brotlipy</a>). <a class="reference external" href="https://github.com/google/brotli">brotli</a> is
still recommended but only <a class="reference external" href="https://github.com/python-hyper/brotlicffi">brotlicffi</a> works on PyPy.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6263">issue 6263</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6269">issue 6269</a>)</p>
</li>
<li><p>Added <a class="reference internal" href="index.html#scrapy.contracts.default.MetadataContract" title="scrapy.contracts.default.MetadataContract"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetadataContract</span></code></a> that sets the
request meta.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6468">issue 6468</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6469">issue 6469</a>)</p></li>
</ul>
</section>
<section id="improvements">
<h5>Improvements<a class="headerlink" href="#improvements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Extended the list of file extensions that
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
ignores by default.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6074">issue 6074</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6125">issue 6125</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.httpobj.urlparse_cached()</span></code> is now used in more places
instead of <a class="reference external" href="https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">urllib.parse.urlparse()</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6228">issue 6228</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6229">issue 6229</a>)</p></li>
</ul>
</section>
<section id="bug-fixes">
<h5>Bug fixes<a class="headerlink" href="#bug-fixes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MediaPipeline</span></code> is now an abstract class and
its methods that were expected to be overridden in subclasses are now
abstract methods.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6365">issue 6365</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6368">issue 6368</a>)</p></li>
<li><p>Fixed handling of invalid <code class="docutils literal notranslate"><span class="pre">&#64;</span></code>-prefixed lines in contract extraction.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6383">issue 6383</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6388">issue 6388</a>)</p></li>
<li><p>Importing <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.telnet</span></code> no longer installs the default
reactor.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6432">issue 6432</a>)</p></li>
<li><p>Reduced log verbosity for dropped requests that was increased in 2.11.2.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6433">issue 6433</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6475">issue 6475</a>)</p></li>
</ul>
</section>
<section id="documentation">
<h5>Documentation<a class="headerlink" href="#documentation" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added <code class="docutils literal notranslate"><span class="pre">SECURITY.md</span></code> that documents the security policy.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5364">issue 5364</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6051">issue 6051</a>)</p></li>
<li><p>Example code for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-from-script"><span class="std std-ref">running Scrapy from a script</span></a> no
longer imports <code class="docutils literal notranslate"><span class="pre">twisted.internet.reactor</span></code> at the top level, which caused
problems with non-default reactors when this code was used unmodified.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6361">issue 6361</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6374">issue 6374</a>)</p></li>
<li><p>Documented the <a class="reference internal" href="index.html#scrapy.extensions.spiderstate.SpiderState" title="scrapy.extensions.spiderstate.SpiderState"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpiderState</span></code></a>
extension.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6278">issue 6278</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6522">issue 6522</a>)</p></li>
<li><p>Other documentation improvements and fixes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5920">issue 5920</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6094">issue 6094</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6177">issue 6177</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6200">issue 6200</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6207">issue 6207</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6216">issue 6216</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6223">issue 6223</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6317">issue 6317</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6328">issue 6328</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6389">issue 6389</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6394">issue 6394</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6402">issue 6402</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6411">issue 6411</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6427">issue 6427</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6429">issue 6429</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6440">issue 6440</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6448">issue 6448</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6449">issue 6449</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6462">issue 6462</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6497">issue 6497</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6506">issue 6506</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6507">issue 6507</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6524">issue 6524</a>)</p></li>
</ul>
</section>
<section id="quality-assurance">
<h5>Quality assurance<a class="headerlink" href="#quality-assurance" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added <code class="docutils literal notranslate"><span class="pre">py.typed</span></code>, in line with <a class="reference external" href="https://peps.python.org/pep-0561/">PEP 561</a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6058">issue 6058</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6059">issue 6059</a>)</p></li>
<li><p>Fully covered the code with type hints (except for the most complicated
parts, mostly related to <code class="docutils literal notranslate"><span class="pre">twisted.web.http</span></code> and other Twisted parts
without type hints).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5989">issue 5989</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6097">issue 6097</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6127">issue 6127</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6129">issue 6129</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6130">issue 6130</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6133">issue 6133</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6143">issue 6143</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6191">issue 6191</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6268">issue 6268</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6274">issue 6274</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6275">issue 6275</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6276">issue 6276</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6279">issue 6279</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6325">issue 6325</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6326">issue 6326</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6333">issue 6333</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6335">issue 6335</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6336">issue 6336</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6337">issue 6337</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6341">issue 6341</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6353">issue 6353</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6356">issue 6356</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6370">issue 6370</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6371">issue 6371</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6384">issue 6384</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6385">issue 6385</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6387">issue 6387</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6391">issue 6391</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6395">issue 6395</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6414">issue 6414</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6422">issue 6422</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6460">issue 6460</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6466">issue 6466</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6472">issue 6472</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6494">issue 6494</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6498">issue 6498</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6516">issue 6516</a>)</p></li>
<li><p>Improved <a class="reference external" href="https://bandit.readthedocs.io/en/latest/">Bandit</a> checks.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6260">issue 6260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6264">issue 6264</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6265">issue 6265</a>)</p></li>
<li><p>Added <a class="reference external" href="https://github.com/asottile/pyupgrade">pyupgrade</a> to the <code class="docutils literal notranslate"><span class="pre">pre-commit</span></code> configuration.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6392">issue 6392</a>)</p>
</li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">flake8-bugbear</span></code>, <code class="docutils literal notranslate"><span class="pre">flake8-comprehensions</span></code>, <code class="docutils literal notranslate"><span class="pre">flake8-debugger</span></code>,
<code class="docutils literal notranslate"><span class="pre">flake8-docstrings</span></code>, <code class="docutils literal notranslate"><span class="pre">flake8-string-format</span></code> and
<code class="docutils literal notranslate"><span class="pre">flake8-type-checking</span></code> to the <code class="docutils literal notranslate"><span class="pre">pre-commit</span></code> configuration.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6406">issue 6406</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6413">issue 6413</a>)</p></li>
<li><p>CI and test improvements and fixes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5285">issue 5285</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5454">issue 5454</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5997">issue 5997</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6078">issue 6078</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6084">issue 6084</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6087">issue 6087</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6132">issue 6132</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6153">issue 6153</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6154">issue 6154</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6201">issue 6201</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6231">issue 6231</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6232">issue 6232</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6235">issue 6235</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6236">issue 6236</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6242">issue 6242</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6245">issue 6245</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6253">issue 6253</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6258">issue 6258</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6259">issue 6259</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6270">issue 6270</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6272">issue 6272</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6286">issue 6286</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6290">issue 6290</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6296">issue 6296</a>
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6367">issue 6367</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6372">issue 6372</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6403">issue 6403</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6416">issue 6416</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6435">issue 6435</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6489">issue 6489</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6501">issue 6501</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6504">issue 6504</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6511">issue 6511</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6543">issue 6543</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6545">issue 6545</a>)</p></li>
<li><p>Code cleanups.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6196">issue 6196</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6197">issue 6197</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6198">issue 6198</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6199">issue 6199</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6254">issue 6254</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6257">issue 6257</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6285">issue 6285</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6305">issue 6305</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6343">issue 6343</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6349">issue 6349</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6386">issue 6386</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6415">issue 6415</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6463">issue 6463</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6470">issue 6470</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6499">issue 6499</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6505">issue 6505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6510">issue 6510</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6531">issue 6531</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6542">issue 6542</a>)</p></li>
</ul>
</section>
<section id="other">
<h5>Other<a class="headerlink" href="#other" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Issue tracker improvements. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6066">issue 6066</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-11-2-2024-05-14">
<span id="release-2-11-2"></span><h4>Scrapy 2.11.2 (2024-05-14)<a class="headerlink" href="#scrapy-2-11-2-2024-05-14" title="Permalink to this heading">¶</a></h4>
<section id="security-bug-fixes">
<h5>Security bug fixes<a class="headerlink" href="#security-bug-fixes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Redirects to non-HTTP protocols are no longer followed. Please, see the
<a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-23j4-mw76-5v7h">23j4-mw76-5v7h security advisory</a> for more information. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/457">issue 457</a>)</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header is now dropped on redirects to a different
scheme (<code class="docutils literal notranslate"><span class="pre">http://</span></code> or <code class="docutils literal notranslate"><span class="pre">https://</span></code>) or port, even if the domain is the
same. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-4qqq-9vqf-3h3f">4qqq-9vqf-3h3f security advisory</a> for more
information.</p>
</li>
<li><p>When using system proxy settings that are different for <code class="docutils literal notranslate"><span class="pre">http://</span></code> and
<code class="docutils literal notranslate"><span class="pre">https://</span></code>, redirects to a different URL scheme will now also trigger the
corresponding change in proxy settings for the redirected request. Please,
see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-jm3v-qxmh-hxwv">jm3v-qxmh-hxwv security advisory</a> for more information.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/767">issue 767</a>)</p>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Spider.allowed_domains</span></code></a> is now
enforced for all requests, and not only requests from spider callbacks.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1042">issue 1042</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2241">issue 2241</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6358">issue 6358</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a> no longer resolves XML
entities. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6265">issue 6265</a>)</p></li>
<li><p><a class="reference external" href="https://github.com/tiran/defusedxml">defusedxml</a> is now used to make
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.request.rpc.XmlRpcRequest</span></code> more secure.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6250">issue 6250</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6251">issue 6251</a>)</p>
</li>
</ul>
</section>
<section id="id1">
<h5>Bug fixes<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Restored support for <a class="reference external" href="https://github.com/python-hyper/brotlipy/">brotlipy</a>, which had been dropped in Scrapy 2.11.1 in
favor of <a class="reference external" href="https://github.com/google/brotli">brotli</a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6261">issue 6261</a>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>brotlipy is deprecated, both in Scrapy and upstream. Use brotli
instead if you can.</p>
</div>
</li>
<li><p>Make <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> <code class="docutils literal notranslate"><span class="pre">[&quot;noscript&quot;]</span></code> by default. This
prevents
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a> from
following redirects that would not be followed by web browsers with
JavaScript enabled. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6342">issue 6342</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6347">issue 6347</a>)</p></li>
<li><p>During <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed export</span></a>, do not close the
underlying file from <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#builtin-plugins"><span class="std std-ref">built-in post-processing plugins</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5932">issue 5932</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6178">issue 6178</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6239">issue 6239</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
now properly applies the <code class="docutils literal notranslate"><span class="pre">unique</span></code> and <code class="docutils literal notranslate"><span class="pre">canonicalize</span></code> parameters.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3273">issue 3273</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6221">issue 6221</a>)</p></li>
<li><p>Do not initialize the scheduler disk queue if <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> is an empty
string. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6121">issue 6121</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6124">issue 6124</a>)</p></li>
<li><p>Fix <a class="reference internal" href="index.html#scrapy.Spider.logger" title="scrapy.Spider.logger"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Spider.logger</span></code></a> not logging custom extra
information. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6323">issue 6323</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6324">issue 6324</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> files with a non-UTF-8 encoding no longer prevent parsing
the UTF-8-compatible (e.g. ASCII) parts of the document.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6292">issue 6292</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6298">issue 6298</a>)</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.http.cookies.WrappedRequest.get_header()</span></code> no longer raises an
exception if <code class="docutils literal notranslate"><span class="pre">default</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6308">issue 6308</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6310">issue 6310</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> now uses
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.response.get_base_url()</span></code> to determine the base URL of a
given <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6265">issue 6265</a>)</p></li>
<li><p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">media_to_download()</span></code> method of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipelines</span></a> now logs exceptions before stripping them.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5067">issue 5067</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5068">issue 5068</a>)</p></li>
<li><p>When passing a callback to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command, build the callback
callable with the right signature.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6182">issue 6182</a>)</p></li>
</ul>
</section>
<section id="id2">
<h5>Documentation<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Add a FAQ entry about <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-blank-request"><span class="std std-ref">creating blank requests</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6203">issue 6203</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6208">issue 6208</a>)</p></li>
<li><p>Document that <code class="xref py py-attr docutils literal notranslate"><span class="pre">scrapy.selector.Selector.type</span></code> can be <code class="docutils literal notranslate"><span class="pre">&quot;json&quot;</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6328">issue 6328</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6334">issue 6334</a>)</p></li>
</ul>
</section>
<section id="id3">
<h5>Quality assurance<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Make builds reproducible. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5019">issue 5019</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6322">issue 6322</a>)</p></li>
<li><p>Packaging and test fixes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6286">issue 6286</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6290">issue 6290</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6312">issue 6312</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6316">issue 6316</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6344">issue 6344</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-11-1-2024-02-14">
<span id="release-2-11-1"></span><h4>Scrapy 2.11.1 (2024-02-14)<a class="headerlink" href="#scrapy-2-11-1-2024-02-14" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Security bug fixes.</p></li>
<li><p>Support for Twisted &gt;= 23.8.0.</p></li>
<li><p>Documentation improvements.</p></li>
</ul>
<section id="id4">
<h5>Security bug fixes<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Addressed <a class="reference external" href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS">ReDoS vulnerabilities</a>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.iterators.xmliter</span></code> is now deprecated in favor of
<a class="reference internal" href="index.html#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a>, which
<a class="reference internal" href="index.html#scrapy.spiders.XMLFeedSpider" title="scrapy.spiders.XMLFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code></a> now uses.</p>
<p>To minimize the impact of this change on existing code,
<a class="reference internal" href="index.html#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a> now supports indicating
the node namespace with a prefix in the node name, and big files with
highly nested trees when using libxml2 2.7+.</p>
</li>
<li><p>Fixed regular expressions in the implementation of the
<a class="reference internal" href="index.html#scrapy.utils.response.open_in_browser" title="scrapy.utils.response.open_in_browser"><code class="xref py py-func docutils literal notranslate"><span class="pre">open_in_browser()</span></code></a> function.</p></li>
</ul>
<p>Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9">cc65-xxvf-f7r9 security advisory</a> for more information.</p>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_WARNSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_WARNSIZE</span></code></a> now also apply
to the decompressed response body. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7">7j7m-v7m3-jqm7 security
advisory</a> for more information.</p>
</li>
<li><p>Also in relation with the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7">7j7m-v7m3-jqm7 security advisory</a>, the
deprecated <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.decompression</span></code> module has been
removed.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header is now dropped on redirects to a different
domain. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cw9j-q3vf-hrrv">cw9j-q3vf-hrrv security advisory</a> for more
information.</p>
</li>
</ul>
</section>
<section id="id5">
<h5>Modified requirements<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The Twisted dependency is no longer restricted to &lt; 23.8.0. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6024">issue 6024</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6064">issue 6064</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6142">issue 6142</a>)</p></li>
</ul>
</section>
<section id="id6">
<h5>Bug fixes<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The OS signal handling code was refactored to no longer use private Twisted
functions. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6024">issue 6024</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6064">issue 6064</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6112">issue 6112</a>)</p></li>
</ul>
</section>
<section id="id7">
<h5>Documentation<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Improved documentation for <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> initialization
changes made in the 2.11.0 release. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6057">issue 6057</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6147">issue 6147</a>)</p></li>
<li><p>Extended documentation for <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5565">issue 5565</a>)</p></li>
<li><p>Fixed the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_merge_cookies</span></code></a> documentation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5936">issue 5936</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6077">issue 6077</a>)</p></li>
<li><p>Added a link to Zyte’s export guides to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> documentation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6183">issue 6183</a>)</p></li>
<li><p>Added a missing note about backward-incompatible changes in
<a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a> to the 2.11.0 release notes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6060">issue 6060</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6081">issue 6081</a>)</p></li>
<li><p>Added a missing note about removing the deprecated
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.boto.is_botocore()</span></code> function to the 2.8.0 release notes.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6056">issue 6056</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6061">issue 6061</a>)</p></li>
<li><p>Other documentation improvements. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6128">issue 6128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6144">issue 6144</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6163">issue 6163</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6190">issue 6190</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6192">issue 6192</a>)</p></li>
</ul>
</section>
<section id="id8">
<h5>Quality assurance<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added Python 3.12 to the CI configuration, re-enabled tests that were
disabled when the pre-release support was added. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5985">issue 5985</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6083">issue 6083</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6098">issue 6098</a>)</p></li>
<li><p>Fixed a test issue on PyPy 7.3.14. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6204">issue 6204</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6205">issue 6205</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-11-0-2023-09-18">
<span id="release-2-11-0"></span><h4>Scrapy 2.11.0 (2023-09-18)<a class="headerlink" href="#scrapy-2-11-0-2023-09-18" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Spiders can now modify <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a> in their
<a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> methods, e.g. based on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">spider
arguments</span></a>.</p></li>
<li><p>Periodic logging of stats.</p></li>
</ul>
<section id="id9">
<h5>Backward-incompatible changes<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Most of the initialization of <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></a> instances is
now done in <a class="reference internal" href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a>, so the state of
instances before that method is called is now different compared to older
Scrapy versions. We do not recommend using the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instances before
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> is called. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6038">issue 6038</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.Spider.from_crawler()</span></code></a> is now called before the initialization
of various components previously initialized in
<code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler.__init__()</span></code> and before the settings are
finalized and frozen. This change was needed to allow changing the settings
in <a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.Spider.from_crawler()</span></code></a>. If you want to access the final
setting values and the initialized <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
attributes in the spider code as early as possible you can do this in
<a class="reference internal" href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> or in a handler of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-engine_started"><code class="xref std std-signal docutils literal notranslate"><span class="pre">engine_started</span></code></a> signal. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6038">issue 6038</a>)</p></li>
<li><p>The <a class="reference internal" href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.json</span></code></a> method now
requires the response to be in a valid JSON encoding (UTF-8, UTF-16, or
UTF-32). If you need to deal with JSON documents in an invalid encoding,
use <code class="docutils literal notranslate"><span class="pre">json.loads(response.text)</span></code> instead. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6016">issue 6016</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a> used the binary output by
default but it no longer does. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6006">issue 6006</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6007">issue 6007</a>)</p></li>
</ul>
</section>
<section id="id10">
<h5>Deprecation removals<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Removed the binary export mode of
<a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a>, deprecated in Scrapy 1.1.0.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6006">issue 6006</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6007">issue 6007</a>)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using this Scrapy version on Scrapy Cloud with a stack
that includes an older Scrapy version and get a “TypeError:
Unexpected options: binary” error, you may need to add
<code class="docutils literal notranslate"><span class="pre">scrapinghub-entrypoint-scrapy</span> <span class="pre">&gt;=</span> <span class="pre">0.14.1</span></code> to your project
requirements or switch to a stack that includes Scrapy 2.11.</p>
</div>
</li>
<li><p>Removed the <code class="docutils literal notranslate"><span class="pre">CrawlerRunner.spiders</span></code> attribute, deprecated in Scrapy
1.0.0, use <code class="xref py py-attr docutils literal notranslate"><span class="pre">CrawlerRunner.spider_loader</span></code> instead. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6010">issue 6010</a>)</p></li>
<li><p>The <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.response.response_httprepr()</span></code> function, deprecated in
Scrapy 2.6.0, has now been removed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6111">issue 6111</a>)</p></li>
</ul>
</section>
<section id="id11">
<h5>Deprecations<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Running <a class="reference internal" href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> more than once on the same
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></a> instance is now deprecated. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1587">issue 1587</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6040">issue 6040</a>)</p></li>
</ul>
</section>
<section id="id12">
<h5>New features<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Spiders can now modify settings in their
<a class="reference internal" href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> method, e.g. based on <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spiderargs"><span class="std std-ref">spider
arguments</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1305">issue 1305</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1580">issue 1580</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2392">issue 2392</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3663">issue 3663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6038">issue 6038</a>)</p></li>
<li><p>Added the <a class="reference internal" href="index.html#scrapy.extensions.periodic_log.PeriodicLog" title="scrapy.extensions.periodic_log.PeriodicLog"><code class="xref py py-class docutils literal notranslate"><span class="pre">PeriodicLog</span></code></a> extension
which can be enabled to log stats and/or their differences periodically.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5926">issue 5926</a>)</p></li>
<li><p>Optimized the memory usage in <a class="reference internal" href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.json</span></code></a> by removing unnecessary body decoding.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5968">issue 5968</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6016">issue 6016</a>)</p></li>
<li><p>Links to <code class="docutils literal notranslate"><span class="pre">.webp</span></code> files are now ignored by <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">link extractors</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6021">issue 6021</a>)</p></li>
</ul>
</section>
<section id="id13">
<h5>Bug fixes<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fixed logging enabled add-ons. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6036">issue 6036</a>)</p></li>
<li><p>Fixed <a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a> producing invalid message bodies
when the <code class="docutils literal notranslate"><span class="pre">charset</span></code> argument is passed to
<a class="reference internal" href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send"><code class="xref py py-meth docutils literal notranslate"><span class="pre">send()</span></code></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5096">issue 5096</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5118">issue 5118</a>)</p></li>
<li><p>Fixed an exception when accessing <code class="docutils literal notranslate"><span class="pre">self.EXCEPTIONS_TO_RETRY</span></code> from a
subclass of <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6049">issue 6049</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6050">issue 6050</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.settings.BaseSettings.getdictorlist" title="scrapy.settings.BaseSettings.getdictorlist"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings.getdictorlist()</span></code></a>, used to parse
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a>, now handles tuple values. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6011">issue 6011</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6013">issue 6013</a>)</p></li>
<li><p>Calls to <code class="docutils literal notranslate"><span class="pre">datetime.utcnow()</span></code>, no longer recommended to be used, have been
replaced with calls to <code class="docutils literal notranslate"><span class="pre">datetime.now()</span></code> with a timezone. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6014">issue 6014</a>)</p></li>
</ul>
</section>
<section id="id14">
<h5>Documentation<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Updated a deprecated function call in a pipeline example. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6008">issue 6008</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6009">issue 6009</a>)</p></li>
</ul>
</section>
<section id="id15">
<h5>Quality assurance<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Extended typing hints. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6003">issue 6003</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6005">issue 6005</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6031">issue 6031</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6034">issue 6034</a>)</p></li>
<li><p>Pinned <a class="reference external" href="https://github.com/google/brotli">brotli</a> to 1.0.9 for the PyPy tests as 1.1.0 breaks them.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6044">issue 6044</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6045">issue 6045</a>)</p></li>
<li><p>Other CI and pre-commit improvements. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6002">issue 6002</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6013">issue 6013</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6046">issue 6046</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-10-1-2023-08-30">
<span id="release-2-10-1"></span><h4>Scrapy 2.10.1 (2023-08-30)<a class="headerlink" href="#scrapy-2-10-1-2023-08-30" title="Permalink to this heading">¶</a></h4>
<p>Marked <code class="docutils literal notranslate"><span class="pre">Twisted</span> <span class="pre">&gt;=</span> <span class="pre">23.8.0</span></code> as unsupported. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/6024">issue 6024</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6026">issue 6026</a>)</p>
</section>
<section id="scrapy-2-10-0-2023-08-04">
<span id="release-2-10-0"></span><h4>Scrapy 2.10.0 (2023-08-04)<a class="headerlink" href="#scrapy-2-10-0-2023-08-04" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Added Python 3.12 support, dropped Python 3.7 support.</p></li>
<li><p>The new add-ons framework simplifies configuring 3rd-party components that
support it.</p></li>
<li><p>Exceptions to retry can now be configured.</p></li>
<li><p>Many fixes and improvements for feed exports.</p></li>
</ul>
<section id="id16">
<h5>Modified requirements<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Dropped support for Python 3.7. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5953">issue 5953</a>)</p></li>
<li><p>Added support for the upcoming Python 3.12. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5984">issue 5984</a>)</p></li>
<li><p>Minimum versions increased for these dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://lxml.de/">lxml</a>: 4.3.0 → 4.4.1</p></li>
<li><p><a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a>: 3.4.6 → 36.0.0</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">pkg_resources</span></code> is no longer used. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5956">issue 5956</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5958">issue 5958</a>)</p></li>
<li><p><a class="reference external" href="https://github.com/boto/boto3">boto3</a> is now recommended instead of <a class="reference external" href="https://github.com/boto/botocore">botocore</a> for exporting to S3.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5833">issue 5833</a>).</p></li>
</ul>
</section>
<section id="id17">
<h5>Backward-incompatible changes<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The value of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY</span></code></a> setting is now <code class="docutils literal notranslate"><span class="pre">True</span></code>
instead of <code class="docutils literal notranslate"><span class="pre">False</span></code>. In earlier Scrapy versions empty files were created
even when this setting was <code class="docutils literal notranslate"><span class="pre">False</span></code> (which was a bug that is now fixed),
so the new default should keep the old behavior. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5847">issue 5847</a>)</p></li>
</ul>
</section>
<section id="id18">
<h5>Deprecation removals<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>When a function is assigned to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> setting,
returning <code class="docutils literal notranslate"><span class="pre">None</span></code> or modifying the <code class="docutils literal notranslate"><span class="pre">params</span></code> input parameter, deprecated
in Scrapy 2.6, is no longer supported. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5994">issue 5994</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5996">issue 5996</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.reqser</span></code> module, deprecated in Scrapy 2.6, is removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5994">issue 5994</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5996">issue 5996</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.squeues</span></code> classes <code class="docutils literal notranslate"><span class="pre">PickleFifoDiskQueueNonRequest</span></code>,
<code class="docutils literal notranslate"><span class="pre">PickleLifoDiskQueueNonRequest</span></code>, <code class="docutils literal notranslate"><span class="pre">MarshalFifoDiskQueueNonRequest</span></code>,
and <code class="docutils literal notranslate"><span class="pre">MarshalLifoDiskQueueNonRequest</span></code>, deprecated in
Scrapy 2.6, are removed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5994">issue 5994</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5996">issue 5996</a>)</p></li>
<li><p>The property <code class="docutils literal notranslate"><span class="pre">open_spiders</span></code> and the methods <code class="docutils literal notranslate"><span class="pre">has_capacity</span></code> and
<code class="docutils literal notranslate"><span class="pre">schedule</span></code> of <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.engine.ExecutionEngine</span></code>,
deprecated in Scrapy 2.6, are removed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5994">issue 5994</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5998">issue 5998</a>)</p></li>
<li><p>Passing a <code class="docutils literal notranslate"><span class="pre">spider</span></code> argument to the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">spider_is_idle()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">download()</span></code> methods of
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.engine.ExecutionEngine</span></code>, deprecated in Scrapy 2.6, is
no longer supported. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5994">issue 5994</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5998">issue 5998</a>)</p></li>
</ul>
</section>
<section id="id19">
<h5>Deprecations<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.CaselessDict</span></code> is deprecated, use
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.CaseInsensitiveDict</span></code> instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5146">issue 5146</a>)</p></li>
<li><p>Passing the <code class="docutils literal notranslate"><span class="pre">custom</span></code> argument to
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.conf.build_component_list()</span></code> is deprecated, it was used
in the past to merge <code class="docutils literal notranslate"><span class="pre">FOO</span></code> and <code class="docutils literal notranslate"><span class="pre">FOO_BASE</span></code> setting values but now Scrapy
uses <a class="reference internal" href="index.html#scrapy.settings.BaseSettings.getwithbase" title="scrapy.settings.BaseSettings.getwithbase"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings.getwithbase()</span></code></a> to do the same.
Code that uses this argument and cannot be switched to <code class="docutils literal notranslate"><span class="pre">getwithbase()</span></code>
can be switched to merging the values explicitly. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5726">issue 5726</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5923">issue 5923</a>)</p></li>
</ul>
</section>
<section id="id20">
<h5>New features<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added support for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-addons"><span class="std std-ref">Scrapy add-ons</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5950">issue 5950</a>)</p></li>
<li><p>Added the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_EXCEPTIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_EXCEPTIONS</span></code></a> setting that configures which
exceptions will be retried by
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2701">issue 2701</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5929">issue 5929</a>)</p></li>
<li><p>Added the possiiblity to close the spider if no items were produced in the
specified time, configured by <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT_NO_ITEM</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5979">issue 5979</a>)</p></li>
<li><p>Added support for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a> setting to feed exports.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5980">issue 5980</a>)</p></li>
<li><p>Added support for using <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> objects that refer to
absolute Windows paths in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5939">issue 5939</a>)</p></li>
</ul>
</section>
<section id="id21">
<h5>Bug fixes<a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fixed creating empty feeds even with <code class="docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY=False</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5847">issue 5847</a>)</p></li>
<li><p>Fixed using absolute Windows paths when specifying output files.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5969">issue 5969</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5971">issue 5971</a>)</p></li>
<li><p>Fixed problems with uploading large files to S3 by switching to multipart
uploads (requires <a class="reference external" href="https://github.com/boto/boto3">boto3</a>). (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/960">issue 960</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5735">issue 5735</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5833">issue 5833</a>)</p></li>
<li><p>Fixed the JSON exporter writing extra commas when some exceptions occur.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3090">issue 3090</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5952">issue 5952</a>)</p></li>
<li><p>Fixed the “read of closed file” error in the CSV exporter. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5043">issue 5043</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5705">issue 5705</a>)</p></li>
<li><p>Fixed an error when a component added by the class object throws
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal notranslate"><span class="pre">NotConfigured</span></code></a> with a message. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5950">issue 5950</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5992">issue 5992</a>)</p></li>
<li><p>Added the missing <a class="reference internal" href="index.html#scrapy.settings.BaseSettings.pop" title="scrapy.settings.BaseSettings.pop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings.pop()</span></code></a> method.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5959">issue 5959</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5960">issue 5960</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5963">issue 5963</a>)</p></li>
<li><p>Added <code class="xref py py-class docutils literal notranslate"><span class="pre">CaseInsensitiveDict</span></code> as a replacement
for <code class="xref py py-class docutils literal notranslate"><span class="pre">CaselessDict</span></code> that fixes some API
inconsistencies. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5146">issue 5146</a>)</p></li>
</ul>
</section>
<section id="id22">
<h5>Documentation<a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Documented <a class="reference internal" href="index.html#scrapy.Spider.update_settings" title="scrapy.Spider.update_settings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.Spider.update_settings()</span></code></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5745">issue 5745</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5846">issue 5846</a>)</p></li>
<li><p>Documented possible problems with early Twisted reactor installation and
their solutions. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5981">issue 5981</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/6000">issue 6000</a>)</p></li>
<li><p>Added examples of making additional requests in callbacks. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5927">issue 5927</a>)</p></li>
<li><p>Improved the feed export docs. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5579">issue 5579</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5931">issue 5931</a>)</p></li>
<li><p>Clarified the docs about request objects on redirection. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5707">issue 5707</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5937">issue 5937</a>)</p></li>
</ul>
</section>
<section id="id23">
<h5>Quality assurance<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added support for running tests against the installed Scrapy version.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4914">issue 4914</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5949">issue 5949</a>)</p></li>
<li><p>Extended typing hints. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5925">issue 5925</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5977">issue 5977</a>)</p></li>
<li><p>Fixed the <code class="docutils literal notranslate"><span class="pre">test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop</span></code>
test. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5951">issue 5951</a>)</p></li>
<li><p>Fixed the <code class="docutils literal notranslate"><span class="pre">test_feedexport.BatchDeliveriesTest.test_batch_path_differ</span></code>
test on Windows. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5847">issue 5847</a>)</p></li>
<li><p>Enabled CI runs for Python 3.11 on Windows. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5999">issue 5999</a>)</p></li>
<li><p>Simplified skipping tests that depend on <code class="docutils literal notranslate"><span class="pre">uvloop</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5984">issue 5984</a>)</p></li>
<li><p>Fixed the <code class="docutils literal notranslate"><span class="pre">extra-deps-pinned</span></code> tox env. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5948">issue 5948</a>)</p></li>
<li><p>Implemented cleanups. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5965">issue 5965</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5986">issue 5986</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-9-0-2023-05-08">
<span id="release-2-9-0"></span><h4>Scrapy 2.9.0 (2023-05-08)<a class="headerlink" href="#scrapy-2-9-0-2023-05-08" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Per-domain download settings.</p></li>
<li><p>Compatibility with new <a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a> and new <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a>.</p></li>
<li><p>JMESPath selectors from the new <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a>.</p></li>
<li><p>Bug fixes.</p></li>
</ul>
<section id="id24">
<h5>Deprecations<a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.extensions.feedexport._FeedSlot</span></code> is renamed to
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.extensions.feedexport.FeedSlot</span></code> and the old name is
deprecated. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5876">issue 5876</a>)</p></li>
</ul>
</section>
<section id="id25">
<h5>New features<a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Settings corresponding to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a> can now be set on a per-domain basis
via the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_SLOTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_SLOTS</span></code></a> setting. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5328">issue 5328</a>)</p></li>
<li><p>Added <code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.jmespath()</span></code>, a shortcut for JMESPath selectors
available since <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a> 1.8.1. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5894">issue 5894</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5915">issue 5915</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-feed_slot_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">feed_slot_closed</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-feed_exporter_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">feed_exporter_closed</span></code></a>
signals. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5876">issue 5876</a>)</p></li>
<li><p>Added <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.request_to_curl()</span></code>, a function to produce a
curl command from a <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> object. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5892">issue 5892</a>)</p></li>
<li><p>Values of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can now be
<a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> instances. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5801">issue 5801</a>)</p></li>
</ul>
</section>
<section id="id26">
<h5>Bug fixes<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fixed a warning with Parsel 1.8.1+. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5903">issue 5903</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5918">issue 5918</a>)</p></li>
<li><p>Fixed an error when using feed postprocessing with S3 storage.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5500">issue 5500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5581">issue 5581</a>)</p></li>
<li><p>Added the missing <a class="reference internal" href="index.html#scrapy.settings.BaseSettings.setdefault" title="scrapy.settings.BaseSettings.setdefault"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings.setdefault()</span></code></a> method.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5811">issue 5811</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5821">issue 5821</a>)</p></li>
<li><p>Fixed an error when using <a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a> 40.0.0+ and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> is enabled.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5857">issue 5857</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5858">issue 5858</a>)</p></li>
<li><p>The checksums returned by <a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code></a>
for files on Google Cloud Storage are no longer Base64-encoded.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5874">issue 5874</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5891">issue 5891</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.request_from_curl()</span></code> now supports $-prefixed
string values for the curl <code class="docutils literal notranslate"><span class="pre">--data-raw</span></code> argument, which are produced by
browsers for data that includes certain symbols. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5899">issue 5899</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5901">issue 5901</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command now also works with async generator callbacks.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5819">issue 5819</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5824">issue 5824</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command now properly works with HTTPS URLs.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3553">issue 3553</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5808">issue 5808</a>)</p></li>
<li><p>Improved handling of asyncio loops. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5831">issue 5831</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5832">issue 5832</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
now skips certain malformed URLs instead of raising an exception.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5881">issue 5881</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.python.get_func_args()</span></code> now supports more types of
callables. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5872">issue 5872</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5885">issue 5885</a>)</p></li>
<li><p>Fixed an error when processing non-UTF8 values of <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> headers.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5914">issue 5914</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5917">issue 5917</a>)</p></li>
<li><p>Fixed an error breaking user handling of send failures in
<a class="reference internal" href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send"><code class="xref py py-meth docutils literal notranslate"><span class="pre">scrapy.mail.MailSender.send()</span></code></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1611">issue 1611</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5880">issue 5880</a>)</p></li>
</ul>
</section>
<section id="id27">
<h5>Documentation<a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Expanded contributing docs. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5109">issue 5109</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5851">issue 5851</a>)</p></li>
<li><p>Added <a class="reference external" href="https://github.com/adamchainz/blacken-docs">blacken-docs</a> to pre-commit and reformatted the docs with it.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5813">issue 5813</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5816">issue 5816</a>)</p></li>
<li><p>Fixed a JS issue. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5875">issue 5875</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5877">issue 5877</a>)</p></li>
<li><p>Fixed <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">htmlview</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5878">issue 5878</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5879">issue 5879</a>)</p></li>
<li><p>Fixed typos and other small errors. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5827">issue 5827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5839">issue 5839</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5883">issue 5883</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5890">issue 5890</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5895">issue 5895</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5904">issue 5904</a>)</p></li>
</ul>
</section>
<section id="id28">
<h5>Quality assurance<a class="headerlink" href="#id28" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Extended typing hints. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5805">issue 5805</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5889">issue 5889</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5896">issue 5896</a>)</p></li>
<li><p>Tests for most of the examples in the docs are now run as a part of CI,
found problems were fixed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5816">issue 5816</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5826">issue 5826</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5919">issue 5919</a>)</p></li>
<li><p>Removed usage of deprecated Python classes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5849">issue 5849</a>)</p></li>
<li><p>Silenced <code class="docutils literal notranslate"><span class="pre">include-ignored</span></code> warnings from coverage. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5820">issue 5820</a>)</p></li>
<li><p>Fixed a random failure of the <code class="docutils literal notranslate"><span class="pre">test_feedexport.test_batch_path_differ</span></code>
test. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5855">issue 5855</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5898">issue 5898</a>)</p></li>
<li><p>Updated docstrings to match output produced by <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a> 1.8.1 so that they
don’t cause test failures. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5902">issue 5902</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5919">issue 5919</a>)</p></li>
<li><p>Other CI and pre-commit improvements. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5802">issue 5802</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5823">issue 5823</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5908">issue 5908</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-8-0-2023-02-02">
<span id="release-2-8-0"></span><h4>Scrapy 2.8.0 (2023-02-02)<a class="headerlink" href="#scrapy-2-8-0-2023-02-02" title="Permalink to this heading">¶</a></h4>
<p>This is a maintenance release, with minor features, bug fixes, and cleanups.</p>
<section id="id29">
<h5>Deprecation removals<a class="headerlink" href="#id29" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.read1</span></code> function, deprecated in Scrapy 2.0, has now
been removed. Use the <a class="reference external" href="https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1" title="(in Python v3.13)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">read1()</span></code></a> method of
<a class="reference external" href="https://docs.python.org/3/library/gzip.html#gzip.GzipFile" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">GzipFile</span></code></a> instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.to_native_str</span></code> function, deprecated in Scrapy
2.0, has now been removed. Use <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.python.to_unicode()</span></code>
instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.MutableChain.next</span></code> method, deprecated in Scrapy
2.0, has now been removed. Use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">__next__()</span></code> instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.linkextractors.FilteringLinkExtractor</span></code> class, deprecated
in Scrapy 2.0, has now been removed. Use
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
instead.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5720">issue 5720</a>)</p></li>
<li><p>Support for using environment variables prefixed with <code class="docutils literal notranslate"><span class="pre">SCRAPY_</span></code> to
override settings, deprecated in Scrapy 2.0, has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5724">issue 5724</a>)</p></li>
<li><p>Support for the <code class="docutils literal notranslate"><span class="pre">noconnect</span></code> query string argument in proxy URLs,
deprecated in Scrapy 2.0, has now been removed. We expect proxies that used
to need it to work fine without it.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5731">issue 5731</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.retry_on_eintr</span></code> function, deprecated in Scrapy
2.3, has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.WeakKeyCache</span></code> class, deprecated in Scrapy 2.4,
has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.boto.is_botocore()</span></code> function, deprecated in Scrapy 2.4,
has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5719">issue 5719</a>)</p></li>
</ul>
</section>
<section id="id30">
<h5>Deprecations<a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-exc docutils literal notranslate"><span class="pre">scrapy.pipelines.images.NoimagesDrop</span></code> is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5368">issue 5368</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5489">issue 5489</a>)</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.convert_image</span></code> must now accept a
<code class="docutils literal notranslate"><span class="pre">response_body</span></code> parameter.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3055">issue 3055</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3689">issue 3689</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4753">issue 4753</a>)</p></li>
</ul>
</section>
<section id="id31">
<h5>New features<a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Applied <a class="reference external" href="https://black.readthedocs.io/en/stable/">black</a> coding style to files generated with the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> commands.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5809">issue 5809</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5814">issue 5814</a>)</p>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a> is now set to <code class="docutils literal notranslate"><span class="pre">&quot;utf-8&quot;</span></code> in the
<code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file that the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command generates.
With this value, JSON exports won’t force the use of escape sequences for
non-ASCII characters.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5797">issue 5797</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5800">issue 5800</a>)</p></li>
<li><p>The <a class="reference internal" href="index.html#scrapy.extensions.memusage.MemoryUsage" title="scrapy.extensions.memusage.MemoryUsage"><code class="xref py py-class docutils literal notranslate"><span class="pre">MemoryUsage</span></code></a> extension now logs the
peak memory usage during checks, and the binary unit MiB is now used to
avoid confusion.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5717">issue 5717</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5722">issue 5722</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5727">issue 5727</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">callback</span></code> parameter of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> can now be set
to <a class="reference internal" href="index.html#scrapy.http.request.NO_CALLBACK" title="scrapy.http.request.NO_CALLBACK"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.http.request.NO_CALLBACK()</span></code></a>, to distinguish it from
<code class="docutils literal notranslate"><span class="pre">None</span></code>, as the latter indicates that the default spider callback
(<a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a>) is to be used.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5798">issue 5798</a>)</p></li>
</ul>
</section>
<section id="id32">
<h5>Bug fixes<a class="headerlink" href="#id32" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Enabled unsafe legacy SSL renegotiation to fix access to some outdated
websites.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5491">issue 5491</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5790">issue 5790</a>)</p></li>
<li><p>Fixed STARTTLS-based email delivery not working with Twisted 21.2.0 and
better.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5386">issue 5386</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5406">issue 5406</a>)</p></li>
<li><p>Fixed the <code class="xref py py-meth docutils literal notranslate"><span class="pre">finish_exporting()</span></code> method of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporters</span></a> not being called for empty files.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5537">issue 5537</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5758">issue 5758</a>)</p></li>
<li><p>Fixed HTTP/2 responses getting only the last value for a header when
multiple headers with the same name are received.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5777">issue 5777</a>)</p></li>
<li><p>Fixed an exception raised by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> command on some cases
when <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">using asyncio</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5740">issue 5740</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5742">issue 5742</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5748">issue 5748</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5759">issue 5759</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5760">issue 5760</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5771">issue 5771</a>)</p></li>
<li><p>When using <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>, callback keyword arguments
(<code class="docutils literal notranslate"><span class="pre">cb_kwargs</span></code>) added to a request in the <code class="docutils literal notranslate"><span class="pre">process_request</span></code> callback of a
<a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> will no longer be ignored.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5699">issue 5699</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#images-pipeline"><span class="std std-ref">images pipeline</span></a> no longer re-encodes JPEG
files.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3055">issue 3055</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3689">issue 3689</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4753">issue 4753</a>)</p></li>
<li><p>Fixed the handling of transparent WebP images by the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#images-pipeline"><span class="std std-ref">images pipeline</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3072">issue 3072</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5766">issue 5766</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5767">issue 5767</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.shell.inspect_response()</span></code> no longer inhibits <code class="docutils literal notranslate"><span class="pre">SIGINT</span></code>
(Ctrl+C).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2918">issue 2918</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
with <code class="docutils literal notranslate"><span class="pre">unique=False</span></code> no longer filters out links that have identical URL
<em>and</em> text.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3798">issue 3798</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3799">issue 3799</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4695">issue 4695</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5458">issue 5458</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobotsTxtMiddleware</span></code></a> now
ignores URL protocols that do not support <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> (<code class="docutils literal notranslate"><span class="pre">data://</span></code>,
<code class="docutils literal notranslate"><span class="pre">file://</span></code>).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5807">issue 5807</a>)</p></li>
<li><p>Silenced the <code class="docutils literal notranslate"><span class="pre">filelock</span></code> debug log messages introduced in Scrapy 2.6.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5753">issue 5753</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5754">issue 5754</a>)</p></li>
<li><p>Fixed the output of <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">-h</span></code> showing an unintended <code class="docutils literal notranslate"><span class="pre">**commands**</span></code>
line.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5709">issue 5709</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5711">issue 5711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5712">issue 5712</a>)</p></li>
<li><p>Made the active project indication in the output of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-commands"><span class="std std-ref">commands</span></a> more clear.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5715">issue 5715</a>)</p></li>
</ul>
</section>
<section id="id33">
<h5>Documentation<a class="headerlink" href="#id33" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Documented how to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#debug-vscode"><span class="std std-ref">debug spiders from Visual Studio Code</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5721">issue 5721</a>)</p></li>
<li><p>Documented how <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> affects per-domain concurrency.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5083">issue 5083</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5540">issue 5540</a>)</p></li>
<li><p>Improved consistency.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5761">issue 5761</a>)</p></li>
<li><p>Fixed typos.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5714">issue 5714</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5744">issue 5744</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5764">issue 5764</a>)</p></li>
</ul>
</section>
<section id="id34">
<h5>Quality assurance<a class="headerlink" href="#id34" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Applied <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#coding-style"><span class="std std-ref">black coding style</span></a>, sorted import statements,
and introduced <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#scrapy-pre-commit"><span class="std std-ref">pre-commit</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4654">issue 4654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4658">issue 4658</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5734">issue 5734</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5737">issue 5737</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5806">issue 5806</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5810">issue 5810</a>)</p></li>
<li><p>Switched from <a class="reference external" href="https://docs.python.org/3/library/os.path.html#module-os.path" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">os.path</span></code></a> to <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#module-pathlib" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">pathlib</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4916">issue 4916</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4497">issue 4497</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5682">issue 5682</a>)</p></li>
<li><p>Addressed many issues reported by Pylint.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5677">issue 5677</a>)</p></li>
<li><p>Improved code readability.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5736">issue 5736</a>)</p></li>
<li><p>Improved package metadata.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5768">issue 5768</a>)</p></li>
<li><p>Removed direct invocations of <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5774">issue 5774</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5776">issue 5776</a>)</p></li>
<li><p>Removed unnecessary <a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.OrderedDict" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">OrderedDict</span></code></a> usages.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5795">issue 5795</a>)</p></li>
<li><p>Removed unnecessary <code class="docutils literal notranslate"><span class="pre">__str__</span></code> definitions.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5150">issue 5150</a>)</p></li>
<li><p>Removed obsolete code and comments.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5725">issue 5725</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5729">issue 5729</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5730">issue 5730</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5732">issue 5732</a>)</p></li>
<li><p>Fixed test and CI issues.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5749">issue 5749</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5750">issue 5750</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5756">issue 5756</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5762">issue 5762</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5765">issue 5765</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5780">issue 5780</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5781">issue 5781</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5782">issue 5782</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5783">issue 5783</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5785">issue 5785</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5786">issue 5786</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-7-1-2022-11-02">
<span id="release-2-7-1"></span><h4>Scrapy 2.7.1 (2022-11-02)<a class="headerlink" href="#scrapy-2-7-1-2022-11-02" title="Permalink to this heading">¶</a></h4>
<section id="id35">
<h5>New features<a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Relaxed the restriction introduced in 2.6.2 so that the
<code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header can again be set explicitly, as long as the
proxy URL in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata has no other credentials, and
for as long as that proxy URL remains the same; this restores compatibility
with scrapy-zyte-smartproxy 2.1.0 and older (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5626">issue 5626</a>).</p></li>
</ul>
</section>
<section id="id36">
<h5>Bug fixes<a class="headerlink" href="#id36" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">-O</span></code>/<code class="docutils literal notranslate"><span class="pre">--overwrite-output</span></code> and <code class="docutils literal notranslate"><span class="pre">-t</span></code>/<code class="docutils literal notranslate"><span class="pre">--output-format</span></code> options
together now produces an error instead of ignoring the former option
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5516">issue 5516</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5605">issue 5605</a>).</p></li>
<li><p>Replaced deprecated <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> APIs that implicitly use the current
event loop with code that explicitly requests a loop from the event loop
policy (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5685">issue 5685</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5689">issue 5689</a>).</p></li>
<li><p>Fixed uses of deprecated Scrapy APIs in Scrapy itself (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5588">issue 5588</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5589">issue 5589</a>).</p></li>
<li><p>Fixed uses of a deprecated Pillow API (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5684">issue 5684</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5692">issue 5692</a>).</p></li>
<li><p>Improved code that checks if generators return values, so that it no longer
fails on decorated methods and partial methods (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5323">issue 5323</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5592">issue 5592</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5599">issue 5599</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5691">issue 5691</a>).</p></li>
</ul>
</section>
<section id="id37">
<h5>Documentation<a class="headerlink" href="#id37" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Upgraded the Code of Conduct to Contributor Covenant v2.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5698">issue 5698</a>).</p></li>
<li><p>Fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5681">issue 5681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5694">issue 5694</a>).</p></li>
</ul>
</section>
<section id="id38">
<h5>Quality assurance<a class="headerlink" href="#id38" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Re-enabled some erroneously disabled flake8 checks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5688">issue 5688</a>).</p></li>
<li><p>Ignored harmless deprecation warnings from <a class="reference external" href="https://docs.python.org/3/library/typing.html#module-typing" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">typing</span></code></a> in tests
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5686">issue 5686</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5697">issue 5697</a>).</p></li>
<li><p>Modernized our CI configuration (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5695">issue 5695</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5696">issue 5696</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-7-0-2022-10-17">
<span id="release-2-7-0"></span><h4>Scrapy 2.7.0 (2022-10-17)<a class="headerlink" href="#scrapy-2-7-0-2022-10-17" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Added Python 3.11 support, dropped Python 3.6 support</p></li>
<li><p>Improved support for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">asynchronous callbacks</span></a></p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">Asyncio support</span></a> is enabled by default on new
projects</p></li>
<li><p>Output names of item fields can now be arbitrary strings</p></li>
<li><p>Centralized <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#request-fingerprints"><span class="std std-ref">request fingerprinting</span></a>
configuration is now possible</p></li>
</ul>
<section id="id39">
<h5>Modified requirements<a class="headerlink" href="#id39" title="Permalink to this heading">¶</a></h5>
<p>Python 3.7 or greater is now required; support for Python 3.6 has been dropped.
Support for the upcoming Python 3.11 has been added.</p>
<p>The minimum required version of some dependencies has changed as well:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://lxml.de/">lxml</a>: 3.5.0 → 4.3.0</p></li>
<li><p><a class="reference external" href="https://python-pillow.org/">Pillow</a> (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#images-pipeline"><span class="std std-ref">images pipeline</span></a>): 4.0.0 → 7.1.0</p></li>
<li><p><a class="reference external" href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>: 5.0.0 → 5.1.0</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5512">issue 5512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5514">issue 5514</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5524">issue 5524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5563">issue 5563</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5664">issue 5664</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5670">issue 5670</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5678">issue 5678</a>)</p>
</section>
<section id="id40">
<h5>Deprecations<a class="headerlink" href="#id40" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.thumb_path</span></code></a> must now accept an
<code class="docutils literal notranslate"><span class="pre">item</span></code> parameter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5504">issue 5504</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5508">issue 5508</a>).</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.decompression</span></code> module is now
deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5546">issue 5546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5547">issue 5547</a>).</p></li>
</ul>
</section>
<section id="id41">
<h5>New features<a class="headerlink" href="#id41" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a> can now be
defined as an <a class="reference external" href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.13)"><span class="xref std std-term">asynchronous generator</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4978">issue 4978</a>).</p></li>
<li><p>The output of <code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> callbacks defined as
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">coroutines</span></a> is now processed asynchronously
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4978">issue 4978</a>).</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code> now supports <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">asynchronous
callbacks</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5657">issue 5657</a>).</p></li>
<li><p>New projects created with the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command have
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">asyncio support</span></a> enabled by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5590">issue 5590</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5679">issue 5679</a>).</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a> setting can now be defined as a
dictionary to customize the output name of item fields, lifting the
restriction that required output names to be valid Python identifiers, e.g.
preventing them to have whitespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1008">issue 1008</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3266">issue 3266</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3696">issue 3696</a>).</p></li>
<li><p>You can now customize <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#request-fingerprints"><span class="std std-ref">request fingerprinting</span></a>
through the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REQUEST_FINGERPRINTER_CLASS</span></code></a> setting, instead of
having to change it on every Scrapy component that relies on request
fingerprinting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/900">issue 900</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3420">issue 3420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4113">issue 4113</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4762">issue 4762</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4524">issue 4524</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">jsonl</span></code> is now supported and encouraged as a file extension for <a class="reference external" href="https://jsonlines.org/">JSON
Lines</a> files (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4848">issue 4848</a>).</p>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.thumb_path</span></code></a> now receives the
source <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5504">issue 5504</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5508">issue 5508</a>).</p></li>
</ul>
</section>
<section id="id42">
<h5>Bug fixes<a class="headerlink" href="#id42" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>When using Google Cloud Storage with a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipeline</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FILES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_EXPIRES</span></code></a> now also works when
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> does not point at the root of your Google Cloud
Storage bucket (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5317">issue 5317</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5318">issue 5318</a>).</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command now supports <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">asynchronous callbacks</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5424">issue 5424</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5577">issue 5577</a>).</p></li>
<li><p>When using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command with a URL for which there is no
available spider, an exception is no longer raised (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3264">issue 3264</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3265">issue 3265</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5375">issue 5375</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5376">issue 5376</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5497">issue 5497</a>).</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> now gives higher priority to the <a class="reference external" href="https://en.wikipedia.org/wiki/Byte_order_mark">byte
order mark</a> when determining the text encoding of the response body,
following the <a class="reference external" href="https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding">HTML living standard</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5601">issue 5601</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5611">issue 5611</a>).</p>
</li>
<li><p>MIME sniffing takes the response body into account in FTP and HTTP/1.0
requests, as well as in cached requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4873">issue 4873</a>).</p></li>
<li><p>MIME sniffing now detects valid HTML 5 documents even if the <code class="docutils literal notranslate"><span class="pre">html</span></code> tag
is missing (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4873">issue 4873</a>).</p></li>
<li><p>An exception is now raised if <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ASYNCIO_EVENT_LOOP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ASYNCIO_EVENT_LOOP</span></code></a> has a value
that does not match the asyncio event loop actually installed
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5529">issue 5529</a>).</p></li>
<li><p>Fixed <code class="xref py py-meth docutils literal notranslate"><span class="pre">Headers.getlist</span></code>
returning only the last header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5515">issue 5515</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5526">issue 5526</a>).</p></li>
<li><p>Fixed <a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a> not ignoring the
<code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> file extension by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1837">issue 1837</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2067">issue 2067</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4066">issue 4066</a>)</p></li>
</ul>
</section>
<section id="id43">
<h5>Documentation<a class="headerlink" href="#id43" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Clarified the return type of <a class="reference internal" href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Spider.parse</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5602">issue 5602</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5608">issue 5608</a>).</p></li>
<li><p>To enable
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a>
to do <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli compression</a>, installing <a class="reference external" href="https://github.com/google/brotli">brotli</a> is now recommended instead
of installing <a class="reference external" href="https://github.com/python-hyper/brotlipy/">brotlipy</a>, as the former provides a more recent version of
brotli.</p>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">Signal documentation</span></a> now mentions <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-coroutines"><span class="std std-ref">coroutine
support</span></a> and uses it in code examples (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4852">issue 4852</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5358">issue 5358</a>).</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#bans"><span class="std std-ref">Avoiding getting banned</span></a> now recommends <a class="reference external" href="https://commoncrawl.org/">Common Crawl</a> instead of <a class="reference external" href="https://www.googleguide.com/cached_pages.html">Google cache</a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3582">issue 3582</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5432">issue 5432</a>).</p>
</li>
<li><p>The new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-components"><span class="std std-ref">Components</span></a> topic covers enforcing requirements on
Scrapy components, like <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middlewares</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions</span></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">spider middlewares</span></a>, and more; <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#enforce-asyncio-requirement"><span class="std std-ref">Enforcing asyncio as a requirement</span></a>
has also been added (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4978">issue 4978</a>).</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a> now indicates that setting values must be
<a class="reference external" href="https://docs.python.org/3/library/pickle.html#pickle-picklable" title="(in Python v3.13)"><span class="xref std std-ref">picklable</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5607">issue 5607</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5629">issue 5629</a>).</p></li>
<li><p>Removed outdated documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5446">issue 5446</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5373">issue 5373</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5369">issue 5369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5370">issue 5370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5554">issue 5554</a>).</p></li>
<li><p>Fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5442">issue 5442</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5455">issue 5455</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5457">issue 5457</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5461">issue 5461</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5538">issue 5538</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5553">issue 5553</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5558">issue 5558</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5624">issue 5624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5631">issue 5631</a>).</p></li>
<li><p>Fixed other issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5283">issue 5283</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5284">issue 5284</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5559">issue 5559</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5567">issue 5567</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5648">issue 5648</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5659">issue 5659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5665">issue 5665</a>).</p></li>
</ul>
</section>
<section id="id45">
<h5>Quality assurance<a class="headerlink" href="#id45" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added a continuous integration job to run <a class="reference external" href="https://twine.readthedocs.io/en/stable/#twine-check">twine check</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5655">issue 5655</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5656">issue 5656</a>).</p>
</li>
<li><p>Addressed test issues and warnings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5560">issue 5560</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5561">issue 5561</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5612">issue 5612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5617">issue 5617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5639">issue 5639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5645">issue 5645</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5662">issue 5662</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5671">issue 5671</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5675">issue 5675</a>).</p></li>
<li><p>Cleaned up code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4991">issue 4991</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4995">issue 4995</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5451">issue 5451</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5487">issue 5487</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5542">issue 5542</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5667">issue 5667</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5668">issue 5668</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5672">issue 5672</a>).</p></li>
<li><p>Applied minor code improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5661">issue 5661</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-6-3-2022-09-27">
<span id="release-2-6-3"></span><h4>Scrapy 2.6.3 (2022-09-27)<a class="headerlink" href="#scrapy-2-6-3-2022-09-27" title="Permalink to this heading">¶</a></h4>
<ul>
<li><p>Added support for <a class="reference external" href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a> 22.1.0, removing support for SSLv3
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5634">issue 5634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5635">issue 5635</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5636">issue 5636</a>).</p></li>
<li><p>Upgraded the minimum versions of the following dependencies:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a>: 2.0 → 3.3</p></li>
<li><p><a class="reference external" href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>: 16.2.0 → 21.0.0</p></li>
<li><p><a class="reference external" href="https://service-identity.readthedocs.io/en/stable/">service_identity</a>: 16.0.0 → 18.1.0</p></li>
<li><p><a class="reference external" href="https://twisted.org/">Twisted</a>: 17.9.0 → 18.9.0</p></li>
<li><p><a class="reference external" href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>: 4.1.3 → 5.0.0</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5621">issue 5621</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5632">issue 5632</a>)</p>
</li>
<li><p>Fixes test and documentation issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5612">issue 5612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5617">issue 5617</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5631">issue 5631</a>).</p></li>
</ul>
</section>
<section id="scrapy-2-6-2-2022-07-25">
<span id="release-2-6-2"></span><h4>Scrapy 2.6.2 (2022-07-25)<a class="headerlink" href="#scrapy-2-6-2-2022-07-25" title="Permalink to this heading">¶</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li><p>When <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>
processes a request with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, and that
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata includes proxy credentials,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> sets
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata every time they process a request.</p>
<p>Because of request retries and redirects, the same request can be processed
by downloader middlewares more than once, including both
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> and
any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata of a request to a new value, but fail to remove
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header from the previous value of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, causing the credentials of one proxy to be sent
to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior of
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> is now
as follows when processing a request:</p>
<ul>
<li><p>If the request being processed defines <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata that
includes credentials, the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is always
updated to feature those credentials.</p></li>
<li><p>If the request being processed defines <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata
without credentials, the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is removed
<em>unless</em> it was originally defined for the same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL, remove
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header.</p>
</li>
<li><p>If the request has no <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, or that metadata is a
falsy value (e.g. <code class="docutils literal notranslate"><span class="pre">None</span></code>), the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is
removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata but set the credentials through the
<code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header. Set proxy credentials through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata instead.</p>
</li>
</ul>
</li>
</ul>
<p>Also fixes the following regressions introduced in 2.6.0:</p>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> supports again crawling multiple
spiders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5435">issue 5435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5436">issue 5436</a>)</p></li>
<li><p>Installing a Twisted reactor before Scrapy does (e.g. importing
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">twisted.internet.reactor</span></code></a> somewhere at the module level) no longer
prevents Scrapy from starting, as long as a different reactor is not
specified in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5525">issue 5525</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5528">issue 5528</a>)</p></li>
<li><p>Fixed an exception that was being logged after the spider finished under
certain conditions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5437">issue 5437</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5440">issue 5440</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--output</span></code>/<code class="docutils literal notranslate"><span class="pre">-o</span></code> command-line parameter supports again a value
starting with a hyphen (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5444">issue 5444</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5445">issue 5445</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">-h</span></code> command no longer throws an error (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5481">issue 5481</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5482">issue 5482</a>)</p></li>
</ul>
</section>
<section id="scrapy-2-6-1-2022-03-01">
<span id="release-2-6-1"></span><h4>Scrapy 2.6.1 (2022-03-01)<a class="headerlink" href="#scrapy-2-6-1-2022-03-01" title="Permalink to this heading">¶</a></h4>
<p>Fixes a regression introduced in 2.6.0 that would unset the request method when
following redirects.</p>
</section>
<section id="scrapy-2-6-0-2022-03-01">
<span id="release-2-6-0"></span><h4>Scrapy 2.6.0 (2022-03-01)<a class="headerlink" href="#scrapy-2-6-0-2022-03-01" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="#security-fixes"><span class="std std-ref">Security fixes for cookie handling</span></a></p></li>
<li><p>Python 3.10 support</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">asyncio support</span></a> is no longer considered
experimental, and works out-of-the-box on Windows regardless of your Python
version</p></li>
<li><p>Feed exports now support <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> output paths and per-feed
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-filter"><span class="std std-ref">item filtering</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#post-processing"><span class="std std-ref">post-processing</span></a></p></li>
</ul>
<section id="security-fixes">
<span id="id46"></span><h5>Security bug fixes<a class="headerlink" href="#security-fixes" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>When a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object with cookies defined gets a
redirect response causing a new <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object to be
scheduled, the cookies defined in the original
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object are no longer copied into the new
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
<p>If you manually set the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header on a
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object and the domain name of the redirect
URL is not an exact match for the domain of the URL of the original
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, your <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header is now dropped
from the new <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
<p>The old behavior could be exploited by an attacker to gain access to your
cookies. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security advisory</a> for more
information.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
<code class="docutils literal notranslate"><span class="pre">example.com</span></code> and any subdomain) by defining the shared domain
suffix (e.g. <code class="docutils literal notranslate"><span class="pre">example.com</span></code>) as the cookie domain when defining
your cookies. See the documentation of the
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class for more information.</p>
</div>
</li>
<li><p>When the domain of a cookie, either received in the <code class="docutils literal notranslate"><span class="pre">Set-Cookie</span></code> header
of a response or defined in a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, is set
to a <a class="reference external" href="https://publicsuffix.org/">public suffix</a>, the cookie is now
ignored unless the cookie domain is the same as the request domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies from a
controlled domain into your cookiejar that could be sent to other domains
not controlled by the attacker. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96 security
advisory</a> for more information.</p>
</li>
</ul>
</section>
<section id="id47">
<h5>Modified requirements<a class="headerlink" href="#id47" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="reference external" href="https://pypi.org/project/h2/">h2</a> dependency is now optional, only needed to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#http2"><span class="std std-ref">enable HTTP/2 support</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5113">issue 5113</a>)</p>
</li>
</ul>
</section>
<section id="id48">
<h5>Backward-incompatible changes<a class="headerlink" href="#id48" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">formdata</span></code> parameter of <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code>, if specified
for a non-POST request, now overrides the URL query string, instead of
being appended to it. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2919">issue 2919</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3579">issue 3579</a>)</p></li>
<li><p>When a function is assigned to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> setting, now
the return value of that function, and not the <code class="docutils literal notranslate"><span class="pre">params</span></code> input parameter,
will determine the feed URI parameters, unless that return value is
<code class="docutils literal notranslate"><span class="pre">None</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4962">issue 4962</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4966">issue 4966</a>)</p></li>
<li><p>In <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.engine.ExecutionEngine</span></code>, methods
<code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">download()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">schedule()</span></code>,
and <code class="xref py py-meth docutils literal notranslate"><span class="pre">spider_is_idle()</span></code>
now raise <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">RuntimeError</span></code></a> if called before
<code class="xref py py-meth docutils literal notranslate"><span class="pre">open_spider()</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5090">issue 5090</a>)</p>
<p>These methods used to assume that
<code class="xref py py-attr docutils literal notranslate"><span class="pre">ExecutionEngine.slot</span></code> had
been defined by a prior call to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">open_spider()</span></code>, so they were
raising <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AttributeError</span></code></a> instead.</p>
</li>
<li><p>If the API of the configured <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">scheduler</span></a> does not
meet expectations, <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a> is now raised at startup time. Before,
other exceptions would be raised at run time. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3559">issue 3559</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">_encoding</span></code> field of serialized <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects
is now named <code class="docutils literal notranslate"><span class="pre">encoding</span></code>, in line with all other fields (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5130">issue 5130</a>)</p></li>
</ul>
</section>
<section id="id49">
<h5>Deprecation removals<a class="headerlink" href="#id49" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.http.TextResponse.body_as_unicode</span></code>, deprecated in Scrapy 2.2, has
now been removed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5393">issue 5393</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.item.BaseItem</span></code>, deprecated in Scrapy 2.2, has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5398">issue 5398</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.item.DictItem</span></code>, deprecated in Scrapy 1.8, has now been removed.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5398">issue 5398</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.Spider.make_requests_from_url</span></code>, deprecated in Scrapy 1.4, has now
been removed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4178">issue 4178</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4356">issue 4356</a>)</p></li>
</ul>
</section>
<section id="id50">
<h5>Deprecations<a class="headerlink" href="#id50" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>When a function is assigned to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> setting,
returning <code class="docutils literal notranslate"><span class="pre">None</span></code> or modifying the <code class="docutils literal notranslate"><span class="pre">params</span></code> input parameter is now
deprecated. Return a new dictionary instead. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4962">issue 4962</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4966">issue 4966</a>)</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.reqser</span></code> is deprecated. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5130">issue 5130</a>)</p>
<ul class="simple">
<li><p>Instead of <code class="xref py py-func docutils literal notranslate"><span class="pre">request_to_dict()</span></code>, use the new
<a class="reference internal" href="index.html#scrapy.http.Request.to_dict" title="scrapy.http.Request.to_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.to_dict</span></code></a> method.</p></li>
<li><p>Instead of <code class="xref py py-func docutils literal notranslate"><span class="pre">request_from_dict()</span></code>, use the new
<a class="reference internal" href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.request_from_dict()</span></code></a> function.</p></li>
</ul>
</li>
<li><p>In <code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.squeues</span></code>, the following queue classes are deprecated:
<code class="xref py py-class docutils literal notranslate"><span class="pre">PickleFifoDiskQueueNonRequest</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">PickleLifoDiskQueueNonRequest</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalFifoDiskQueueNonRequest</span></code>,
and <code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalLifoDiskQueueNonRequest</span></code>. You should
instead use:
<code class="xref py py-class docutils literal notranslate"><span class="pre">PickleFifoDiskQueue</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">PickleLifoDiskQueue</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalFifoDiskQueue</span></code>,
and <code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalLifoDiskQueue</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5117">issue 5117</a>)</p></li>
<li><p>Many aspects of <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.engine.ExecutionEngine</span></code> that come from
a time when this class could handle multiple <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>
objects at a time have been deprecated. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5090">issue 5090</a>)</p>
<ul>
<li><p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">has_capacity()</span></code> method
is deprecated.</p></li>
<li><p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">schedule()</span></code> method is
deprecated, use <code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code> or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">download()</span></code> instead.</p></li>
<li><p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">open_spiders</span></code> attribute
is deprecated, use <code class="xref py py-attr docutils literal notranslate"><span class="pre">spider</span></code>
instead.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">spider</span></code> parameter is deprecated for the following methods:</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">spider_is_idle()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">download()</span></code></p></li>
</ul>
<p>Instead, call <code class="xref py py-meth docutils literal notranslate"><span class="pre">open_spider()</span></code>
first to set the <a class="reference internal" href="index.html#scrapy.Spider" title="scrapy.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object.</p>
</li>
</ul>
</li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.response.response_httprepr()</span></code> is now deprecated.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4972">issue 4972</a>)</p></li>
</ul>
</section>
<section id="id51">
<h5>New features<a class="headerlink" href="#id51" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>You can now use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-filter"><span class="std std-ref">item filtering</span></a> to control which items
are exported to each output feed. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4575">issue 4575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5178">issue 5178</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5161">issue 5161</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5203">issue 5203</a>)</p></li>
<li><p>You can now apply <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#post-processing"><span class="std std-ref">post-processing</span></a> to feeds, and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#builtin-plugins"><span class="std std-ref">built-in post-processing plugins</span></a> are provided for
output file compression. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2174">issue 2174</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5168">issue 5168</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5190">issue 5190</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting now supports <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> objects as
keys. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5383">issue 5383</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5384">issue 5384</a>)</p></li>
<li><p>Enabling <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">asyncio</span></a> while using Windows and Python 3.8
or later will automatically switch the asyncio event loop to one that
allows Scrapy to work. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#asyncio-windows"><span class="std std-ref">Windows-specific notes</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4976">issue 4976</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5315">issue 5315</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command now supports a start URL instead of a
domain name. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4439">issue 4439</a>)</p></li>
<li><p><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.defer</span></code> gained 2 new functions,
<a class="reference internal" href="index.html#scrapy.utils.defer.deferred_to_future" title="scrapy.utils.defer.deferred_to_future"><code class="xref py py-func docutils literal notranslate"><span class="pre">deferred_to_future()</span></code></a> and
<a class="reference internal" href="index.html#scrapy.utils.defer.maybe_deferred_to_future" title="scrapy.utils.defer.maybe_deferred_to_future"><code class="xref py py-func docutils literal notranslate"><span class="pre">maybe_deferred_to_future()</span></code></a>, to help <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#asyncio-await-dfd"><span class="std std-ref">await
on Deferreds when using the asyncio reactor</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5288">issue 5288</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">Amazon S3 feed export storage</span></a> gained
support for <a class="reference external" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html">temporary security credentials</a>
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_SESSION_TOKEN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_SESSION_TOKEN</span></code></a>) and endpoint customization
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a>). (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4998">issue 4998</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5210">issue 5210</a>)</p>
</li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FILE_APPEND"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE_APPEND</span></code></a> setting to allow truncating the log file.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5279">issue 5279</a>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cookies</span></code> values that are
<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a> or <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a> are cast to <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5252">issue 5252</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5253">issue 5253</a>)</p></li>
<li><p>You may now raise <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a> from a handler of
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_idle"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_idle</span></code></a> signal to customize the reason why the spider is
stopping. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5191">issue 5191</a>)</p></li>
<li><p>When using
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>, the
proxy URL for non-HTTPS HTTP/1.1 requests no longer needs to include a URL
scheme. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4505">issue 4505</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4649">issue 4649</a>)</p></li>
<li><p>All built-in queues now expose a <code class="docutils literal notranslate"><span class="pre">peek</span></code> method that returns the next
queue object (like <code class="docutils literal notranslate"><span class="pre">pop</span></code>) but does not remove the returned object from
the queue. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5112">issue 5112</a>)</p>
<p>If the underlying queue does not support peeking (e.g. because you are not
using <code class="docutils literal notranslate"><span class="pre">queuelib</span></code> 1.6.1 or later), the <code class="docutils literal notranslate"><span class="pre">peek</span></code> method raises
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">NotImplementedError</span></code></a>.</p>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> now have
an <code class="docutils literal notranslate"><span class="pre">attributes</span></code> attribute that makes subclassing easier. For
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, it also allows subclasses to work with
<a class="reference internal" href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.request.request_from_dict()</span></code></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1877">issue 1877</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5130">issue 5130</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5218">issue 5218</a>)</p></li>
<li><p>The <a class="reference internal" href="index.html#scrapy.core.scheduler.BaseScheduler.open" title="scrapy.core.scheduler.BaseScheduler.open"><code class="xref py py-meth docutils literal notranslate"><span class="pre">open()</span></code></a> and
<a class="reference internal" href="index.html#scrapy.core.scheduler.BaseScheduler.close" title="scrapy.core.scheduler.BaseScheduler.close"><code class="xref py py-meth docutils literal notranslate"><span class="pre">close()</span></code></a> methods of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">scheduler</span></a> are now optional. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3559">issue 3559</a>)</p></li>
<li><p>HTTP/1.1 <code class="xref py py-exc docutils literal notranslate"><span class="pre">TunnelError</span></code>
exceptions now only truncate response bodies longer than 1000 characters,
instead of those longer than 32 characters, making it easier to debug such
errors. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4881">issue 4881</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5007">issue 5007</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> now supports non-text responses.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5145">issue 5145</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5269">issue 5269</a>)</p></li>
</ul>
</section>
<section id="id52">
<h5>Bug fixes<a class="headerlink" href="#id52" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ASYNCIO_EVENT_LOOP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ASYNCIO_EVENT_LOOP</span></code></a> settings
are no longer ignored if defined in <a class="reference internal" href="index.html#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4485">issue 4485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5352">issue 5352</a>)</p></li>
<li><p>Removed a module-level Twisted reactor import that could prevent
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">using the asyncio reactor</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5357">issue 5357</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command works with existing folders again.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4665">issue 4665</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4676">issue 4676</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> setting now behaves as documented.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4962">issue 4962</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4966">issue 4966</a>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code> once again allows the
<code class="docutils literal notranslate"><span class="pre">callback</span></code> keyword. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5237">issue 5237</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5251">issue 5251</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5264">issue 5264</a>)</p></li>
<li><p>Made <a class="reference internal" href="index.html#scrapy.utils.response.open_in_browser" title="scrapy.utils.response.open_in_browser"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.response.open_in_browser()</span></code></a> support more complex
HTML. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5319">issue 5319</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5320">issue 5320</a>)</p></li>
<li><p>Fixed <a class="reference internal" href="index.html#scrapy.spiders.CSVFeedSpider.quotechar" title="scrapy.spiders.CSVFeedSpider.quotechar"><code class="xref py py-attr docutils literal notranslate"><span class="pre">CSVFeedSpider.quotechar</span></code></a> being interpreted as the CSV file
encoding. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5391">issue 5391</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5394">issue 5394</a>)</p></li>
<li><p>Added missing <a class="reference external" href="https://pypi.org/project/setuptools/">setuptools</a> to the list of dependencies. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5122">issue 5122</a>)</p>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
now also works as expected with links that have comma-separated <code class="docutils literal notranslate"><span class="pre">rel</span></code>
attribute values including <code class="docutils literal notranslate"><span class="pre">nofollow</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5225">issue 5225</a>)</p></li>
<li><p>Fixed a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a> that could be raised during <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed export</span></a> parameter parsing. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5359">issue 5359</a>)</p></li>
</ul>
</section>
<section id="id53">
<h5>Documentation<a class="headerlink" href="#id53" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">asyncio support</span></a> is no longer considered
experimental. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5332">issue 5332</a>)</p></li>
<li><p>Included <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#asyncio-windows"><span class="std std-ref">Windows-specific help for asyncio usage</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4976">issue 4976</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5315">issue 5315</a>)</p></li>
<li><p>Rewrote <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-headless-browsing"><span class="std std-ref">Using a headless browser</span></a> with up-to-date best practices.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4484">issue 4484</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4613">issue 4613</a>)</p></li>
<li><p>Documented <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-file-naming"><span class="std std-ref">local file naming in media pipelines</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5069">issue 5069</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5152">issue 5152</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq"><span class="std std-ref">Frequently Asked Questions</span></a> now covers spider file name collision issues. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2680">issue 2680</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3669">issue 3669</a>)</p></li>
<li><p>Provided better context and instructions to disable the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-URLLENGTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">URLLENGTH_LIMIT</span></code></a> setting. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5135">issue 5135</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5250">issue 5250</a>)</p></li>
<li><p>Documented that Reppy parser does not support Python 3.9+.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5226">issue 5226</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5231">issue 5231</a>)</p></li>
<li><p>Documented <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-scheduler"><span class="std std-ref">the scheduler component</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3537">issue 3537</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3559">issue 3559</a>)</p></li>
<li><p>Documented the method used by <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipelines</span></a> to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#file-expiration"><span class="std std-ref">determine if a file has expired</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5120">issue 5120</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5254">issue 5254</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-multiple-spiders"><span class="std std-ref">Running multiple spiders in the same process</span></a> now features
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.project.get_project_settings()</span></code> usage. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5070">issue 5070</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#run-multiple-spiders"><span class="std std-ref">Running multiple spiders in the same process</span></a> now covers what happens when you define
different per-spider values for some settings that cannot differ at run
time. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4485">issue 4485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5352">issue 5352</a>)</p></li>
<li><p>Extended the documentation of the
<a class="reference internal" href="index.html#scrapy.extensions.statsmailer.StatsMailer" title="scrapy.extensions.statsmailer.StatsMailer"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsMailer</span></code></a> extension.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5199">issue 5199</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5217">issue 5217</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-JOBDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">JOBDIR</span></code></a> to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5173">issue 5173</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5224">issue 5224</a>)</p></li>
<li><p>Documented <code class="xref py py-attr docutils literal notranslate"><span class="pre">Spider.attribute</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5174">issue 5174</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5244">issue 5244</a>)</p></li>
<li><p>Documented <a class="reference internal" href="index.html#scrapy.http.TextResponse.urljoin" title="scrapy.http.TextResponse.urljoin"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.urljoin</span></code></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1582">issue 1582</a>)</p></li>
<li><p>Added the <code class="docutils literal notranslate"><span class="pre">body_length</span></code> parameter to the documented signature of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-headers_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">headers_received</span></code></a> signal. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5270">issue 5270</a>)</p></li>
<li><p>Clarified <a class="reference internal" href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SelectorList.get</span></code></a> usage
in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5256">issue 5256</a>)</p></li>
<li><p>The documentation now features the shortest import path of classes with
multiple import paths. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2733">issue 2733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5099">issue 5099</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">quotes.toscrape.com</span></code> references now use HTTPS instead of HTTP.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5395">issue 5395</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5396">issue 5396</a>)</p></li>
<li><p>Added a link to <a class="reference external" href="https://discord.com/invite/mv3yErfpvq">our Discord server</a>
to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#getting-help"><span class="std std-ref">Getting help</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5421">issue 5421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5422">issue 5422</a>)</p></li>
<li><p>The pronunciation of the project name is now <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-overview"><span class="std std-ref">officially</span></a> /ˈskreɪpaɪ/. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5280">issue 5280</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5281">issue 5281</a>)</p></li>
<li><p>Added the Scrapy logo to the README. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5255">issue 5255</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5258">issue 5258</a>)</p></li>
<li><p>Fixed issues and implemented minor improvements. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3155">issue 3155</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4335">issue 4335</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5074">issue 5074</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5098">issue 5098</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5134">issue 5134</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5180">issue 5180</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5194">issue 5194</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5239">issue 5239</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5266">issue 5266</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5271">issue 5271</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5273">issue 5273</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5274">issue 5274</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5276">issue 5276</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5347">issue 5347</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5356">issue 5356</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5414">issue 5414</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5415">issue 5415</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5416">issue 5416</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5419">issue 5419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5420">issue 5420</a>)</p></li>
</ul>
</section>
<section id="id54">
<h5>Quality Assurance<a class="headerlink" href="#id54" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added support for Python 3.10. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5212">issue 5212</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5221">issue 5221</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5265">issue 5265</a>)</p></li>
<li><p>Significantly reduced memory usage by
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.response.response_httprepr()</span></code>, used by the
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.stats.DownloaderStats" title="scrapy.downloadermiddlewares.stats.DownloaderStats"><code class="xref py py-class docutils literal notranslate"><span class="pre">DownloaderStats</span></code></a> downloader
middleware, which is enabled by default. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4964">issue 4964</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4972">issue 4972</a>)</p></li>
<li><p>Removed uses of the deprecated <a class="reference external" href="https://docs.python.org/3/library/optparse.html#module-optparse" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">optparse</span></code></a> module. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5366">issue 5366</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5374">issue 5374</a>)</p></li>
<li><p>Extended typing hints. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5077">issue 5077</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5090">issue 5090</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5100">issue 5100</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5108">issue 5108</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5171">issue 5171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5215">issue 5215</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5334">issue 5334</a>)</p></li>
<li><p>Improved tests, fixed CI issues, removed unused code. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5094">issue 5094</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5157">issue 5157</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5162">issue 5162</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5198">issue 5198</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5207">issue 5207</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5208">issue 5208</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5229">issue 5229</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5298">issue 5298</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5299">issue 5299</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5310">issue 5310</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5316">issue 5316</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5333">issue 5333</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5388">issue 5388</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5389">issue 5389</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5400">issue 5400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5401">issue 5401</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5404">issue 5404</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5405">issue 5405</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5407">issue 5407</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5410">issue 5410</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5412">issue 5412</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5425">issue 5425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5427">issue 5427</a>)</p></li>
<li><p>Implemented improvements for contributors. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5080">issue 5080</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5082">issue 5082</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5177">issue 5177</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5200">issue 5200</a>)</p></li>
<li><p>Implemented cleanups. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5095">issue 5095</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5106">issue 5106</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5209">issue 5209</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5228">issue 5228</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5235">issue 5235</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5245">issue 5245</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5246">issue 5246</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5292">issue 5292</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5314">issue 5314</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5322">issue 5322</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-5-1-2021-10-05">
<span id="release-2-5-1"></span><h4>Scrapy 2.5.1 (2021-10-05)<a class="headerlink" href="#scrapy-2-5-1-2021-10-05" title="Permalink to this heading">¶</a></h4>
<ul>
<li><p><strong>Security bug fix:</strong></p>
<p>If you use
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>
(i.e. the <code class="docutils literal notranslate"><span class="pre">http_user</span></code> and <code class="docutils literal notranslate"><span class="pre">http_pass</span></code> spider attributes) for HTTP
authentication, any request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to unintended
domains, you must now additionally set a new, additional spider attribute,
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code>, and point it to the specific domain to which the
authentication credentials must be sent.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute is not set, the domain of the
first request will be considered the HTTP authentication target, and
authentication credentials will only be sent in requests targeting that
domain.</p>
<p>If you need to send the same HTTP authentication credentials to multiple
domains, you can use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.2)"><code class="xref py py-func docutils literal notranslate"><span class="pre">w3lib.http.basic_auth_header()</span></code></a> instead to
set the value of the <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header of your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP authentication
credentials to any domain, set the <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute
to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>Finally, if you are a user of <a class="reference external" href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>, know that this version of
Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will
need to upgrade scrapy-splash to a greater version for it to continue to
work.</p>
</li>
</ul>
</section>
<section id="scrapy-2-5-0-2021-04-06">
<span id="release-2-5-0"></span><h4>Scrapy 2.5.0 (2021-04-06)<a class="headerlink" href="#scrapy-2-5-0-2021-04-06" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Official Python 3.9 support</p></li>
<li><p>Experimental <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#http2"><span class="std std-ref">HTTP/2 support</span></a></p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_retry_request()</span></code></a> function
to retry requests from spider callbacks</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a> signal that allows stopping
downloads early</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response.protocol</span></code></a> attribute</p></li>
</ul>
<section id="id55">
<h5>Deprecation removals<a class="headerlink" href="#id55" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Removed all code that <a class="hxr-hoverxref hxr-tooltip reference internal" href="#id115"><span class="std std-ref">was deprecated in 1.7.0</span></a> and
had not <a class="hxr-hoverxref hxr-tooltip reference internal" href="#id63"><span class="std std-ref">already been removed in 2.4.0</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4901">issue 4901</a>)</p></li>
<li><p>Removed support for the <code class="docutils literal notranslate"><span class="pre">SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</span></code> environment
variable, <a class="hxr-hoverxref hxr-tooltip reference internal" href="#id107"><span class="std std-ref">deprecated in 1.8.0</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4912">issue 4912</a>)</p></li>
</ul>
</section>
<section id="id56">
<h5>Deprecations<a class="headerlink" href="#id56" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.py36</span></code> module is now deprecated in favor of
<code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.asyncgen</span></code>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4900">issue 4900</a>)</p></li>
</ul>
</section>
<section id="id57">
<h5>New features<a class="headerlink" href="#id57" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Experimental <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#http2"><span class="std std-ref">HTTP/2 support</span></a> through a new download handler
that can be assigned to the <code class="docutils literal notranslate"><span class="pre">https</span></code> protocol in the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> setting.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1854">issue 1854</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4769">issue 4769</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5058">issue 5058</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5059">issue 5059</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5066">issue 5066</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.retry.get_retry_request()</span></code></a>
function may be used from spider callbacks or middlewares to handle the
retrying of a request beyond the scenarios that
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> supports.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3590">issue 3590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3685">issue 3685</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4902">issue 4902</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received"><code class="xref py py-class docutils literal notranslate"><span class="pre">headers_received</span></code></a> signal gives early access
to response headers and allows <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">stopping downloads</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1772">issue 1772</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4897">issue 4897</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.protocol</span></code></a>
attribute gives access to the string that identifies the protocol used to
download a response. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4878">issue 4878</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats</span></a> now include the following entries that indicate
the number of successes and failures in storing
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feeds</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">feedexport</span><span class="o">/</span><span class="n">success_count</span><span class="o">/&lt;</span><span class="n">storage</span> <span class="nb">type</span><span class="o">&gt;</span>
<span class="n">feedexport</span><span class="o">/</span><span class="n">failed_count</span><span class="o">/&lt;</span><span class="n">storage</span> <span class="nb">type</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">&lt;storage</span> <span class="pre">type&gt;</span></code> is the feed storage backend class name, such as
<code class="xref py py-class docutils literal notranslate"><span class="pre">FileFeedStorage</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">FTPFeedStorage</span></code>.</p>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3947">issue 3947</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4850">issue 4850</a>)</p>
</li>
<li><p>The <a class="reference internal" href="index.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UrlLengthMiddleware</span></code></a> spider
middleware now logs ignored URLs with <code class="docutils literal notranslate"><span class="pre">INFO</span></code> <a class="reference external" href="https://docs.python.org/3/library/logging.html#levels" title="(in Python v3.13)"><span class="xref std std-ref">logging level</span></a> instead of <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code>, and it now includes the following entry
into <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stats"><span class="std std-ref">stats</span></a> to keep track of the number of ignored
URLs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">urllength</span><span class="o">/</span><span class="n">request_ignored_count</span>
</pre></div>
</div>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5036">issue 5036</a>)</p>
</li>
<li><p>The
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a>
downloader middleware now logs the number of decompressed responses and the
total count of resulting bytes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">httpcompression</span><span class="o">/</span><span class="n">response_bytes</span>
<span class="n">httpcompression</span><span class="o">/</span><span class="n">response_count</span>
</pre></div>
</div>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4797">issue 4797</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4799">issue 4799</a>)</p>
</li>
</ul>
</section>
<section id="id58">
<h5>Bug fixes<a class="headerlink" href="#id58" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fixed installation on PyPy installing PyDispatcher in addition to
PyPyDispatcher, which could prevent Scrapy from working depending on which
package got imported. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4710">issue 4710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4814">issue 4814</a>)</p></li>
<li><p>When inspecting a callback to check if it is a generator that also returns
a value, an exception is no longer raised if the callback has a docstring
with lower indentation than the following code.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4477">issue 4477</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4935">issue 4935</a>)</p></li>
<li><p>The <a class="reference external" href="https://datatracker.ietf.org/doc/html/rfc2616#section-14.13">Content-Length</a>
header is no longer omitted from responses when using the default, HTTP/1.1
download handler (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>).
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5009">issue 5009</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5034">issue 5034</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5045">issue 5045</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5057">issue 5057</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5062">issue 5062</a>)</p></li>
<li><p>Setting the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-handle_httpstatus_all"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code></a> request meta key to <code class="docutils literal notranslate"><span class="pre">False</span></code>
now has the same effect as not setting it at all, instead of having the
same effect as setting it to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3851">issue 3851</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4694">issue 4694</a>)</p></li>
</ul>
</section>
<section id="id59">
<h5>Documentation<a class="headerlink" href="#id59" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added instructions to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-install-windows"><span class="std std-ref">install Scrapy in Windows using pip</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4715">issue 4715</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4736">issue 4736</a>)</p></li>
<li><p>Logging documentation now includes <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging-advanced-customization"><span class="std std-ref">additional ways to filter logs</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4216">issue 4216</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4257">issue 4257</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4965">issue 4965</a>)</p></li>
<li><p>Covered how to deal with long lists of allowed domains in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq"><span class="std std-ref">FAQ</span></a>. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2263">issue 2263</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3667">issue 3667</a>)</p></li>
<li><p>Covered <a class="reference external" href="https://github.com/scrapy/scrapy-bench">scrapy-bench</a> in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#benchmarking"><span class="std std-ref">Benchmarking</span></a>.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4996">issue 4996</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5016">issue 5016</a>)</p></li>
<li><p>Clarified that one <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-extensions"><span class="std std-ref">extension</span></a> instance is created
per crawler.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5014">issue 5014</a>)</p></li>
<li><p>Fixed some errors in examples.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4829">issue 4829</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4830">issue 4830</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4907">issue 4907</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4909">issue 4909</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5008">issue 5008</a>)</p></li>
<li><p>Fixed some external links, typos, and so on.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4892">issue 4892</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4899">issue 4899</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4936">issue 4936</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4942">issue 4942</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5005">issue 5005</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5063">issue 5063</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-request-meta"><span class="std std-ref">list of Request.meta keys</span></a> is now sorted
alphabetically.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5061">issue 5061</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5065">issue 5065</a>)</p></li>
<li><p>Updated references to Scrapinghub, which is now called Zyte.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4973">issue 4973</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5072">issue 5072</a>)</p></li>
<li><p>Added a mention to contributors in the README. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4956">issue 4956</a>)</p></li>
<li><p>Reduced the top margin of lists. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4974">issue 4974</a>)</p></li>
</ul>
</section>
<section id="id60">
<h5>Quality Assurance<a class="headerlink" href="#id60" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Made Python 3.9 support official (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4757">issue 4757</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4759">issue 4759</a>)</p></li>
<li><p>Extended typing hints (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4895">issue 4895</a>)</p></li>
<li><p>Fixed deprecated uses of the Twisted API.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4940">issue 4940</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4950">issue 4950</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5073">issue 5073</a>)</p></li>
<li><p>Made our tests run with the new pip resolver.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4710">issue 4710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4814">issue 4814</a>)</p></li>
<li><p>Added tests to ensure that <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#coroutine-support"><span class="std std-ref">coroutine support</span></a>
is tested. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4987">issue 4987</a>)</p></li>
<li><p>Migrated from Travis CI to GitHub Actions. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4924">issue 4924</a>)</p></li>
<li><p>Fixed CI issues.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4986">issue 4986</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5020">issue 5020</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5022">issue 5022</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5027">issue 5027</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5052">issue 5052</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/5053">issue 5053</a>)</p></li>
<li><p>Implemented code refactorings, style fixes and cleanups.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4911">issue 4911</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4982">issue 4982</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5001">issue 5001</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5002">issue 5002</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/5076">issue 5076</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-4-1-2020-11-17">
<span id="release-2-4-1"></span><h4>Scrapy 2.4.1 (2020-11-17)<a class="headerlink" href="#scrapy-2-4-1-2020-11-17" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Fixed <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> overwrite support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4845">issue 4845</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4857">issue 4857</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4859">issue 4859</a>)</p></li>
<li><p>Fixed the AsyncIO event loop handling, which could make code hang
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4855">issue 4855</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4872">issue 4872</a>)</p></li>
<li><p>Fixed the IPv6-capable DNS resolver
<code class="xref py py-class docutils literal notranslate"><span class="pre">CachingHostnameResolver</span></code> for download handlers
that call
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve" title="(in Twisted)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reactor.resolve</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4802">issue 4802</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4803">issue 4803</a>)</p></li>
<li><p>Fixed the output of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command showing placeholders
instead of the import path of the generated spider module (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4874">issue 4874</a>)</p></li>
<li><p>Migrated Windows CI from Azure Pipelines to GitHub Actions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4869">issue 4869</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4876">issue 4876</a>)</p></li>
</ul>
</section>
<section id="scrapy-2-4-0-2020-10-11">
<span id="release-2-4-0"></span><h4>Scrapy 2.4.0 (2020-10-11)<a class="headerlink" href="#scrapy-2-4-0-2020-10-11" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul>
<li><p>Python 3.5 support has been dropped.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">file_path</span></code> method of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipelines</span></a>
can now access the source <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a>.</p>
<p>This allows you to set a download file path based on item data.</p>
</li>
<li><p>The new <code class="docutils literal notranslate"><span class="pre">item_export_kwargs</span></code> key of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting allows
to define keyword parameters to pass to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporter classes</span></a></p></li>
<li><p>You can now choose whether <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a>
overwrite or append to the output file.</p>
<p>For example, when using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a>
commands, you can use the <code class="docutils literal notranslate"><span class="pre">-O</span></code> option instead of <code class="docutils literal notranslate"><span class="pre">-o</span></code> to overwrite the
output file.</p>
</li>
<li><p>Zstd-compressed responses are now supported if <a class="reference external" href="https://pypi.org/project/zstandard/">zstandard</a> is installed.</p></li>
<li><p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead.</p></li>
</ul>
<section id="id61">
<h5>Modified requirements<a class="headerlink" href="#id61" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Python 3.6 or greater is now required; support for Python 3.5 has been
dropped</p>
<p>As a result:</p>
<ul class="simple">
<li><p>When using PyPy, PyPy 7.2.0 or greater <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-python-versions"><span class="std std-ref">is now required</span></a></p></li>
<li><p>For Amazon S3 storage support in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">feed exports</span></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#media-pipelines-s3"><span class="std std-ref">media pipelines</span></a>, <a class="reference external" href="https://github.com/boto/botocore">botocore</a> 1.4.87 or greater is now required</p></li>
<li><p>To use the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#images-pipeline"><span class="std std-ref">images pipeline</span></a>, <a class="reference external" href="https://python-pillow.org/">Pillow</a> 4.0.0 or
greater is now required</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4718">issue 4718</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4732">issue 4732</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4733">issue 4733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4742">issue 4742</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4743">issue 4743</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4764">issue 4764</a>)</p>
</li>
</ul>
</section>
<section id="id62">
<h5>Backward-incompatible changes<a class="headerlink" href="#id62" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p><a class="reference internal" href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code></a> once again
discards cookies defined in <a class="reference internal" href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.headers</span></code></a>.</p>
<p>We decided to revert this bug fix, introduced in Scrapy 2.2.0, because it
was reported that the current implementation could break existing code.</p>
<p>If you need to set cookies for a request, use the <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request.cookies</span></code></a> parameter.</p>
<p>A future version of Scrapy will include a new, better implementation of the
reverted bug fix.</p>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4717">issue 4717</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4823">issue 4823</a>)</p>
</li>
</ul>
</section>
<section id="id63">
<span id="id64"></span><h5>Deprecation removals<a class="headerlink" href="#id63" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.extensions.feedexport.S3FeedStorage</span></code> no longer reads the
values of <code class="docutils literal notranslate"><span class="pre">access_key</span></code> and <code class="docutils literal notranslate"><span class="pre">secret_key</span></code> from the running project
settings when they are not passed to its <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method; you must
either pass those parameters to its <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method or use
<code class="xref py py-class docutils literal notranslate"><span class="pre">S3FeedStorage.from_crawler</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4356">issue 4356</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4411">issue 4411</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4688">issue 4688</a>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">Rule.process_request</span></code>
no longer admits callables which expect a single <code class="docutils literal notranslate"><span class="pre">request</span></code> parameter,
rather than both <code class="docutils literal notranslate"><span class="pre">request</span></code> and <code class="docutils literal notranslate"><span class="pre">response</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4818">issue 4818</a>)</p></li>
</ul>
</section>
<section id="id65">
<h5>Deprecations<a class="headerlink" href="#id65" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>In custom <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipelines</span></a>, signatures that
do not accept a keyword-only <code class="docutils literal notranslate"><span class="pre">item</span></code> parameter in any of the  methods that
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#media-pipeline-item-parameter"><span class="std std-ref">now support this parameter</span></a> are now
deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4628">issue 4628</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4686">issue 4686</a>)</p></li>
<li><p>In custom <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage"><span class="std std-ref">feed storage backend classes</span></a>,
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method signatures that do not accept a keyword-only
<code class="docutils literal notranslate"><span class="pre">feed_options</span></code> parameter are now deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/716">issue 716</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4512">issue 4512</a>)</p></li>
<li><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.python.WeakKeyCache</span></code> class is now deprecated
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4684">issue 4684</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4701">issue 4701</a>)</p></li>
<li><p>The <code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.boto.is_botocore()</span></code> function is now deprecated, use
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.boto.is_botocore_available()</span></code> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4734">issue 4734</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4776">issue 4776</a>)</p></li>
</ul>
</section>
<section id="id66">
<h5>New features<a class="headerlink" href="#id66" title="Permalink to this heading">¶</a></h5>
<ul id="media-pipeline-item-parameter">
<li><p>The following methods of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipelines</span></a> now
accept an <code class="docutils literal notranslate"><span class="pre">item</span></code> keyword-only parameter containing the source
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a>:</p>
<ul class="simple">
<li><p>In <a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.files.FilesPipeline</span></code></a>:</p>
<ul>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_downloaded()</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">media_downloaded()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">media_to_download()</span></code></p></li>
</ul>
</li>
<li><p>In <a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pipelines.images.ImagesPipeline</span></code></a>:</p>
<ul>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_downloaded()</span></code></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_images()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">image_downloaded()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">media_downloaded()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">media_to_download()</span></code></p></li>
</ul>
</li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4628">issue 4628</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4686">issue 4686</a>)</p>
</li>
<li><p>The new <code class="docutils literal notranslate"><span class="pre">item_export_kwargs</span></code> key of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting allows
to define keyword parameters to pass to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-exporters"><span class="std std-ref">item exporter classes</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4606">issue 4606</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4768">issue 4768</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a> gained overwrite support:</p>
<ul class="simple">
<li><p>When using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> commands, you
can use the <code class="docutils literal notranslate"><span class="pre">-O</span></code> option instead of <code class="docutils literal notranslate"><span class="pre">-o</span></code> to overwrite the output
file</p></li>
<li><p>You can use the <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> key in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting to
configure whether to overwrite the output file (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or append to
its content (<code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> methods of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage"><span class="std std-ref">feed storage
backend classes</span></a> now receive a new keyword-only
parameter, <code class="docutils literal notranslate"><span class="pre">feed_options</span></code>, which is a dictionary of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#feed-options"><span class="std std-ref">feed
options</span></a></p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/716">issue 716</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4512">issue 4512</a>)</p>
</li>
<li><p>Zstd-compressed responses are now supported if <a class="reference external" href="https://pypi.org/project/zstandard/">zstandard</a> is installed
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4831">issue 4831</a>)</p></li>
<li><p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3870">issue 3870</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3873">issue 3873</a>).</p>
<p>This includes also settings where only part of its value is made of an
import path, such as <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> or
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>.</p>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader middlewares</span></a> can now
override <a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-class docutils literal notranslate"><span class="pre">response.request</span></code></a>.</p>
<p>If a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">downloader middleware</span></a> returns
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object from
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
or
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a>
with a custom <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object assigned to
<a class="reference internal" href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-class docutils literal notranslate"><span class="pre">response.request</span></code></a>:</p>
<ul class="simple">
<li><p>The response is handled by the callback of that custom
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, instead of being handled by the
callback of the original <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object</p></li>
<li><p>That custom <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object is now sent as the
<code class="docutils literal notranslate"><span class="pre">request</span></code> argument to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-response_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">response_received</span></code></a> signal, instead
of the original <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4529">issue 4529</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4632">issue 4632</a>)</p>
</li>
<li><p>When using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-ftp"><span class="std std-ref">FTP feed storage backend</span></a>:</p>
<ul class="simple">
<li><p>It is now possible to set the new <code class="docutils literal notranslate"><span class="pre">overwrite</span></code> <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#feed-options"><span class="std std-ref">feed option</span></a> to <code class="docutils literal notranslate"><span class="pre">False</span></code> to append to an existing file instead of
overwriting it</p></li>
<li><p>The FTP password can now be omitted if it is not necessary</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/716">issue 716</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4512">issue 4512</a>)</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of <a class="reference internal" href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a> now
supports an <code class="docutils literal notranslate"><span class="pre">errors</span></code> parameter to indicate how to handle encoding errors
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4755">issue 4755</a>)</p></li>
<li><p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-asyncio"><span class="std std-ref">using asyncio</span></a>, it is now possible to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#using-custom-loops"><span class="std std-ref">set a custom asyncio loop</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4306">issue 4306</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4414">issue 4414</a>)</p></li>
<li><p>Serialized requests (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">Jobs: pausing and resuming crawls</span></a>) now support callbacks that are
spider methods that delegate on other callable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4756">issue 4756</a>)</p></li>
<li><p>When a response is larger than <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a>, the logged
message is now a warning, instead of an error (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3874">issue 3874</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3886">issue 3886</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4752">issue 4752</a>)</p></li>
</ul>
</section>
<section id="id67">
<h5>Bug fixes<a class="headerlink" href="#id67" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command no longer overwrites existing files
unless the <code class="docutils literal notranslate"><span class="pre">--force</span></code> option is used (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4561">issue 4561</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4616">issue 4616</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4623">issue 4623</a>)</p></li>
<li><p>Cookies with an empty value are no longer considered invalid cookies
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4772">issue 4772</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command now supports files with the <code class="docutils literal notranslate"><span class="pre">.pyw</span></code> file
extension (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4643">issue 4643</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4646">issue 4646</a>)</p></li>
<li><p>The <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>
middleware now simply ignores unsupported proxy values (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3331">issue 3331</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4778">issue 4778</a>)</p></li>
<li><p>Checks for generator callbacks with a <code class="docutils literal notranslate"><span class="pre">return</span></code> statement no longer warn
about <code class="docutils literal notranslate"><span class="pre">return</span></code> statements in nested functions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4720">issue 4720</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4721">issue 4721</a>)</p></li>
<li><p>The system file mode creation mask no longer affects the permissions of
files generated using the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4722">issue 4722</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.iterators.xmliter()</span></code> now supports namespaced node names
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/861">issue 861</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4746">issue 4746</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code> objects can now have <code class="docutils literal notranslate"><span class="pre">about:</span></code> URLs, which can
work when using a headless browser (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4835">issue 4835</a>)</p></li>
</ul>
</section>
<section id="id68">
<h5>Documentation<a class="headerlink" href="#id68" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_URI_PARAMS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI_PARAMS</span></code></a> setting is now documented (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4671">issue 4671</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4724">issue 4724</a>)</p></li>
<li><p>Improved the documentation of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">link extractors</span></a> with an usage example from
a spider callback and reference documentation for the
<a class="reference internal" href="index.html#scrapy.link.Link" title="scrapy.link.Link"><code class="xref py py-class docutils literal notranslate"><span class="pre">Link</span></code></a> class (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4751">issue 4751</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4775">issue 4775</a>)</p></li>
<li><p>Clarified the impact of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a> when using the
<a class="reference internal" href="index.html#scrapy.extensions.closespider.CloseSpider" title="scrapy.extensions.closespider.CloseSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CloseSpider</span></code></a> extension
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4836">issue 4836</a>)</p></li>
<li><p>Removed references to Python 2’s <code class="docutils literal notranslate"><span class="pre">unicode</span></code> type (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4547">issue 4547</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4703">issue 4703</a>)</p></li>
<li><p>We now have an <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#deprecation-policy"><span class="std std-ref">official deprecation policy</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4705">issue 4705</a>)</p></li>
<li><p>Our <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#documentation-policies"><span class="std std-ref">documentation policies</span></a> now cover usage
of Sphinx’s <a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v8.2.0)"><code class="xref rst rst-dir docutils literal notranslate"><span class="pre">versionadded</span></code></a> and <a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v8.2.0)"><code class="xref rst rst-dir docutils literal notranslate"><span class="pre">versionchanged</span></code></a>
directives, and we have removed usages referencing Scrapy 1.4.0 and earlier
versions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3971">issue 3971</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4310">issue 4310</a>)</p></li>
<li><p>Other documentation cleanups (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4090">issue 4090</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4782">issue 4782</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4800">issue 4800</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4801">issue 4801</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4809">issue 4809</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4816">issue 4816</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4825">issue 4825</a>)</p></li>
</ul>
</section>
<section id="id69">
<h5>Quality assurance<a class="headerlink" href="#id69" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Extended typing hints (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4243">issue 4243</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4691">issue 4691</a>)</p></li>
<li><p>Added tests for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4663">issue 4663</a>)</p></li>
<li><p>Fixed test failures on Debian (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4726">issue 4726</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4727">issue 4727</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4735">issue 4735</a>)</p></li>
<li><p>Improved Windows test coverage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4723">issue 4723</a>)</p></li>
<li><p>Switched to <a class="reference external" href="https://docs.python.org/3/reference/lexical_analysis.html#f-strings" title="(in Python v3.13)"><span class="xref std std-ref">formatted string literals</span></a> where possible
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4307">issue 4307</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4324">issue 4324</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4672">issue 4672</a>)</p></li>
<li><p>Modernized <code class="xref py py-func docutils literal notranslate"><span class="pre">super()</span></code> usage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4707">issue 4707</a>)</p></li>
<li><p>Other code and test cleanups (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1790">issue 1790</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3288">issue 3288</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4165">issue 4165</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4564">issue 4564</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4651">issue 4651</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4714">issue 4714</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4738">issue 4738</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4745">issue 4745</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4747">issue 4747</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4761">issue 4761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4765">issue 4765</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4804">issue 4804</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4817">issue 4817</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4820">issue 4820</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4822">issue 4822</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4839">issue 4839</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-3-0-2020-08-04">
<span id="release-2-3-0"></span><h4>Scrapy 2.3.0 (2020-08-04)<a class="headerlink" href="#scrapy-2-3-0-2020-08-04" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a> now support <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-gcs"><span class="std std-ref">Google Cloud
Storage</span></a> as a storage backend</p></li>
<li><p>The new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> setting allows to deliver
output items in batches of up to the specified number of items.</p>
<p>It also serves as a workaround for <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#delayed-file-delivery"><span class="std std-ref">delayed file delivery</span></a>, which causes Scrapy to only start item delivery
after the crawl has finished when using certain storage backends
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3</span></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-ftp"><span class="std std-ref">FTP</span></a>,
and now <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-gcs"><span class="std std-ref">GCS</span></a>).</p>
</li>
<li><p>The base implementation of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-loaders"><span class="std std-ref">item loaders</span></a> has been
moved into a separate library, <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)"><span class="xref std std-doc">itemloaders</span></a>,
allowing usage from outside Scrapy and a separate release schedule</p></li>
</ul>
<section id="id70">
<h5>Deprecation removals<a class="headerlink" href="#id70" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Removed the following classes and their parent modules from
<code class="docutils literal notranslate"><span class="pre">scrapy.linkextractors</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">htmlparser.HtmlParserLinkExtractor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">regex.RegexLinkExtractor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sgml.BaseSgmlLinkExtractor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sgml.SgmlLinkExtractor</span></code></p></li>
</ul>
<p>Use
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4356">issue 4356</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4679">issue 4679</a>)</p>
</li>
</ul>
</section>
<section id="id71">
<h5>Deprecations<a class="headerlink" href="#id71" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python.retry_on_eintr</span></code> function is now deprecated
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4683">issue 4683</a>)</p></li>
</ul>
</section>
<section id="id72">
<h5>New features<a class="headerlink" href="#id72" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a> support <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-storage-gcs"><span class="std std-ref">Google Cloud
Storage</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/685">issue 685</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3608">issue 3608</a>)</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> setting for batch deliveries
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4250">issue 4250</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4434">issue 4434</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command now allows specifying an output file
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4317">issue 4317</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4377">issue 4377</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.from_curl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs"><code class="xref py py-func docutils literal notranslate"><span class="pre">curl_to_request_kwargs()</span></code></a> now also support
<code class="docutils literal notranslate"><span class="pre">--data-raw</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4612">issue 4612</a>)</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback may now be used in built-in spider subclasses, such
as <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/712">issue 712</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/732">issue 732</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/781">issue 781</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4254">issue 4254</a> )</p></li>
</ul>
</section>
<section id="id73">
<h5>Bug fixes<a class="headerlink" href="#id73" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fixed the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-feed-format-csv"><span class="std std-ref">CSV exporting</span></a> of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#dataclass-items"><span class="std std-ref">dataclass items</span></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#attrs-items"><span class="std std-ref">attr.s items</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4667">issue 4667</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4668">issue 4668</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.from_curl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs"><code class="xref py py-func docutils literal notranslate"><span class="pre">curl_to_request_kwargs()</span></code></a> now set the request
method to <code class="docutils literal notranslate"><span class="pre">POST</span></code> when a request body is specified and no request method
is specified (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4612">issue 4612</a>)</p></li>
<li><p>The processing of ANSI escape sequences in enabled in Windows 10.0.14393
and later, where it is required for colored output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4393">issue 4393</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4403">issue 4403</a>)</p></li>
</ul>
</section>
<section id="id74">
<h5>Documentation<a class="headerlink" href="#id74" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Updated the <a class="reference external" href="https://docs.openssl.org/master/man1/openssl-ciphers/#cipher-list-format">OpenSSL cipher list format</a> link in the documentation about
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4653">issue 4653</a>)</p></li>
<li><p>Simplified the code example in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-loaders-dataclass"><span class="std std-ref">Working with dataclass items</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4652">issue 4652</a>)</p></li>
</ul>
</section>
<section id="id75">
<h5>Quality assurance<a class="headerlink" href="#id75" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The base implementation of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-loaders"><span class="std std-ref">item loaders</span></a> has been
moved into <a class="reference external" href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)"><span class="xref std std-doc">itemloaders</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4005">issue 4005</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4516">issue 4516</a>)</p></li>
<li><p>Fixed a silenced error in some scheduler tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4644">issue 4644</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4645">issue 4645</a>)</p></li>
<li><p>Renewed the localhost certificate used for SSL tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4650">issue 4650</a>)</p></li>
<li><p>Removed cookie-handling code specific to Python 2 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4682">issue 4682</a>)</p></li>
<li><p>Stopped using Python 2 unicode literal syntax (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4704">issue 4704</a>)</p></li>
<li><p>Stopped using a backlash for line continuation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4673">issue 4673</a>)</p></li>
<li><p>Removed unneeded entries from the MyPy exception list (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4690">issue 4690</a>)</p></li>
<li><p>Automated tests now pass on Windows as part of our continuous integration
system (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4458">issue 4458</a>)</p></li>
<li><p>Automated tests now pass on the latest PyPy version for supported Python
versions in our continuous integration system (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4504">issue 4504</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-2-1-2020-07-17">
<span id="release-2-2-1"></span><h4>Scrapy 2.2.1 (2020-07-17)<a class="headerlink" href="#scrapy-2-2-1-2020-07-17" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command no longer makes unintended changes to
the permissions of files in the destination folder, such as removing
execution permissions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4662">issue 4662</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4666">issue 4666</a>)</p></li>
</ul>
</section>
<section id="scrapy-2-2-0-2020-06-24">
<span id="release-2-2-0"></span><h4>Scrapy 2.2.0 (2020-06-24)<a class="headerlink" href="#scrapy-2-2-0-2020-06-24" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Python 3.5.2+ is required now</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#dataclass-items"><span class="std std-ref">dataclass objects</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#attrs-items"><span class="std std-ref">attrs objects</span></a> are now valid <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item types</span></a></p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.json</span></code></a> method</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-bytes_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">bytes_received</span></code></a> signal that allows canceling response download</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code></a> fixes</p></li>
</ul>
<section id="id76">
<h5>Backward-incompatible changes<a class="headerlink" href="#id76" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now refuses to
run with a Python version lower than 3.5.2, which introduced
<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">typing.Type</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4615">issue 4615</a>)</p></li>
</ul>
</section>
<section id="id77">
<h5>Deprecations<a class="headerlink" href="#id77" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.body_as_unicode</span></code> is now deprecated, use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.text</span></code></a> instead
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4546">issue 4546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4555">issue 4555</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4579">issue 4579</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.item.BaseItem</span></code> is now deprecated, use
<a class="reference internal" href="index.html#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.item.Item</span></code></a> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4534">issue 4534</a>)</p></li>
</ul>
</section>
<section id="id78">
<h5>New features<a class="headerlink" href="#id78" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#dataclass-items"><span class="std std-ref">dataclass objects</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#attrs-items"><span class="std std-ref">attrs objects</span></a> are now valid <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#item-types"><span class="std std-ref">item types</span></a>, and a new <a class="reference external" href="https://github.com/scrapy/itemadapter">itemadapter</a> library makes it easy to
write code that <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#supporting-item-types"><span class="std std-ref">supports any item type</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2749">issue 2749</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2807">issue 2807</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3761">issue 3761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3881">issue 3881</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4642">issue 4642</a>)</p></li>
<li><p>A new <a class="reference internal" href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json"><code class="xref py py-meth docutils literal notranslate"><span class="pre">TextResponse.json</span></code></a> method
allows to deserialize JSON responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2444">issue 2444</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4460">issue 4460</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4574">issue 4574</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-bytes_received"><code class="xref std std-signal docutils literal notranslate"><span class="pre">bytes_received</span></code></a> signal allows monitoring response download
progress and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-stop-response-download"><span class="std std-ref">stopping downloads</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4205">issue 4205</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4559">issue 4559</a>)</p></li>
<li><p>The dictionaries in the result list of a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipeline</span></a> now include a new key, <code class="docutils literal notranslate"><span class="pre">status</span></code>, which indicates
if the file was downloaded or, if the file was not downloaded, why it was
not downloaded; see <a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.get_media_requests</span></code></a> for more
information (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2893">issue 2893</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4486">issue 4486</a>)</p></li>
<li><p>When using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#media-pipeline-gcs"><span class="std std-ref">Google Cloud Storage</span></a> for
a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipeline</span></a>, a warning is now logged if
the configured credentials do not grant the required permissions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4346">issue 4346</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4508">issue 4508</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link extractors</span></a> are now serializable,
as long as you do not use <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#lambda" title="(in Python v3.13)"><span class="xref std std-ref">lambdas</span></a> for parameters; for
example, you can now pass link extractors in <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> or
<a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> when <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-jobs"><span class="std std-ref">persisting
scheduled requests</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4554">issue 4554</a>)</p></li>
<li><p>Upgraded the <a class="reference external" href="https://docs.python.org/3/library/pickle.html#pickle-protocols" title="(in Python v3.13)"><span class="xref std std-ref">pickle protocol</span></a> that Scrapy uses
from protocol 2 to protocol 4, improving serialization capabilities and
performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4135">issue 4135</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4541">issue 4541</a>)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.misc.create_instance()</span></code> now raises a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a>
exception if the resulting instance is <code class="docutils literal notranslate"><span class="pre">None</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4528">issue 4528</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4532">issue 4532</a>)</p></li>
</ul>
</section>
<section id="id79">
<h5>Bug fixes<a class="headerlink" href="#id79" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code></a> no longer
discards cookies defined in <a class="reference internal" href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.headers</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1992">issue 1992</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2400">issue 2400</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">CookiesMiddleware</span></code></a> no longer
re-encodes cookies defined as <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bytes</span></code></a> in the <code class="docutils literal notranslate"><span class="pre">cookies</span></code> parameter
of the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2400">issue 2400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3575">issue 3575</a>)</p></li>
<li><p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> defines multiple URIs, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY</span></code></a> is
<code class="docutils literal notranslate"><span class="pre">False</span></code> and the crawl yields no items, Scrapy no longer stops feed
exports after the first URI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4621">issue 4621</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4626">issue 4626</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> callbacks defined using <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">coroutine
syntax</span></a> no longer need to return an iterable, and may
instead return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, an
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-items"><span class="std std-ref">item</span></a>, or <code class="docutils literal notranslate"><span class="pre">None</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4609">issue 4609</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command now ensures that the generated project
folders and files have the right permissions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4604">issue 4604</a>)</p></li>
<li><p>Fix a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">KeyError</span></code></a> exception being sometimes raised from
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.LocalWeakReferencedCache</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4597">issue 4597</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4599">issue 4599</a>)</p></li>
<li><p>When <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> defines multiple URIs, log messages about items being
stored now contain information from the corresponding feed, instead of
always containing information about only one of the feeds (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4619">issue 4619</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4629">issue 4629</a>)</p></li>
</ul>
</section>
<section id="id80">
<h5>Documentation<a class="headerlink" href="#id80" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added a new section about <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#errback-cb-kwargs"><span class="std std-ref">accessing cb_kwargs from errbacks</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4598">issue 4598</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4634">issue 4634</a>)</p></li>
<li><p>Covered <a class="reference external" href="https://github.com/Nykakin/chompjs">chompjs</a> in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-parsing-javascript"><span class="std std-ref">Parsing JavaScript code</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4556">issue 4556</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4562">issue 4562</a>)</p></li>
<li><p>Removed from <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">Coroutines</span></a> the warning about the API being
experimental (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4511">issue 4511</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4513">issue 4513</a>)</p></li>
<li><p>Removed references to unsupported versions of <a class="reference external" href="https://docs.twisted.org/en/stable/index.html" title="(in Twisted v24.10)"><span class="xref std std-doc">Twisted</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4533">issue 4533</a>)</p></li>
<li><p>Updated the description of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#screenshotpipeline"><span class="std std-ref">screenshot pipeline example</span></a>, which now uses <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">coroutine syntax</span></a> instead of returning a
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4514">issue 4514</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4593">issue 4593</a>)</p></li>
<li><p>Removed a misleading import line from the
<a class="reference internal" href="index.html#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.log.configure_logging()</span></code></a> code example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4510">issue 4510</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4587">issue 4587</a>)</p></li>
<li><p>The display-on-hover behavior of internal documentation references now also
covers links to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-commands"><span class="std std-ref">commands</span></a>, <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> keys, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-signals"><span class="std std-ref">signals</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4495">issue 4495</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4563">issue 4563</a>)</p></li>
<li><p>It is again possible to download the documentation for offline reading
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4578">issue 4578</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4585">issue 4585</a>)</p></li>
<li><p>Removed backslashes preceding <code class="docutils literal notranslate"><span class="pre">*args</span></code> and <code class="docutils literal notranslate"><span class="pre">**kwargs</span></code> in some function
and method signatures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4592">issue 4592</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4596">issue 4596</a>)</p></li>
</ul>
</section>
<section id="id81">
<h5>Quality assurance<a class="headerlink" href="#id81" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Adjusted the code base further to our <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#coding-style"><span class="std std-ref">style guidelines</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4237">issue 4237</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4525">issue 4525</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4538">issue 4538</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4539">issue 4539</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4540">issue 4540</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4542">issue 4542</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4543">issue 4543</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4544">issue 4544</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4545">issue 4545</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4557">issue 4557</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4558">issue 4558</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4566">issue 4566</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4568">issue 4568</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4572">issue 4572</a>)</p></li>
<li><p>Removed remnants of Python 2 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4550">issue 4550</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4553">issue 4553</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4568">issue 4568</a>)</p></li>
<li><p>Improved code sharing between the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a>
commands (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4548">issue 4548</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4552">issue 4552</a>)</p></li>
<li><p>Replaced <code class="docutils literal notranslate"><span class="pre">chain(*iterable)</span></code> with <code class="docutils literal notranslate"><span class="pre">chain.from_iterable(iterable)</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4635">issue 4635</a>)</p></li>
<li><p>You may now run the <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> tests with Tox on any Python version
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4521">issue 4521</a>)</p></li>
<li><p>Updated test requirements to reflect an incompatibility with pytest 5.4 and
5.4.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4588">issue 4588</a>)</p></li>
<li><p>Improved <a class="reference internal" href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpiderLoader</span></code></a> test coverage for
scenarios involving duplicate spider names (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4549">issue 4549</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4560">issue 4560</a>)</p></li>
<li><p>Configured Travis CI to also run the tests with Python 3.5.2
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4518">issue 4518</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4615">issue 4615</a>)</p></li>
<li><p>Added a <a class="reference external" href="https://www.pylint.org/">Pylint</a> job to Travis CI
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3727">issue 3727</a>)</p></li>
<li><p>Added a <a class="reference external" href="https://mypy-lang.org/">Mypy</a> job to Travis CI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4637">issue 4637</a>)</p></li>
<li><p>Made use of set literals in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4573">issue 4573</a>)</p></li>
<li><p>Cleaned up the Travis CI configuration (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4517">issue 4517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4519">issue 4519</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4522">issue 4522</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4537">issue 4537</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-1-0-2020-04-24">
<span id="release-2-1-0"></span><h4>Scrapy 2.1.0 (2020-04-24)<a class="headerlink" href="#scrapy-2-1-0-2020-04-24" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting to export to multiple feeds</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.ip_address</span></code></a> attribute</p></li>
</ul>
<section id="id82">
<h5>Backward-incompatible changes<a class="headerlink" href="#id82" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AssertionError</span></code></a> exceptions triggered by <a class="reference external" href="https://docs.python.org/3/reference/simple_stmts.html#assert" title="(in Python v3.13)"><span class="xref std std-ref">assert</span></a>
statements have been replaced by new exception types, to support running
Python in optimized mode (see <a class="reference external" href="https://docs.python.org/3/using/cmdline.html#cmdoption-O" title="(in Python v3.13)"><code class="xref std std-option docutils literal notranslate"><span class="pre">-O</span></code></a>) without changing Scrapy’s
behavior in any unexpected ways.</p>
<p>If you catch an <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AssertionError</span></code></a> exception from Scrapy, update your
code to catch the corresponding new exception.</p>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4440">issue 4440</a>)</p>
</li>
</ul>
</section>
<section id="id83">
<h5>Deprecation removals<a class="headerlink" href="#id83" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">LOG_UNSERIALIZABLE_REQUESTS</span></code> setting is no longer supported, use
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4385">issue 4385</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">REDIRECT_MAX_METAREFRESH_DELAY</span></code> setting is no longer supported, use
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-METAREFRESH_MAXDELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_MAXDELAY</span></code></a> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4385">issue 4385</a>)</p></li>
<li><p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">ChunkedTransferMiddleware</span></code>
middleware has been removed, including the entire
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.chunked</span></code> module; chunked transfers
work out of the box (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4431">issue 4431</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">spiders</span></code> property has been removed from
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>, use <code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.spider_loader</span></code> or instantiate
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> with your settings instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4398">issue 4398</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">MultiValueDict</span></code>, <code class="docutils literal notranslate"><span class="pre">MultiValueDictKeyError</span></code>, and <code class="docutils literal notranslate"><span class="pre">SiteNode</span></code>
classes have been removed from <code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.datatypes</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4400">issue 4400</a>)</p></li>
</ul>
</section>
<section id="id84">
<h5>Deprecations<a class="headerlink" href="#id84" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code> and <code class="docutils literal notranslate"><span class="pre">FEED_URI</span></code> settings have been deprecated in
favor of the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1336">issue 1336</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3858">issue 3858</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4507">issue 4507</a>)</p></li>
</ul>
</section>
<section id="id85">
<h5>New features<a class="headerlink" href="#id85" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>A new setting, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEEDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEEDS</span></code></a>, allows configuring multiple output feeds
with different settings each (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1336">issue 1336</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3858">issue 3858</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4507">issue 4507</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> commands now support multiple
<code class="docutils literal notranslate"><span class="pre">-o</span></code> parameters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1336">issue 1336</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3858">issue 3858</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4507">issue 4507</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> commands now support
specifying an output format by appending <code class="docutils literal notranslate"><span class="pre">:&lt;format&gt;</span></code> to the output file
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1336">issue 1336</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3858">issue 3858</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4507">issue 4507</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.ip_address</span></code></a>
attribute gives access to the IP address that originated a response
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3903">issue 3903</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3940">issue 3940</a>)</p></li>
<li><p>A warning is now issued when a value in
<code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code> includes a port
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/50">issue 50</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3198">issue 3198</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4413">issue 4413</a>)</p></li>
<li><p>Zsh completion now excludes used option aliases from the completion list
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4438">issue 4438</a>)</p></li>
</ul>
</section>
<section id="id86">
<h5>Bug fixes<a class="headerlink" href="#id86" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#request-serialization"><span class="std std-ref">Request serialization</span></a> no longer breaks for
callbacks that are spider attributes which are assigned a function with a
different name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4500">issue 4500</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> values in <code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code> no longer
cause a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a> exception (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4410">issue 4410</a>)</p></li>
<li><p>Zsh completion no longer allows options after arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4438">issue 4438</a>)</p></li>
<li><p>zope.interface 5.0.0 and later versions are now supported
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4447">issue 4447</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4448">issue 4448</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Spider.make_requests_from_url</span></code>, deprecated in Scrapy 1.4.0, now issues a
warning when used (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4412">issue 4412</a>)</p></li>
</ul>
</section>
<section id="id87">
<h5>Documentation<a class="headerlink" href="#id87" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Improved the documentation about signals that allow their handlers to
return a <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Deferred</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4295">issue 4295</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4390">issue 4390</a>)</p></li>
<li><p>Our PyPI entry now includes links for our documentation, our source code
repository and our issue tracker (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4456">issue 4456</a>)</p></li>
<li><p>Covered the <a class="reference external" href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>
service in the documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4206">issue 4206</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4455">issue 4455</a>)</p></li>
<li><p>Removed references to the Guppy library, which only works in Python 2
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4285">issue 4285</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4343">issue 4343</a>)</p></li>
<li><p>Extended use of InterSphinx to link to Python 3 documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4444">issue 4444</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4445">issue 4445</a>)</p></li>
<li><p>Added support for Sphinx 3.0 and later (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4475">issue 4475</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4480">issue 4480</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4496">issue 4496</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4503">issue 4503</a>)</p></li>
</ul>
</section>
<section id="id88">
<h5>Quality assurance<a class="headerlink" href="#id88" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Removed warnings about using old, removed settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4404">issue 4404</a>)</p></li>
<li><p>Removed a warning about importing
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">StringTransport</span></code></a> from
<code class="docutils literal notranslate"><span class="pre">twisted.test.proto_helpers</span></code> in Twisted 19.7.0 or newer (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4409">issue 4409</a>)</p></li>
<li><p>Removed outdated Debian package build files (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4384">issue 4384</a>)</p></li>
<li><p>Removed <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a> usage as a base class (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4430">issue 4430</a>)</p></li>
<li><p>Removed code that added support for old versions of Twisted that we no
longer support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4472">issue 4472</a>)</p></li>
<li><p>Fixed code style issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4468">issue 4468</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4469">issue 4469</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4471">issue 4471</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4481">issue 4481</a>)</p></li>
<li><p>Removed <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue" title="(in Twisted)"><code class="xref py py-func docutils literal notranslate"><span class="pre">twisted.internet.defer.returnValue()</span></code></a> calls (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4443">issue 4443</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4446">issue 4446</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4489">issue 4489</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-2-0-1-2020-03-18">
<span id="release-2-0-1"></span><h4>Scrapy 2.0.1 (2020-03-18)<a class="headerlink" href="#scrapy-2-0-1-2020-03-18" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.follow_all</span></code></a> now supports
an empty URL iterable as input (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4408">issue 4408</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4420">issue 4420</a>)</p></li>
<li><p>Removed top-level <a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> imports to prevent
errors about the wrong Twisted reactor being installed when setting a
different Twisted reactor using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4401">issue 4401</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4406">issue 4406</a>)</p></li>
<li><p>Fixed tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4422">issue 4422</a>)</p></li>
</ul>
</section>
<section id="scrapy-2-0-0-2020-03-03">
<span id="release-2-0-0"></span><h4>Scrapy 2.0.0 (2020-03-03)<a class="headerlink" href="#scrapy-2-0-0-2020-03-03" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Python 2 support has been removed</p></li>
<li><p><a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">Partial</span></a> <a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.13)"><span class="xref std std-ref">coroutine syntax</span></a> support
and <a class="reference internal" href="index.html#document-topics/asyncio"><span class="doc">experimental</span></a> <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> support</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.follow_all</span></code></a> method</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#media-pipeline-ftp"><span class="std std-ref">FTP support</span></a> for media pipelines</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.certificate</span></code></a>
attribute</p></li>
<li><p>IPv6 support through <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_RESOLVER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_RESOLVER</span></code></a></p></li>
</ul>
<section id="id89">
<h5>Backward-incompatible changes<a class="headerlink" href="#id89" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Python 2 support has been removed, following <a class="reference external" href="https://www.python.org/doc/sunset-python-2/">Python 2 end-of-life on
January 1, 2020</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4091">issue 4091</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4114">issue 4114</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4115">issue 4115</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4121">issue 4121</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4138">issue 4138</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4231">issue 4231</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4242">issue 4242</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4304">issue 4304</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4309">issue 4309</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4373">issue 4373</a>)</p></li>
<li><p>Retry gaveups (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a>) are now logged as errors instead
of as debug information (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3171">issue 3171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3566">issue 3566</a>)</p></li>
<li><p>File extensions that
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
ignores by default now also include <code class="docutils literal notranslate"><span class="pre">7z</span></code>, <code class="docutils literal notranslate"><span class="pre">7zip</span></code>, <code class="docutils literal notranslate"><span class="pre">apk</span></code>, <code class="docutils literal notranslate"><span class="pre">bz2</span></code>,
<code class="docutils literal notranslate"><span class="pre">cdr</span></code>, <code class="docutils literal notranslate"><span class="pre">dmg</span></code>, <code class="docutils literal notranslate"><span class="pre">ico</span></code>, <code class="docutils literal notranslate"><span class="pre">iso</span></code>, <code class="docutils literal notranslate"><span class="pre">tar</span></code>, <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code>, <code class="docutils literal notranslate"><span class="pre">webm</span></code>, and
<code class="docutils literal notranslate"><span class="pre">xz</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1837">issue 1837</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2067">issue 2067</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4066">issue 4066</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> setting is now an empty list by
default, following web browser behavior (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3844">issue 3844</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4311">issue 4311</a>)</p></li>
<li><p>The
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a>
now includes spaces after commas in the value of the <code class="docutils literal notranslate"><span class="pre">Accept-Encoding</span></code>
header that it sets, following web browser behavior (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4293">issue 4293</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of custom download handlers (see
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>) or subclasses of the following downloader
handlers  no longer receives a <code class="docutils literal notranslate"><span class="pre">settings</span></code> parameter:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.downloader.handlers.file.FileDownloadHandler</span></code></p></li>
</ul>
<p>Use the <code class="docutils literal notranslate"><span class="pre">from_settings</span></code> or <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class methods to expose such
a parameter to your custom download handlers.</p>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4126">issue 4126</a>)</p>
</li>
<li><p>We have refactored the <a class="reference internal" href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.scheduler.Scheduler</span></code></a> class and
related queue classes (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DISK_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DISK_QUEUE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_MEMORY_QUEUE</span></code></a>) to
make it easier to implement custom scheduler queue classes. See
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#scheduler-queue-changes"><span class="std std-ref">Changes to scheduler queue classes</span></a> below for details.</p></li>
<li><p>Overridden settings are now logged in a different format. This is more in
line with similar information logged at startup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4199">issue 4199</a>)</p></li>
</ul>
</section>
<section id="id90">
<h5>Deprecation removals<a class="headerlink" href="#id90" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> no longer provides a <cite>sel</cite> proxy
object, use <code class="xref py py-meth docutils literal notranslate"><span class="pre">response.selector</span></code>
instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4347">issue 4347</a>)</p></li>
<li><p>LevelDB support has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4112">issue 4112</a>)</p></li>
<li><p>The following functions have been removed from <code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.python</span></code>:
<code class="docutils literal notranslate"><span class="pre">isbinarytext</span></code>, <code class="docutils literal notranslate"><span class="pre">is_writable</span></code>, <code class="docutils literal notranslate"><span class="pre">setattr_default</span></code>, <code class="docutils literal notranslate"><span class="pre">stringify_dict</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4362">issue 4362</a>)</p></li>
</ul>
</section>
<section id="id91">
<h5>Deprecations<a class="headerlink" href="#id91" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Using environment variables prefixed with <code class="docutils literal notranslate"><span class="pre">SCRAPY_</span></code> to override settings
is deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4300">issue 4300</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4374">issue 4374</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4375">issue 4375</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.linkextractors.FilteringLinkExtractor</span></code> is deprecated, use
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code></a> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4045">issue 4045</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">noconnect</span></code> query string argument of proxy URLs is deprecated and
should be removed from proxy URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4198">issue 4198</a>)</p></li>
<li><p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">next</span></code> method of
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.python.MutableChain</span></code> is deprecated, use the global
<a class="reference external" href="https://docs.python.org/3/library/functions.html#next" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">next()</span></code></a> function or <code class="xref py py-meth docutils literal notranslate"><span class="pre">MutableChain.__next__</span></code> instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4153">issue 4153</a>)</p></li>
</ul>
</section>
<section id="id92">
<h5>New features<a class="headerlink" href="#id92" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Added <a class="reference internal" href="index.html#document-topics/coroutines"><span class="doc">partial support</span></a> for Python’s
<a class="reference external" href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.13)"><span class="xref std std-ref">coroutine syntax</span></a> and <a class="reference internal" href="index.html#document-topics/asyncio"><span class="doc">experimental support</span></a> for <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a> and <a class="reference external" href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.13)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">asyncio</span></code></a>-powered libraries
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4010">issue 4010</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4259">issue 4259</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4269">issue 4269</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4270">issue 4270</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4271">issue 4271</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4316">issue 4316</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4318">issue 4318</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.follow_all</span></code></a>
method offers the same functionality as
<a class="reference internal" href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.follow</span></code></a> but supports an
iterable of URLs as input and returns an iterable of requests
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2582">issue 2582</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4057">issue 4057</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4286">issue 4286</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">Media pipelines</span></a> now support <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#media-pipeline-ftp"><span class="std std-ref">FTP
storage</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3928">issue 3928</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3961">issue 3961</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.certificate</span></code></a>
attribute exposes the SSL certificate of the server as a
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html" title="(in Twisted)"><code class="xref py py-class docutils literal notranslate"><span class="pre">twisted.internet.ssl.Certificate</span></code></a> object for HTTPS responses
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2726">issue 2726</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4054">issue 4054</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DNS_RESOLVER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNS_RESOLVER</span></code></a> setting allows enabling IPv6 support
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1031">issue 1031</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4227">issue 4227</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCRAPER_SLOT_MAX_ACTIVE_SIZE</span></code></a> setting allows configuring
the existing soft limit that pauses request downloads when the total
response data being processed is too high (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1410">issue 1410</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3551">issue 3551</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TWISTED_REACTOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TWISTED_REACTOR</span></code></a> setting allows customizing the
<a class="reference external" href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">reactor</span></code></a> that Scrapy uses, allowing to
<a class="reference internal" href="index.html#document-topics/asyncio"><span class="doc">enable asyncio support</span></a> or deal with a
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-specific-reactor"><span class="std std-ref">common macOS issue</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2905">issue 2905</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4294">issue 4294</a>)</p></li>
<li><p>Scheduler disk and memory queues may now use the class methods
<code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> or <code class="docutils literal notranslate"><span class="pre">from_settings</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3884">issue 3884</a>)</p></li>
<li><p>The new <a class="reference internal" href="index.html#scrapy.http.Response.cb_kwargs" title="scrapy.http.Response.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.cb_kwargs</span></code></a>
attribute serves as a shortcut for <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request.cb_kwargs</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4331">issue 4331</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Response.follow</span></code></a> now supports a
<code class="docutils literal notranslate"><span class="pre">flags</span></code> parameter, for consistency with <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4277">issue 4277</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4279">issue 4279</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-loaders-processors"><span class="std std-ref">Item loader processors</span></a> can now be
regular functions, they no longer need to be methods (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3899">issue 3899</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> now accepts an <code class="docutils literal notranslate"><span class="pre">errback</span></code> parameter
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4000">issue 4000</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> no longer requires a <code class="docutils literal notranslate"><span class="pre">callback</span></code> parameter
when an <code class="docutils literal notranslate"><span class="pre">errback</span></code> parameter is specified (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3586">issue 3586</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4008">issue 4008</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogFormatter</span></code></a> now supports some additional
methods:</p>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.logformatter.LogFormatter.download_error" title="scrapy.logformatter.LogFormatter.download_error"><code class="xref py py-class docutils literal notranslate"><span class="pre">download_error</span></code></a> for
download errors</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.logformatter.LogFormatter.item_error" title="scrapy.logformatter.LogFormatter.item_error"><code class="xref py py-class docutils literal notranslate"><span class="pre">item_error</span></code></a> for exceptions
raised during item processing by <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipelines</span></a></p></li>
<li><p><a class="reference internal" href="index.html#scrapy.logformatter.LogFormatter.spider_error" title="scrapy.logformatter.LogFormatter.spider_error"><code class="xref py py-class docutils literal notranslate"><span class="pre">spider_error</span></code></a> for exceptions
raised from <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-spiders"><span class="std std-ref">spider callbacks</span></a></p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/374">issue 374</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3986">issue 3986</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3989">issue 3989</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4176">issue 4176</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4188">issue 4188</a>)</p>
</li>
<li><p>The <code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI</span></code> setting now supports <a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">pathlib.Path</span></code></a> values
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3731">issue 3731</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4074">issue 4074</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-request_left_downloader"><code class="xref std std-signal docutils literal notranslate"><span class="pre">request_left_downloader</span></code></a> signal is sent when a request
leaves the downloader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4303">issue 4303</a>)</p></li>
<li><p>Scrapy logs a warning when it detects a request callback or errback that
uses <code class="docutils literal notranslate"><span class="pre">yield</span></code> but also returns a value, since the returned value would be
lost (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3484">issue 3484</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3869">issue 3869</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> objects now raise an <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AttributeError</span></code></a>
exception if they do not have a <code class="xref py py-class docutils literal notranslate"><span class="pre">start_urls</span></code>
attribute nor reimplement <code class="xref py py-class docutils literal notranslate"><span class="pre">start_requests</span></code>,
but have a <code class="docutils literal notranslate"><span class="pre">start_url</span></code> attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4133">issue 4133</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4170">issue 4170</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> subclasses may now use
<code class="docutils literal notranslate"><span class="pre">super().__init__(**kwargs)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">self._configure(kwargs)</span></code> in
their <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, passing <code class="docutils literal notranslate"><span class="pre">dont_fail=True</span></code> to the parent
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method if needed, and accessing <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> at <code class="docutils literal notranslate"><span class="pre">self._kwargs</span></code>
after calling their parent <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4193">issue 4193</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4370">issue 4370</a>)</p></li>
<li><p>A new <code class="docutils literal notranslate"><span class="pre">keep_fragments</span></code> parameter of
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_fingerprint</span></code> allows to generate
different fingerprints for requests with different fragments in their URL
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4104">issue 4104</a>)</p></li>
<li><p>Download handlers (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>) may now use the
<code class="docutils literal notranslate"><span class="pre">from_settings</span></code> and <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class methods that other Scrapy
components already supported (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4126">issue 4126</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.python.MutableChain.__iter__</span></code> now returns <code class="docutils literal notranslate"><span class="pre">self</span></code>,
<a class="reference external" href="https://lgtm.com/rules/4850080/">allowing it to be used as a sequence</a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4153">issue 4153</a>)</p></li>
</ul>
</section>
<section id="id93">
<h5>Bug fixes<a class="headerlink" href="#id93" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> command now also exits with exit code 1 when an
exception happens before the crawling starts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4175">issue 4175</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4207">issue 4207</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor.extract_links</span></code></a> no longer
re-encodes the query string or URLs from non-UTF-8 responses in UTF-8
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/998">issue 998</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1403">issue 1403</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1949">issue 1949</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4321">issue 4321</a>)</p></li>
<li><p>The first spider middleware (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a>) now also
processes exceptions raised from callbacks that are generators
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4260">issue 4260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4272">issue 4272</a>)</p></li>
<li><p>Redirects to URLs starting with 3 slashes (<code class="docutils literal notranslate"><span class="pre">///</span></code>) are now supported
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4032">issue 4032</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4042">issue 4042</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> no longer accepts strings as <code class="docutils literal notranslate"><span class="pre">url</span></code> simply
because they have a colon (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2552">issue 2552</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4094">issue 4094</a>)</p></li>
<li><p>The correct encoding is now used for attach names in
<a class="reference internal" href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4229">issue 4229</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4239">issue 4239</a>)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code>, the default
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a>, no longer writes an extra <code class="docutils literal notranslate"><span class="pre">\r</span></code> character on
each line in Windows, which made the size of the <code class="docutils literal notranslate"><span class="pre">requests.seen</span></code> file
unnecessarily large on that platform (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4283">issue 4283</a>)</p></li>
<li><p>Z shell auto-completion now looks for <code class="docutils literal notranslate"><span class="pre">.html</span></code> files, not <code class="docutils literal notranslate"><span class="pre">.http</span></code> files,
and covers the <code class="docutils literal notranslate"><span class="pre">-h</span></code> command-line switch (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4122">issue 4122</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4291">issue 4291</a>)</p></li>
<li><p>Adding items to a <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.LocalCache</span></code> object
without a <code class="docutils literal notranslate"><span class="pre">limit</span></code> defined no longer raises a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a> exception
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4123">issue 4123</a>)</p></li>
<li><p>Fixed a typo in the message of the <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code></a> exception raised when
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.misc.create_instance()</span></code> gets both <code class="docutils literal notranslate"><span class="pre">settings</span></code> and
<code class="docutils literal notranslate"><span class="pre">crawler</span></code> set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4128">issue 4128</a>)</p></li>
</ul>
</section>
<section id="id94">
<h5>Documentation<a class="headerlink" href="#id94" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>API documentation now links to an online, syntax-highlighted view of the
corresponding source code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4148">issue 4148</a>)</p></li>
<li><p>Links to unexisting documentation pages now allow access to the sidebar
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4152">issue 4152</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4169">issue 4169</a>)</p></li>
<li><p>Cross-references within our documentation now display a tooltip when
hovered (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4173">issue 4173</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4183">issue 4183</a>)</p></li>
<li><p>Improved the documentation about <a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"><code class="xref py py-meth docutils literal notranslate"><span class="pre">LinkExtractor.extract_links</span></code></a> and
simplified <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractors</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4045">issue 4045</a>)</p></li>
<li><p>Clarified how <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.item" title="scrapy.loader.ItemLoader.item"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader.item</span></code></a>
works (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3574">issue 3574</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4099">issue 4099</a>)</p></li>
<li><p>Clarified that <a class="reference external" href="https://docs.python.org/3/library/logging.html#logging.basicConfig" title="(in Python v3.13)"><code class="xref py py-func docutils literal notranslate"><span class="pre">logging.basicConfig()</span></code></a> should not be used when also
using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2149">issue 2149</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2352">issue 2352</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3146">issue 3146</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3960">issue 3960</a>)</p></li>
<li><p>Clarified the requirements for <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#request-serialization"><span class="std std-ref">when using persistence</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4124">issue 4124</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4139">issue 4139</a>)</p></li>
<li><p>Clarified how to install a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#media-pipeline-example"><span class="std std-ref">custom image pipeline</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4034">issue 4034</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4252">issue 4252</a>)</p></li>
<li><p>Fixed the signatures of the <code class="docutils literal notranslate"><span class="pre">file_path</span></code> method in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">media pipeline</span></a> examples (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4290">issue 4290</a>)</p></li>
<li><p>Covered a backward-incompatible change in Scrapy 1.7.0 affecting custom
<a class="reference internal" href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.scheduler.Scheduler</span></code></a> subclasses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4274">issue 4274</a>)</p></li>
<li><p>Improved the <code class="docutils literal notranslate"><span class="pre">README.rst</span></code> and <code class="docutils literal notranslate"><span class="pre">CODE_OF_CONDUCT.md</span></code> files
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4059">issue 4059</a>)</p></li>
<li><p>Documentation examples are now checked as part of our test suite and we
have fixed some of the issues detected (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4142">issue 4142</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4146">issue 4146</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4171">issue 4171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4184">issue 4184</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4190">issue 4190</a>)</p></li>
<li><p>Fixed logic issues, broken links and typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4247">issue 4247</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4258">issue 4258</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4282">issue 4282</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4288">issue 4288</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4305">issue 4305</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4308">issue 4308</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4323">issue 4323</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4338">issue 4338</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4359">issue 4359</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4361">issue 4361</a>)</p></li>
<li><p>Improved consistency when referring to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method of an object
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4086">issue 4086</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4088">issue 4088</a>)</p></li>
<li><p>Fixed an inconsistency between code and output in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-overview"><span class="std std-ref">Scrapy at a glance</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4213">issue 4213</a>)</p></li>
<li><p>Extended <a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx" title="(in Sphinx v8.2.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">intersphinx</span></code></a> usage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4147">issue 4147</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4172">issue 4172</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4185">issue 4185</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4194">issue 4194</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4197">issue 4197</a>)</p></li>
<li><p>We now use a recent version of Python to build the documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4140">issue 4140</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4249">issue 4249</a>)</p></li>
<li><p>Cleaned up documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4143">issue 4143</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4275">issue 4275</a>)</p></li>
</ul>
</section>
<section id="id95">
<h5>Quality assurance<a class="headerlink" href="#id95" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Re-enabled proxy <code class="docutils literal notranslate"><span class="pre">CONNECT</span></code> tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2545">issue 2545</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4114">issue 4114</a>)</p></li>
<li><p>Added <a class="reference external" href="https://bandit.readthedocs.io/en/latest/">Bandit</a> security checks to our test suite (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4162">issue 4162</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4181">issue 4181</a>)</p></li>
<li><p>Added <a class="reference external" href="https://flake8.pycqa.org/en/latest/">Flake8</a> style checks to our test suite and applied many of the
corresponding changes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3944">issue 3944</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3945">issue 3945</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4137">issue 4137</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4157">issue 4157</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4167">issue 4167</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4174">issue 4174</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4186">issue 4186</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4195">issue 4195</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4238">issue 4238</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4246">issue 4246</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4355">issue 4355</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4360">issue 4360</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4365">issue 4365</a>)</p></li>
<li><p>Improved test coverage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4097">issue 4097</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4218">issue 4218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4236">issue 4236</a>)</p></li>
<li><p>Started reporting slowest tests, and improved the performance of some of
them (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4163">issue 4163</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4164">issue 4164</a>)</p></li>
<li><p>Fixed broken tests and refactored some tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4014">issue 4014</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4095">issue 4095</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4244">issue 4244</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4268">issue 4268</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4372">issue 4372</a>)</p></li>
<li><p>Modified the <a class="reference external" href="https://tox.wiki/en/latest/index.html" title="(in Project name not set v4.23)"><span class="xref std std-doc">tox</span></a> configuration to allow running tests
with any Python version, run <a class="reference external" href="https://bandit.readthedocs.io/en/latest/">Bandit</a> and <a class="reference external" href="https://flake8.pycqa.org/en/latest/">Flake8</a> tests by default, and
enforce a minimum tox version programmatically (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4179">issue 4179</a>)</p></li>
<li><p>Cleaned up code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3937">issue 3937</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4208">issue 4208</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4209">issue 4209</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4210">issue 4210</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4212">issue 4212</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4369">issue 4369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4376">issue 4376</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4378">issue 4378</a>)</p></li>
</ul>
</section>
<section id="changes-to-scheduler-queue-classes">
<span id="scheduler-queue-changes"></span><h5>Changes to scheduler queue classes<a class="headerlink" href="#changes-to-scheduler-queue-classes" title="Permalink to this heading">¶</a></h5>
<p>The following changes may impact any custom queue classes of all types:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">push</span></code> method no longer receives a second positional parameter
containing <code class="docutils literal notranslate"><span class="pre">request.priority</span> <span class="pre">*</span> <span class="pre">-1</span></code>. If you need that value, get it
from the first positional parameter, <code class="docutils literal notranslate"><span class="pre">request</span></code>, instead, or use
the new <code class="xref py py-meth docutils literal notranslate"><span class="pre">priority()</span></code>
method in <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.core.scheduler.ScrapyPriorityQueue</span></code>
subclasses.</p></li>
</ul>
<p>The following changes may impact custom priority queue classes:</p>
<ul class="simple">
<li><p>In the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method or the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> or <code class="docutils literal notranslate"><span class="pre">from_settings</span></code>
class methods:</p>
<ul>
<li><p>The parameter that used to contain a factory function,
<code class="docutils literal notranslate"><span class="pre">qfactory</span></code>, is now passed as a keyword parameter named
<code class="docutils literal notranslate"><span class="pre">downstream_queue_cls</span></code>.</p></li>
<li><p>A new keyword parameter has been added: <code class="docutils literal notranslate"><span class="pre">key</span></code>. It is a string
that is always an empty string for memory queues and indicates the
<code class="xref std std-setting docutils literal notranslate"><span class="pre">JOB_DIR</span></code> value for disk queues.</p></li>
<li><p>The parameter for disk queues that contains data from the previous
crawl, <code class="docutils literal notranslate"><span class="pre">startprios</span></code> or <code class="docutils literal notranslate"><span class="pre">slot_startprios</span></code>, is now passed as a
keyword parameter named <code class="docutils literal notranslate"><span class="pre">startprios</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">serialize</span></code> parameter is no longer passed. The disk queue
class must take care of request serialization on its own before
writing to disk, using the
<code class="xref py py-func docutils literal notranslate"><span class="pre">request_to_dict()</span></code> and
<code class="xref py py-func docutils literal notranslate"><span class="pre">request_from_dict()</span></code> functions from the
<code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.utils.reqser</span></code> module.</p></li>
</ul>
</li>
</ul>
<p>The following changes may impact custom disk and memory queue classes:</p>
<ul class="simple">
<li><p>The signature of the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method is now
<code class="docutils literal notranslate"><span class="pre">__init__(self,</span> <span class="pre">crawler,</span> <span class="pre">key)</span></code>.</p></li>
</ul>
<p>The following changes affect specifically the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ScrapyPriorityQueue</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">DownloaderAwarePriorityQueue</span></code> classes from
<a class="reference internal" href="index.html#module-scrapy.core.scheduler" title="scrapy.core.scheduler"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.core.scheduler</span></code></a> and may affect subclasses:</p>
<ul>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, most of the changes described above apply.</p>
<p><code class="docutils literal notranslate"><span class="pre">__init__</span></code> may still receive all parameters as positional parameters,
however:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">downstream_queue_cls</span></code>, which replaced <code class="docutils literal notranslate"><span class="pre">qfactory</span></code>, must be
instantiated differently.</p>
<p><code class="docutils literal notranslate"><span class="pre">qfactory</span></code> was instantiated with a priority value (integer).</p>
<p>Instances of <code class="docutils literal notranslate"><span class="pre">downstream_queue_cls</span></code> should be created using
the new
<code class="xref py py-meth docutils literal notranslate"><span class="pre">ScrapyPriorityQueue.qfactory</span></code>
or
<code class="xref py py-meth docutils literal notranslate"><span class="pre">DownloaderAwarePriorityQueue.pqfactory</span></code>
methods.</p>
</li>
<li><p>The new <code class="docutils literal notranslate"><span class="pre">key</span></code> parameter displaced the <code class="docutils literal notranslate"><span class="pre">startprios</span></code>
parameter 1 position to the right.</p></li>
</ul>
</li>
<li><p>The following class attributes have been added:</p>
<ul class="simple">
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawler</span></code></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">downstream_queue_cls</span></code>
(details above)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">key</span></code> (details above)</p></li>
</ul>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">serialize</span></code> attribute has been removed (details above)</p></li>
</ul>
<p>The following changes affect specifically the
<code class="xref py py-class docutils literal notranslate"><span class="pre">ScrapyPriorityQueue</span></code> class and may affect
subclasses:</p>
<ul>
<li><p>A new <code class="xref py py-meth docutils literal notranslate"><span class="pre">priority()</span></code>
method has been added which, given a request, returns
<code class="docutils literal notranslate"><span class="pre">request.priority</span> <span class="pre">*</span> <span class="pre">-1</span></code>.</p>
<p>It is used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">push()</span></code>
to make up for the removal of its <code class="docutils literal notranslate"><span class="pre">priority</span></code> parameter.</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">spider</span></code> attribute has been removed. Use
<code class="xref py py-attr docutils literal notranslate"><span class="pre">crawler.spider</span></code>
instead.</p></li>
</ul>
<p>The following changes affect specifically the
<code class="xref py py-class docutils literal notranslate"><span class="pre">DownloaderAwarePriorityQueue</span></code> class and may
affect subclasses:</p>
<ul class="simple">
<li><p>A new <code class="xref py py-attr docutils literal notranslate"><span class="pre">pqueues</span></code>
attribute offers a mapping of downloader slot names to the
corresponding instances of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">downstream_queue_cls</span></code>.</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3884">issue 3884</a>)</p>
</section>
</section>
<section id="scrapy-1-8-4-2024-02-14">
<span id="release-1-8-4"></span><h4>Scrapy 1.8.4 (2024-02-14)<a class="headerlink" href="#scrapy-1-8-4-2024-02-14" title="Permalink to this heading">¶</a></h4>
<p><strong>Security bug fixes:</strong></p>
<ul>
<li><p>Due to its <a class="reference external" href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS">ReDoS vulnerabilities</a>, <code class="docutils literal notranslate"><span class="pre">scrapy.utils.iterators.xmliter</span></code> is
now deprecated in favor of <a class="reference internal" href="index.html#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a>,
which <a class="reference internal" href="index.html#scrapy.spiders.XMLFeedSpider" title="scrapy.spiders.XMLFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code></a> now uses.</p>
<p>To minimize the impact of this change on existing code,
<a class="reference internal" href="index.html#scrapy.utils.iterators.xmliter_lxml" title="scrapy.utils.iterators.xmliter_lxml"><code class="xref py py-func docutils literal notranslate"><span class="pre">xmliter_lxml()</span></code></a> now supports indicating
the node namespace as a prefix in the node name, and big files with highly
nested trees when using libxml2 2.7+.</p>
<p>Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9">cc65-xxvf-f7r9 security advisory</a> for more information.</p>
</li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_WARNSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_WARNSIZE</span></code></a> now also apply
to the decompressed response body. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7">7j7m-v7m3-jqm7 security
advisory</a> for more information.</p></li>
<li><p>Also in relation with the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7">7j7m-v7m3-jqm7 security advisory</a>, use of the
<code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.decompression</span></code> module is discouraged and
will trigger a warning.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header is now dropped on redirects to a different
domain. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cw9j-q3vf-hrrv">cw9j-q3vf-hrrv security advisory</a> for more
information.</p>
</li>
</ul>
</section>
<section id="scrapy-1-8-3-2022-07-25">
<span id="release-1-8-3"></span><h4>Scrapy 1.8.3 (2022-07-25)<a class="headerlink" href="#scrapy-1-8-3-2022-07-25" title="Permalink to this heading">¶</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li><p>When <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>
processes a request with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, and that
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata includes proxy credentials,
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> sets
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata every time they process a request.</p>
<p>Because of request retries and redirects, the same request can be processed
by downloader middlewares more than once, including both
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> and
any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata of a request to a new value, but fail to remove
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header from the previous value of the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, causing the credentials of one proxy to be sent
to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior of
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a> is now
as follows when processing a request:</p>
<ul>
<li><p>If the request being processed defines <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata that
includes credentials, the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is always
updated to feature those credentials.</p></li>
<li><p>If the request being processed defines <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata
without credentials, the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is removed
<em>unless</em> it was originally defined for the same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL, remove
the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header.</p>
</li>
<li><p>If the request has no <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata, or that metadata is a
falsy value (e.g. <code class="docutils literal notranslate"><span class="pre">None</span></code>), the <code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header is
removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata but set the credentials through the
<code class="docutils literal notranslate"><span class="pre">Proxy-Authorization</span></code> header. Set proxy credentials through the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> metadata instead.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="scrapy-1-8-2-2022-03-01">
<span id="release-1-8-2"></span><h4>Scrapy 1.8.2 (2022-03-01)<a class="headerlink" href="#scrapy-1-8-2-2022-03-01" title="Permalink to this heading">¶</a></h4>
<p><strong>Security bug fixes:</strong></p>
<ul>
<li><p>When a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object with cookies defined gets a
redirect response causing a new <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object to be
scheduled, the cookies defined in the original
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object are no longer copied into the new
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
<p>If you manually set the <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header on a
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object and the domain name of the redirect
URL is not an exact match for the domain of the URL of the original
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, your <code class="docutils literal notranslate"><span class="pre">Cookie</span></code> header is now dropped
from the new <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
<p>The old behavior could be exploited by an attacker to gain access to your
cookies. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security advisory</a> for more
information.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
<code class="docutils literal notranslate"><span class="pre">example.com</span></code> and any subdomain) by defining the shared domain
suffix (e.g. <code class="docutils literal notranslate"><span class="pre">example.com</span></code>) as the cookie domain when defining
your cookies. See the documentation of the
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class for more information.</p>
</div>
</li>
<li><p>When the domain of a cookie, either received in the <code class="docutils literal notranslate"><span class="pre">Set-Cookie</span></code> header
of a response or defined in a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, is set
to a <a class="reference external" href="https://publicsuffix.org/">public suffix</a>, the cookie is now
ignored unless the cookie domain is the same as the request domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies into
your requests to some other domains. Please, see the <a class="reference external" href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96
security advisory</a> for more information.</p>
</li>
</ul>
</section>
<section id="scrapy-1-8-1-2021-10-05">
<span id="release-1-8-1"></span><h4>Scrapy 1.8.1 (2021-10-05)<a class="headerlink" href="#scrapy-1-8-1-2021-10-05" title="Permalink to this heading">¶</a></h4>
<ul>
<li><p><strong>Security bug fix:</strong></p>
<p>If you use
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>
(i.e. the <code class="docutils literal notranslate"><span class="pre">http_user</span></code> and <code class="docutils literal notranslate"><span class="pre">http_pass</span></code> spider attributes) for HTTP
authentication, any request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to unintended
domains, you must now additionally set a new, additional spider attribute,
<code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code>, and point it to the specific domain to which the
authentication credentials must be sent.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute is not set, the domain of the
first request will be considered the HTTP authentication target, and
authentication credentials will only be sent in requests targeting that
domain.</p>
<p>If you need to send the same HTTP authentication credentials to multiple
domains, you can use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.2)"><code class="xref py py-func docutils literal notranslate"><span class="pre">w3lib.http.basic_auth_header()</span></code></a> instead to
set the value of the <code class="docutils literal notranslate"><span class="pre">Authorization</span></code> header of your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP authentication
credentials to any domain, set the <code class="docutils literal notranslate"><span class="pre">http_auth_domain</span></code> spider attribute
to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>Finally, if you are a user of <a class="reference external" href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>, know that this version of
Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will
need to upgrade scrapy-splash to a greater version for it to continue to
work.</p>
</li>
</ul>
</section>
<section id="scrapy-1-8-0-2019-10-28">
<span id="release-1-8-0"></span><h4>Scrapy 1.8.0 (2019-10-28)<a class="headerlink" href="#scrapy-1-8-0-2019-10-28" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>Dropped Python 3.4 support and updated minimum requirements; made Python 3.8
support official</p></li>
<li><p>New <a class="reference internal" href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.from_curl</span></code></a> class method</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_USER_AGENT</span></code></a> settings</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> settings</p></li>
</ul>
<section id="id101">
<h5>Backward-incompatible changes<a class="headerlink" href="#id101" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Python 3.4 is no longer supported, and some of the minimum requirements of
Scrapy have also changed:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)"><span class="xref std std-doc">cssselect</span></a> 0.9.1</p></li>
<li><p><a class="reference external" href="https://cryptography.io/en/latest/">cryptography</a> 2.0</p></li>
<li><p><a class="reference external" href="https://lxml.de/">lxml</a> 3.5.0</p></li>
<li><p><a class="reference external" href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a> 16.2.0</p></li>
<li><p><a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a> 1.4.2</p></li>
<li><p><a class="reference external" href="https://service-identity.readthedocs.io/en/stable/">service_identity</a> 16.0.0</p></li>
<li><p><a class="reference external" href="https://six.readthedocs.io/">six</a> 1.10.0</p></li>
<li><p><a class="reference external" href="https://twisted.org/">Twisted</a> 17.9.0 (16.0.0 with Python 2)</p></li>
<li><p><a class="reference external" href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a> 4.1.3</p></li>
</ul>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3892">issue 3892</a>)</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">JSONRequest</span></code> is now called <a class="reference internal" href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonRequest</span></code></a> for
consistency with similar classes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3929">issue 3929</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3982">issue 3982</a>)</p></li>
<li><p>If you are using a custom context factory
(<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENTCONTEXTFACTORY</span></code></a>), its <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method must
accept two new parameters: <code class="docutils literal notranslate"><span class="pre">tls_verbose_logging</span></code> and <code class="docutils literal notranslate"><span class="pre">tls_ciphers</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2111">issue 2111</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3392">issue 3392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3442">issue 3442</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3450">issue 3450</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> now turns the values of its input item
into lists:</p>
<div class="highlight-pycon notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;field&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;value1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;field&quot;</span><span class="p">]</span>
<span class="go">[&#39;value1&#39;]</span>
</pre></div>
</div>
<p>This is needed to allow adding values to existing fields
(<code class="docutils literal notranslate"><span class="pre">loader.add_value('field',</span> <span class="pre">'value2')</span></code>).</p>
<p>(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3804">issue 3804</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3819">issue 3819</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3897">issue 3897</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3976">issue 3976</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3998">issue 3998</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4036">issue 4036</a>)</p>
</li>
</ul>
<p>See also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#id105"><span class="std std-ref">Deprecation removals</span></a> below.</p>
</section>
<section id="id102">
<h5>New features<a class="headerlink" href="#id102" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>A new <a class="reference internal" href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Request.from_curl</span></code></a> class
method allows <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#requests-from-curl"><span class="std std-ref">creating a request from a cURL command</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2985">issue 2985</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3862">issue 3862</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_PARSER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_PARSER</span></code></a> setting allows choosing which <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a>
parser to use. It includes built-in support for
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#python-robotfileparser"><span class="std std-ref">RobotFileParser</span></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#protego-parser"><span class="std std-ref">Protego</span></a> (default), Reppy, and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#rerp-parser"><span class="std std-ref">Robotexclusionrulesparser</span></a>, and allows you to
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#support-for-new-robots-parser"><span class="std std-ref">implement support for additional parsers</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/754">issue 754</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2669">issue 2669</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3796">issue 3796</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3935">issue 3935</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3969">issue 3969</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4006">issue 4006</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_USER_AGENT</span></code></a> setting allows defining a separate
user agent string to use for <a class="reference external" href="https://www.robotstxt.org/">robots.txt</a> parsing (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3931">issue 3931</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3966">issue 3966</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> no longer requires a <a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a> parameter
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/781">issue 781</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4016">issue 4016</a>)</p></li>
<li><p>Use the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> setting to customize
the TLS/SSL ciphers used by the default HTTP/1.1 downloader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3392">issue 3392</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3442">issue 3442</a>)</p></li>
<li><p>Set the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> setting to
<code class="docutils literal notranslate"><span class="pre">True</span></code> to enable debug-level messages about TLS connection parameters
after establishing HTTPS connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2111">issue 2111</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3450">issue 3450</a>)</p></li>
<li><p>Callbacks that receive keyword arguments
(see <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a>) can now be
tested using the new <a class="reference internal" href="index.html#scrapy.contracts.default.CallbackKeywordArgumentsContract" title="scrapy.contracts.default.CallbackKeywordArgumentsContract"><code class="xref py py-class docutils literal notranslate"><span class="pre">&#64;cb_kwargs</span></code></a>
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-contracts"><span class="std std-ref">spider contract</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3985">issue 3985</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3988">issue 3988</a>)</p></li>
<li><p>When a <a class="reference internal" href="index.html#scrapy.contracts.default.ScrapesContract" title="scrapy.contracts.default.ScrapesContract"><code class="xref py py-class docutils literal notranslate"><span class="pre">&#64;scrapes</span></code></a> spider
contract fails, all missing fields are now reported (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/766">issue 766</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3939">issue 3939</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#custom-log-formats"><span class="std std-ref">Custom log formats</span></a> can now drop messages by
having the corresponding methods of the configured <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_FORMATTER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMATTER</span></code></a>
return <code class="docutils literal notranslate"><span class="pre">None</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3984">issue 3984</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3987">issue 3987</a>)</p></li>
<li><p>A much improved completion definition is now available for <a class="reference external" href="https://www.zsh.org/">Zsh</a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4069">issue 4069</a>)</p></li>
</ul>
</section>
<section id="id103">
<h5>Bug fixes<a class="headerlink" href="#id103" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> no
longer makes later calls to <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_output_value()</span></code></a> or
<a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> return
empty data (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3804">issue 3804</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3819">issue 3819</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3897">issue 3897</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3976">issue 3976</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3998">issue 3998</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4036">issue 4036</a>)</p></li>
<li><p>Fixed <a class="reference internal" href="index.html#scrapy.statscollectors.DummyStatsCollector" title="scrapy.statscollectors.DummyStatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">DummyStatsCollector</span></code></a> raising a
<a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">TypeError</span></code></a> exception (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4007">issue 4007</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4052">issue 4052</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.file_path</span></code></a> and
<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.file_path</span></code></a> no longer choose
file extensions that are not <a class="reference external" href="https://www.iana.org/assignments/media-types/media-types.xhtml">registered with IANA</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1287">issue 1287</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3953">issue 3953</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3954">issue 3954</a>)</p></li>
<li><p>When using <a class="reference external" href="https://github.com/boto/botocore">botocore</a> to persist files in S3, all botocore-supported headers
are properly mapped now (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3904">issue 3904</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3905">issue 3905</a>)</p></li>
<li><p>FTP passwords in <code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI</span></code> containing percent-escaped characters
are now properly decoded (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3941">issue 3941</a>)</p></li>
<li><p>A memory-handling and error-handling issue in
<code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.ssl.get_temp_key_info()</span></code> has been fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3920">issue 3920</a>)</p></li>
</ul>
</section>
<section id="id104">
<h5>Documentation<a class="headerlink" href="#id104" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The documentation now covers how to define and configure a <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#custom-log-formats"><span class="std std-ref">custom log
format</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3616">issue 3616</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3660">issue 3660</a>)</p></li>
<li><p>API documentation added for <a class="reference internal" href="index.html#scrapy.exporters.MarshalItemExporter" title="scrapy.exporters.MarshalItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalItemExporter</span></code></a>
and <a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3973">issue 3973</a>)</p></li>
<li><p>API documentation added for <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItem</span></code> and
<a class="reference internal" href="index.html#scrapy.item.ItemMeta" title="scrapy.item.ItemMeta"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemMeta</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3999">issue 3999</a>)</p></li>
<li><p>Minor documentation fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2998">issue 2998</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3398">issue 3398</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3597">issue 3597</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3894">issue 3894</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3934">issue 3934</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3978">issue 3978</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3993">issue 3993</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4022">issue 4022</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4028">issue 4028</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4033">issue 4033</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4046">issue 4046</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4050">issue 4050</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4055">issue 4055</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4056">issue 4056</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4061">issue 4061</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4072">issue 4072</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4071">issue 4071</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4079">issue 4079</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4081">issue 4081</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4089">issue 4089</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4093">issue 4093</a>)</p></li>
</ul>
</section>
<section id="id105">
<span id="id106"></span><h5>Deprecation removals<a class="headerlink" href="#id105" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.xlib</span></code> has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4015">issue 4015</a>)</p></li>
</ul>
</section>
<section id="id107">
<span id="id108"></span><h5>Deprecations<a class="headerlink" href="#id107" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <a class="reference external" href="https://github.com/google/leveldb">LevelDB</a> storage backend
(<code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.LeveldbCacheStorage</span></code>) of
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code></a> is
deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4085">issue 4085</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4092">issue 4092</a>)</p></li>
<li><p>Use of the undocumented <code class="docutils literal notranslate"><span class="pre">SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</span></code> environment
variable is deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3910">issue 3910</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.item.DictItem</span></code> is deprecated, use <a class="reference internal" href="index.html#scrapy.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>
instead (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3999">issue 3999</a>)</p></li>
</ul>
</section>
<section id="other-changes">
<h5>Other changes<a class="headerlink" href="#other-changes" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p>Minimum versions of optional Scrapy requirements that are covered by
continuous integration tests have been updated:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/boto/botocore">botocore</a> 1.3.23</p></li>
<li><p><a class="reference external" href="https://python-pillow.org/">Pillow</a> 3.4.2</p></li>
</ul>
<p>Lower versions of these optional requirements may work, but it is not
guaranteed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3892">issue 3892</a>)</p>
</li>
<li><p>GitHub templates for bug reports and feature requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3126">issue 3126</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3471">issue 3471</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3749">issue 3749</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3754">issue 3754</a>)</p></li>
<li><p>Continuous integration fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3923">issue 3923</a>)</p></li>
<li><p>Code cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3391">issue 3391</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3907">issue 3907</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3946">issue 3946</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3950">issue 3950</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/4023">issue 4023</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/4031">issue 4031</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-7-4-2019-10-21">
<span id="release-1-7-4"></span><h4>Scrapy 1.7.4 (2019-10-21)<a class="headerlink" href="#scrapy-1-7-4-2019-10-21" title="Permalink to this heading">¶</a></h4>
<p>Revert the fix for <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3804">issue 3804</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3819">issue 3819</a>), which has a few undesired
side effects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3897">issue 3897</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3976">issue 3976</a>).</p>
<p>As a result, when an item loader is initialized with an item,
<a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> once again
makes later calls to <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_output_value()</span></code></a> or <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> return empty data.</p>
</section>
<section id="scrapy-1-7-3-2019-08-01">
<span id="release-1-7-3"></span><h4>Scrapy 1.7.3 (2019-08-01)<a class="headerlink" href="#scrapy-1-7-3-2019-08-01" title="Permalink to this heading">¶</a></h4>
<p>Enforce lxml 4.3.5 or lower for Python 3.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3912">issue 3912</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3918">issue 3918</a>).</p>
</section>
<section id="scrapy-1-7-2-2019-07-23">
<span id="release-1-7-2"></span><h4>Scrapy 1.7.2 (2019-07-23)<a class="headerlink" href="#scrapy-1-7-2-2019-07-23" title="Permalink to this heading">¶</a></h4>
<p>Fix Python 2 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3889">issue 3889</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3893">issue 3893</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3896">issue 3896</a>).</p>
</section>
<section id="scrapy-1-7-1-2019-07-18">
<span id="release-1-7-1"></span><h4>Scrapy 1.7.1 (2019-07-18)<a class="headerlink" href="#scrapy-1-7-1-2019-07-18" title="Permalink to this heading">¶</a></h4>
<p>Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.</p>
</section>
<section id="scrapy-1-7-0-2019-07-18">
<span id="release-1-7-0"></span><h4>Scrapy 1.7.0 (2019-07-18)<a class="headerlink" href="#scrapy-1-7-0-2019-07-18" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI
is the result of an erroneous commit tagging and does not include all
the changes described below.</p>
</div>
<p>Highlights:</p>
<ul class="simple">
<li><p>Improvements for crawls targeting multiple domains</p></li>
<li><p>A cleaner way to pass arguments to callbacks</p></li>
<li><p>A new class for JSON requests</p></li>
<li><p>Improvements for rule-based spiders</p></li>
<li><p>New features for feed exports</p></li>
</ul>
<section id="id109">
<h5>Backward-incompatible changes<a class="headerlink" href="#id109" title="Permalink to this heading">¶</a></h5>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">429</span></code> is now part of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> setting by default</p>
<p>This change is <strong>backward incompatible</strong>. If you don’t want to retry
<code class="docutils literal notranslate"><span class="pre">429</span></code>, you must override <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> accordingly.</p>
</li>
<li><p><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code></a>
no longer accept a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instance, they
only accept a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass now.</p>
<p><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instances were never meant to
work, and they were not working as one would expect: instead of using the
passed <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instance, their
<code class="xref py py-class docutils literal notranslate"><span class="pre">from_crawler</span></code> method was called to generate
a new instance.</p>
</li>
<li><p>Non-default values for the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting
may stop working. Scheduler priority queue classes now need to handle
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects instead of arbitrary Python data
structures.</p></li>
<li><p>An additional <code class="docutils literal notranslate"><span class="pre">crawler</span></code> parameter has been added to the <code class="docutils literal notranslate"><span class="pre">__init__</span></code>
method of the <a class="reference internal" href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Scheduler</span></code></a> class. Custom
scheduler subclasses which don’t accept arbitrary parameters in their
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method might break because of this change.</p>
<p>For more information, see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER</span></code></a>.</p>
</li>
</ul>
<p>See also <a class="hxr-hoverxref hxr-tooltip reference internal" href="#id113"><span class="std std-ref">Deprecation removals</span></a> below.</p>
</section>
<section id="id110">
<h5>New features<a class="headerlink" href="#id110" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>A new scheduler priority queue,
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>, may be
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#broad-crawls-scheduler-priority-queue"><span class="std std-ref">enabled</span></a> for a significant
scheduling improvement on crawls targeting multiple web domains, at the
cost of no <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3520">issue 3520</a>)</p></li>
<li><p>A new <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute
provides a cleaner way to pass keyword arguments to callback methods
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1138">issue 1138</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3563">issue 3563</a>)</p></li>
<li><p>A new <a class="reference internal" href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONRequest</span></code></a> class offers a more
convenient way to build JSON requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3504">issue 3504</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3505">issue 3505</a>)</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">process_request</span></code> callback passed to the <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a>
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method now receives the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object that
originated the request as its second argument (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3682">issue 3682</a>)</p></li>
<li><p>A new <code class="docutils literal notranslate"><span class="pre">restrict_text</span></code> parameter for the
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method allows filtering links by linking text (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3622">issue 3622</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3635">issue 3635</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a> setting allows defining a custom ACL
for feeds exported to Amazon S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3607">issue 3607</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting allows using FTP’s active
connection mode for feeds exported to FTP servers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3829">issue 3829</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> setting allows overriding which
HTML tags are ignored when searching a response for HTML meta tags that
trigger a redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1422">issue 1422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3768">issue 3768</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a> request meta key exposes the reason
(status code, meta refresh) behind every followed redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3581">issue 3581</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3687">issue 3687</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">SCRAPY_CHECK</span></code> variable is now set to the <code class="docutils literal notranslate"><span class="pre">true</span></code> string during runs
of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a> command, which allows <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#detecting-contract-check-runs"><span class="std std-ref">detecting contract
check runs from code</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3704">issue 3704</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3739">issue 3739</a>)</p></li>
<li><p>A new <code class="xref py py-meth docutils literal notranslate"><span class="pre">Item.deepcopy()</span></code> method makes it
easier to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#copying-items"><span class="std std-ref">deep-copy items</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1493">issue 1493</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3671">issue 3671</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.extensions.corestats.CoreStats" title="scrapy.extensions.corestats.CoreStats"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoreStats</span></code></a> also logs
<code class="docutils literal notranslate"><span class="pre">elapsed_time_seconds</span></code> now (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3638">issue 3638</a>)</p></li>
<li><p>Exceptions from <a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-loaders-processors"><span class="std std-ref">input and output
processors</span></a> are now more verbose
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3836">issue 3836</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3840">issue 3840</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code></a>
now fail gracefully if they receive a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>
subclass instance instead of the subclass itself (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2283">issue 2283</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3610">issue 3610</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3872">issue 3872</a>)</p></li>
</ul>
</section>
<section id="id111">
<h5>Bug fixes<a class="headerlink" href="#id111" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a>
is now also invoked for generators (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/220">issue 220</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2061">issue 2061</a>)</p></li>
<li><p>System exceptions like <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt">KeyboardInterrupt</a> are no longer caught
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3726">issue 3726</a>)</p></li>
<li><p><a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> no
longer makes later calls to <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_output_value()</span></code></a> or
<a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> return
empty data (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3804">issue 3804</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3819">issue 3819</a>)</p></li>
<li><p>The images pipeline (<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a>) no
longer ignores these Amazon S3 settings: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_USE_SSL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_USE_SSL</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_VERIFY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_VERIFY</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3625">issue 3625</a>)</p></li>
<li><p>Fixed a memory leak in <code class="docutils literal notranslate"><span class="pre">scrapy.pipelines.media.MediaPipeline</span></code> affecting,
for example, non-200 responses and exceptions from custom middlewares
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3813">issue 3813</a>)</p></li>
<li><p>Requests with private callbacks are now correctly unserialized from disk
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3790">issue 3790</a>)</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code>
now handles invalid methods like major web browsers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3777">issue 3777</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3794">issue 3794</a>)</p></li>
</ul>
</section>
<section id="id112">
<h5>Documentation<a class="headerlink" href="#id112" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>A new topic, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-dynamic-content"><span class="std std-ref">Selecting dynamically-loaded content</span></a>, covers recommended approaches
to read dynamically-loaded data (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3703">issue 3703</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-broad-crawls"><span class="std std-ref">Broad Crawls</span></a> now features information about memory usage
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1264">issue 1264</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3866">issue 3866</a>)</p></li>
<li><p>The documentation of <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> now covers how to access
the text of a link when using <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3711">issue 3711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3712">issue 3712</a>)</p></li>
<li><p>A new section, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#httpcache-storage-custom"><span class="std std-ref">Writing your own storage backend</span></a>, covers writing a custom
cache storage backend for
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3683">issue 3683</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3692">issue 3692</a>)</p></li>
<li><p>A new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq"><span class="std std-ref">FAQ</span></a> entry, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-split-item"><span class="std std-ref">How to split an item into multiple items in an item pipeline?</span></a>, explains what to do
when you want to split an item into multiple items from an item pipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2240">issue 2240</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3672">issue 3672</a>)</p></li>
<li><p>Updated the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">FAQ entry about crawl order</span></a> to explain why
the first few requests rarely follow the desired order (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1739">issue 1739</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3621">issue 3621</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOGSTATS_INTERVAL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOGSTATS_INTERVAL</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3730">issue 3730</a>), the
<a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.file_path</span></code></a>
and
<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.file_path</span></code></a>
methods (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2253">issue 2253</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3609">issue 3609</a>) and the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.stop" title="scrapy.crawler.Crawler.stop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Crawler.stop()</span></code></a> method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3842">issue 3842</a>)
are now documented</p></li>
<li><p>Some parts of the documentation that were confusing or misleading are now
clearer (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1347">issue 1347</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1789">issue 1789</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2289">issue 2289</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3069">issue 3069</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3615">issue 3615</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3626">issue 3626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3668">issue 3668</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3670">issue 3670</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3673">issue 3673</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3728">issue 3728</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3762">issue 3762</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3861">issue 3861</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3882">issue 3882</a>)</p></li>
<li><p>Minor documentation fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3648">issue 3648</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3649">issue 3649</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3662">issue 3662</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3674">issue 3674</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3676">issue 3676</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3694">issue 3694</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3724">issue 3724</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3764">issue 3764</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3767">issue 3767</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3791">issue 3791</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3797">issue 3797</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3806">issue 3806</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3812">issue 3812</a>)</p></li>
</ul>
</section>
<section id="id113">
<span id="id114"></span><h5>Deprecation removals<a class="headerlink" href="#id113" title="Permalink to this heading">¶</a></h5>
<p>The following deprecated APIs have been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3578">issue 3578</a>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.conf</span></code> (use <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Crawler.settings</span></code></a>)</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">scrapy.core.downloader.handlers</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">http.HttpDownloadHandler</span></code> (use <code class="docutils literal notranslate"><span class="pre">http10.HTTP10DownloadHandler</span></code>)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.loader.ItemLoader._get_values</span></code> (use <code class="docutils literal notranslate"><span class="pre">_get_xpathvalues</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.loader.XPathItemLoader</span></code> (use <a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.log</span></code> (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a>)</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">scrapy.pipelines</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">files.FilesPipeline.file_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.file_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.image_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.thumb_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">thumb_path</span></code>)</p></li>
</ul>
</li>
<li><p>From both <code class="docutils literal notranslate"><span class="pre">scrapy.selector</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.selector.lxmlsel</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">HtmlXPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XmlXPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">XPathSelectorList</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</p></li>
</ul>
</li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">scrapy.selector.csstranslator</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ScrapyGenericTranslator</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator">parsel.csstranslator.GenericTranslator</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ScrapyHTMLTranslator</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator">parsel.csstranslator.HTMLTranslator</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ScrapyXPathExpr</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr">parsel.csstranslator.XPathExpr</a>)</p></li>
</ul>
</li>
<li><p>From <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">_root</span></code> (both the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method argument and the object property, use
<code class="docutils literal notranslate"><span class="pre">root</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">extract_unquoted</span></code> (use <code class="docutils literal notranslate"><span class="pre">getall</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">select</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</p></li>
</ul>
</li>
<li><p>From <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">extract_unquoted</span></code> (use <code class="docutils literal notranslate"><span class="pre">getall</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">select</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.spiders.BaseSpider</span></code> (use <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>)</p></li>
<li><p>From <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> (and subclasses):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code> (use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#spider-download-delay-attribute"><span class="std std-ref">download_delay</span></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">set_crawler</span></code> (use <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code>)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.spiders.spiders</span></code> (use <a class="reference internal" href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpiderLoader</span></code></a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.telnet</span></code> (use <a class="reference internal" href="index.html#module-scrapy.extensions.telnet" title="scrapy.extensions.telnet: Telnet console"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.extensions.telnet</span></code></a>)</p></li>
<li><p>From <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">str_to_unicode</span></code> (use <code class="docutils literal notranslate"><span class="pre">to_unicode</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">unicode_to_str</span></code> (use <code class="docutils literal notranslate"><span class="pre">to_bytes</span></code>)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.response.body_or_str</span></code></p></li>
</ul>
<p>The following deprecated settings have also been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3578">issue 3578</a>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SPIDER_MANAGER_CLASS</span></code> (use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a>)</p></li>
</ul>
</section>
<section id="id115">
<span id="id116"></span><h5>Deprecations<a class="headerlink" href="#id115" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">queuelib.PriorityQueue</span></code> value for the
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting is deprecated. Use
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.ScrapyPriorityQueue</span></code> instead.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">process_request</span></code> callbacks passed to <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> that
do not accept two arguments are deprecated.</p></li>
<li><p>The following modules are deprecated:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.http</span></code> (use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http">w3lib.http</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup</span></code> (use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html">w3lib.html</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.multipart</span></code> (use <a class="reference external" href="https://urllib3.readthedocs.io/en/latest/index.html">urllib3</a>)</p></li>
</ul>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MergeDict</span></code> class is deprecated for Python 3
code bases. Use <a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.ChainMap" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChainMap</span></code></a> instead. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3878">issue 3878</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.is_gzipped</span></code> function is deprecated. Use
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.gzip_magic_number</span></code> instead.</p></li>
</ul>
</section>
<section id="id117">
<h5>Other changes<a class="headerlink" href="#id117" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>It is now possible to run all tests from the same <a class="reference external" href="https://pypi.org/project/tox/">tox</a> environment in
parallel; the documentation now covers <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#running-tests"><span class="std std-ref">this and other ways to run
tests</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3707">issue 3707</a>)</p></li>
<li><p>It is now possible to generate an API documentation coverage report
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3806">issue 3806</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3810">issue 3810</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3860">issue 3860</a>)</p></li>
<li><p>The <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#documentation-policies"><span class="std std-ref">documentation policies</span></a> now require
<a class="reference external" href="https://docs.python.org/3/glossary.html#term-docstring">docstrings</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3701">issue 3701</a>) that follow <a class="reference external" href="https://peps.python.org/pep-0257/">PEP 257</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3748">issue 3748</a>)</p></li>
<li><p>Internal fixes and cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3629">issue 3629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3643">issue 3643</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3684">issue 3684</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3698">issue 3698</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3734">issue 3734</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3735">issue 3735</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3736">issue 3736</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3737">issue 3737</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3809">issue 3809</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3821">issue 3821</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3825">issue 3825</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3827">issue 3827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3833">issue 3833</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3857">issue 3857</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3877">issue 3877</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-6-0-2019-01-30">
<span id="release-1-6-0"></span><h4>Scrapy 1.6.0 (2019-01-30)<a class="headerlink" href="#scrapy-1-6-0-2019-01-30" title="Permalink to this heading">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li><p>better Windows support;</p></li>
<li><p>Python 3.7 compatibility;</p></li>
<li><p>big documentation improvements, including a switch
from <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> + <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> API to <code class="docutils literal notranslate"><span class="pre">.get()</span></code> + <code class="docutils literal notranslate"><span class="pre">.getall()</span></code>
API;</p></li>
<li><p>feed exports, FilePipeline and MediaPipeline improvements;</p></li>
<li><p>better extensibility: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-item_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">item_error</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-request_reached_downloader"><code class="xref std std-signal docutils literal notranslate"><span class="pre">request_reached_downloader</span></code></a> signals; <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support
for feed exporters, feed storages and dupefilters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.contracts</span></code> fixes and new features;</p></li>
<li><p>telnet console security improvements, first released as a
backport in <a class="hxr-hoverxref hxr-tooltip reference internal" href="#release-1-5-2"><span class="std std-ref">Scrapy 1.5.2 (2019-01-22)</span></a>;</p></li>
<li><p>clean-up of the deprecated code;</p></li>
<li><p>various bug fixes, small new features and usability improvements across
the codebase.</p></li>
</ul>
<section id="selector-api-changes">
<h5>Selector API changes<a class="headerlink" href="#selector-api-changes" title="Permalink to this heading">¶</a></h5>
<p>While these are not changes in Scrapy itself, but rather in the <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a>
library which Scrapy uses for xpath/css selectors, these changes are
worth mentioning here. Scrapy now depends on parsel &gt;= 1.5, and
Scrapy documentation is updated to follow recent <code class="docutils literal notranslate"><span class="pre">parsel</span></code> API conventions.</p>
<p>Most visible change is that <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> selector
methods are now preferred over <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> and <code class="docutils literal notranslate"><span class="pre">.extract()</span></code>.
We feel that these new methods result in a more concise and readable code.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are currently <strong>no plans</strong> to deprecate <code class="docutils literal notranslate"><span class="pre">.extract()</span></code>
and <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> methods.</p>
</div>
<p>Another useful new feature is the introduction of <code class="docutils literal notranslate"><span class="pre">Selector.attrib</span></code> and
<code class="docutils literal notranslate"><span class="pre">SelectorList.attrib</span></code> properties, which make it easier to get
attributes of HTML elements. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
<p>CSS selectors are cached in parsel &gt;= 1.5, which makes them faster
when the same CSS path is used many times. This is very common in
case of Scrapy spiders: callbacks are usually called several times,
on different pages.</p>
<p>If you’re using custom <code class="docutils literal notranslate"><span class="pre">Selector</span></code> or <code class="docutils literal notranslate"><span class="pre">SelectorList</span></code> subclasses,
a <strong>backward incompatible</strong> change in parsel may affect your code.
See <a class="reference external" href="https://parsel.readthedocs.io/en/latest/history.html">parsel changelog</a> for a detailed description, as well as for the
full list of improvements.</p>
</section>
<section id="telnet-console">
<h5>Telnet console<a class="headerlink" href="#telnet-console" title="Permalink to this heading">¶</a></h5>
<p><strong>Backward incompatible</strong>: Scrapy’s telnet console now requires username
and password. See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a> for more details. This change
fixes a <strong>security issue</strong>; see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#release-1-5-2"><span class="std std-ref">Scrapy 1.5.2 (2019-01-22)</span></a> release notes for details.</p>
</section>
<section id="new-extensibility-features">
<h5>New extensibility features<a class="headerlink" href="#new-extensibility-features" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support is added to feed exporters and feed storages. This,
among other things, allows to access Scrapy settings from custom feed
storages and exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1605">issue 1605</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3348">issue 3348</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support is added to dupefilters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2956">issue 2956</a>); this allows
to access e.g. settings or a spider from a dupefilter.</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-item_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">item_error</span></code></a> is fired when an error happens in a pipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3256">issue 3256</a>);</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-request_reached_downloader"><code class="xref std std-signal docutils literal notranslate"><span class="pre">request_reached_downloader</span></code></a> is fired when Downloader gets
a new Request; this signal can be useful e.g. for custom Schedulers
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3393">issue 3393</a>).</p></li>
<li><p>new SitemapSpider <a class="reference internal" href="index.html#scrapy.spiders.SitemapSpider.sitemap_filter" title="scrapy.spiders.SitemapSpider.sitemap_filter"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sitemap_filter()</span></code></a> method which allows
to select sitemap entries based on their attributes in SitemapSpider
subclasses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3512">issue 3512</a>).</p></li>
<li><p>Lazy loading of Downloader Handlers is now optional; this enables better
initialization error handling in custom Downloader Handlers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3394">issue 3394</a>).</p></li>
</ul>
</section>
<section id="new-filepipeline-and-mediapipeline-features">
<h5>New FilePipeline and MediaPipeline features<a class="headerlink" href="#new-filepipeline-and-mediapipeline-features" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Expose more options for S3FilesStore: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_USE_SSL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_USE_SSL</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_VERIFY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_VERIFY</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a>.
For example, this allows to use alternative or self-hosted
AWS-compatible providers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2609">issue 2609</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3548">issue 3548</a>).</p></li>
<li><p>ACL support for Google Cloud Storage: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FILES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_GCS_ACL</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-IMAGES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_GCS_ACL</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3199">issue 3199</a>).</p></li>
</ul>
</section>
<section id="scrapy-contracts-improvements">
<h5><code class="docutils literal notranslate"><span class="pre">scrapy.contracts</span></code> improvements<a class="headerlink" href="#scrapy-contracts-improvements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Exceptions in contracts code are handled better (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3377">issue 3377</a>);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dont_filter=True</span></code> is used for contract requests, which allows to test
different callbacks with the same URL (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3381">issue 3381</a>);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">request_cls</span></code> attribute in Contract subclasses allow to use different
Request classes in contracts, for example FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3383">issue 3383</a>).</p></li>
<li><p>Fixed errback handling in contracts, e.g. for cases where a contract
is executed for URL which returns non-200 response (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3371">issue 3371</a>).</p></li>
</ul>
</section>
<section id="usability-improvements">
<h5>Usability improvements<a class="headerlink" href="#usability-improvements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>more stats for RobotsTxtMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3100">issue 3100</a>)</p></li>
<li><p>INFO log level is used to show telnet host/port (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3115">issue 3115</a>)</p></li>
<li><p>a message is added to IgnoreRequest in RobotsTxtMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3113">issue 3113</a>)</p></li>
<li><p>better validation of <code class="docutils literal notranslate"><span class="pre">url</span></code> argument in <code class="docutils literal notranslate"><span class="pre">Response.follow</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3131">issue 3131</a>)</p></li>
<li><p>non-zero exit code is returned from Scrapy commands when error happens
on spider initialization (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3226">issue 3226</a>)</p></li>
<li><p>Link extraction improvements: “ftp” is added to scheme list (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3152">issue 3152</a>);
“flv” is added to common video extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3165">issue 3165</a>)</p></li>
<li><p>better error message when an exporter is disabled (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3358">issue 3358</a>);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">--help</span></code> mentions syntax required for local files
(<code class="docutils literal notranslate"><span class="pre">./file.html</span></code>) - <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3496">issue 3496</a>.</p></li>
<li><p>Referer header value is added to RFPDupeFilter log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3588">issue 3588</a>)</p></li>
</ul>
</section>
<section id="id118">
<h5>Bug fixes<a class="headerlink" href="#id118" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>fixed issue with extra blank lines in .csv exports under Windows
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3039">issue 3039</a>);</p></li>
<li><p>proper handling of pickling errors in Python 3 when serializing objects
for disk queues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3082">issue 3082</a>)</p></li>
<li><p>flags are now preserved when copying Requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3342">issue 3342</a>);</p></li>
<li><p>FormRequest.from_response clickdata shouldn’t ignore elements with
<code class="docutils literal notranslate"><span class="pre">input[type=image]</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3153">issue 3153</a>).</p></li>
<li><p>FormRequest.from_response should preserve duplicate keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3247">issue 3247</a>)</p></li>
</ul>
</section>
<section id="documentation-improvements">
<h5>Documentation improvements<a class="headerlink" href="#documentation-improvements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Docs are re-written to suggest .get/.getall API instead of
.extract/.extract_first. Also, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a> docs are updated
and re-structured to match latest parsel docs; they now contain more topics,
such as <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a> or <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors-css-extensions"><span class="std std-ref">Extensions to CSS Selectors</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3390">issue 3390</a>).</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-developer-tools"><span class="std std-ref">Using your browser’s Developer Tools for scraping</span></a> is a new tutorial which replaces
old Firefox and Firebug tutorials (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3400">issue 3400</a>).</p></li>
<li><p>SCRAPY_PROJECT environment variable is documented (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3518">issue 3518</a>);</p></li>
<li><p>troubleshooting section is added to install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3517">issue 3517</a>);</p></li>
<li><p>improved links to beginner resources in the tutorial
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3367">issue 3367</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3468">issue 3468</a>);</p></li>
<li><p>fixed <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> default values in docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3335">issue 3335</a>);</p></li>
<li><p>remove unused <code class="docutils literal notranslate"><span class="pre">DEPTH_STATS</span></code> option from docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3245">issue 3245</a>);</p></li>
<li><p>other cleanups (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3347">issue 3347</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3350">issue 3350</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3445">issue 3445</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3544">issue 3544</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3605">issue 3605</a>).</p></li>
</ul>
</section>
<section id="id119">
<h5>Deprecation removals<a class="headerlink" href="#id119" title="Permalink to this heading">¶</a></h5>
<p>Compatibility shims for pre-1.0 Scrapy module names are removed
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3318">issue 3318</a>):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.command</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> (with all submodules)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code> (with all submodules)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.dupefilter</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.linkextractor</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.project</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.spider</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.spidermanager</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.squeue</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.stats</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.statscol</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.decorator</span></code></p></li>
</ul>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="#module-relocations"><span class="std std-ref">Module Relocations</span></a> for more information, or use suggestions
from Scrapy 1.5.x deprecation warnings to update your code.</p>
<p>Other deprecation removals:</p>
<ul class="simple">
<li><p>Deprecated scrapy.interfaces.ISpiderManager is removed; please use
scrapy.interfaces.ISpiderLoader.</p></li>
<li><p>Deprecated <code class="docutils literal notranslate"><span class="pre">CrawlerSettings</span></code> class is removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3327">issue 3327</a>).</p></li>
<li><p>Deprecated <code class="docutils literal notranslate"><span class="pre">Settings.overrides</span></code> and <code class="docutils literal notranslate"><span class="pre">Settings.defaults</span></code> attributes
are removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3327">issue 3327</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3359">issue 3359</a>).</p></li>
</ul>
</section>
<section id="other-improvements-cleanups">
<h5>Other improvements, cleanups<a class="headerlink" href="#other-improvements-cleanups" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>All Scrapy tests now pass on Windows; Scrapy testing suite is executed
in a Windows environment on CI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3315">issue 3315</a>).</p></li>
<li><p>Python 3.7 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3326">issue 3326</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3150">issue 3150</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3547">issue 3547</a>).</p></li>
<li><p>Testing and CI fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3526">issue 3526</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3538">issue 3538</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3308">issue 3308</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3311">issue 3311</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3309">issue 3309</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3305">issue 3305</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3210">issue 3210</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3299">issue 3299</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.http.cookies.CookieJar.clear</span></code> accepts “domain”, “path” and “name”
optional arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3231">issue 3231</a>).</p></li>
<li><p>additional files are included to sdist (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3495">issue 3495</a>);</p></li>
<li><p>code style fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3405">issue 3405</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3304">issue 3304</a>);</p></li>
<li><p>unneeded .strip() call is removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3519">issue 3519</a>);</p></li>
<li><p>collections.deque is used to store MiddlewareManager methods instead
of a list (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3476">issue 3476</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-5-2-2019-01-22">
<span id="release-1-5-2"></span><h4>Scrapy 1.5.2 (2019-01-22)<a class="headerlink" href="#scrapy-1-5-2-2019-01-22" title="Permalink to this heading">¶</a></h4>
<ul>
<li><p><em>Security bugfix</em>: Telnet console extension can be easily exploited by rogue
websites POSTing content to <a class="reference external" href="http://localhost:6023">http://localhost:6023</a>, we haven’t found a way to
exploit it from Scrapy, but it is very easy to trick a browser to do so and
elevates the risk for local development environment.</p>
<p><em>The fix is backward incompatible</em>, it enables telnet user-password
authentication by default with a random generated password. If you can’t
upgrade right away, please consider setting <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PORT</span></code></a>
out of its default value.</p>
<p>See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a> documentation for more info</p>
</li>
<li><p>Backport CI build failure under GCE environment due to boto import error.</p></li>
</ul>
</section>
<section id="scrapy-1-5-1-2018-07-12">
<span id="release-1-5-1"></span><h4>Scrapy 1.5.1 (2018-07-12)<a class="headerlink" href="#scrapy-1-5-1-2018-07-12" title="Permalink to this heading">¶</a></h4>
<p>This is a maintenance release with important bug fixes, but no new features:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">O(N^2)</span></code> gzip decompression issue which affected Python 3 and PyPy
is fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3281">issue 3281</a>);</p></li>
<li><p>skipping of TLS validation errors is improved (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3166">issue 3166</a>);</p></li>
<li><p>Ctrl-C handling is fixed in Python 3.5+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3096">issue 3096</a>);</p></li>
<li><p>testing fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3092">issue 3092</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3263">issue 3263</a>);</p></li>
<li><p>documentation improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3058">issue 3058</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3059">issue 3059</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3089">issue 3089</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3123">issue 3123</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3127">issue 3127</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3189">issue 3189</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3224">issue 3224</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3280">issue 3280</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3279">issue 3279</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3201">issue 3201</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3260">issue 3260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3284">issue 3284</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3298">issue 3298</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3294">issue 3294</a>).</p></li>
</ul>
</section>
<section id="scrapy-1-5-0-2017-12-29">
<span id="release-1-5-0"></span><h4>Scrapy 1.5.0 (2017-12-29)<a class="headerlink" href="#scrapy-1-5-0-2017-12-29" title="Permalink to this heading">¶</a></h4>
<p>This release brings small new features and improvements across the codebase.
Some highlights:</p>
<ul class="simple">
<li><p>Google Cloud Storage is supported in FilesPipeline and ImagesPipeline.</p></li>
<li><p>Crawling with proxy servers becomes more efficient, as connections
to proxies can be reused now.</p></li>
<li><p>Warnings, exception and logging messages are improved to make debugging
easier.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> command now allows to set custom request meta via
<code class="docutils literal notranslate"><span class="pre">--meta</span></code> argument.</p></li>
<li><p>Compatibility with Python 3.6, PyPy and PyPy3 is improved;
PyPy and PyPy3 are now supported officially, by running tests on CI.</p></li>
<li><p>Better default handling of HTTP 308, 522 and 524 status codes.</p></li>
<li><p>Documentation is improved, as usual.</p></li>
</ul>
<section id="id120">
<h5>Backward Incompatible Changes<a class="headerlink" href="#id120" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Scrapy 1.5 drops support for Python 3.3.</p></li>
<li><p>Default Scrapy User-Agent now uses https link to scrapy.org (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2983">issue 2983</a>).
<strong>This is technically backward-incompatible</strong>; override
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> if you relied on old value.</p></li>
<li><p>Logging of settings overridden by <code class="docutils literal notranslate"><span class="pre">custom_settings</span></code> is fixed;
<strong>this is technically backward-incompatible</strong> because the logger
changes from <code class="docutils literal notranslate"><span class="pre">[scrapy.utils.log]</span></code> to <code class="docutils literal notranslate"><span class="pre">[scrapy.crawler]</span></code>. If you’re
parsing Scrapy logs, please update your log parsers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1343">issue 1343</a>).</p></li>
<li><p>LinkExtractor now ignores <code class="docutils literal notranslate"><span class="pre">m4v</span></code> extension by default, this is change
in behavior.</p></li>
<li><p>522 and 524 status codes are added to <code class="docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2851">issue 2851</a>)</p></li>
</ul>
</section>
<section id="id121">
<h5>New features<a class="headerlink" href="#id121" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Support <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> tags in <code class="docutils literal notranslate"><span class="pre">Response.follow</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2785">issue 2785</a>)</p></li>
<li><p>Support for <code class="docutils literal notranslate"><span class="pre">ptpython</span></code> REPL (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2654">issue 2654</a>)</p></li>
<li><p>Google Cloud Storage support for FilesPipeline and ImagesPipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2923">issue 2923</a>).</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">--meta</span></code> option of the “scrapy parse” command allows to pass additional
request.meta (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2883">issue 2883</a>)</p></li>
<li><p>Populate spider variable when using <code class="docutils literal notranslate"><span class="pre">shell.inspect_response</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2812">issue 2812</a>)</p></li>
<li><p>Handle HTTP 308 Permanent Redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2844">issue 2844</a>)</p></li>
<li><p>Add 522 and 524 to <code class="docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2851">issue 2851</a>)</p></li>
<li><p>Log versions information at startup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2857">issue 2857</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.mail.MailSender</span></code> now works in Python 3 (it requires Twisted 17.9.0)</p></li>
<li><p>Connections to proxy servers are reused (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2743">issue 2743</a>)</p></li>
<li><p>Add template for a downloader middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2755">issue 2755</a>)</p></li>
<li><p>Explicit message for NotImplementedError when parse callback not defined
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2831">issue 2831</a>)</p></li>
<li><p>CrawlerProcess got an option to disable installation of root log handler
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2921">issue 2921</a>)</p></li>
<li><p>LinkExtractor now ignores <code class="docutils literal notranslate"><span class="pre">m4v</span></code> extension by default</p></li>
<li><p>Better log messages for responses over <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_WARNSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_WARNSIZE</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> limits (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2927">issue 2927</a>)</p></li>
<li><p>Show warning when a URL is put to <code class="docutils literal notranslate"><span class="pre">Spider.allowed_domains</span></code> instead of
a domain (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2250">issue 2250</a>).</p></li>
</ul>
</section>
<section id="id122">
<h5>Bug fixes<a class="headerlink" href="#id122" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fix logging of settings overridden by <code class="docutils literal notranslate"><span class="pre">custom_settings</span></code>;
<strong>this is technically backward-incompatible</strong> because the logger
changes from <code class="docutils literal notranslate"><span class="pre">[scrapy.utils.log]</span></code> to <code class="docutils literal notranslate"><span class="pre">[scrapy.crawler]</span></code>, so please
update your log parsers if needed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1343">issue 1343</a>)</p></li>
<li><p>Default Scrapy User-Agent now uses https link to scrapy.org (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2983">issue 2983</a>).
<strong>This is technically backward-incompatible</strong>; override
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> if you relied on old value.</p></li>
<li><p>Fix PyPy and PyPy3 test failures, support them officially
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2793">issue 2793</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2935">issue 2935</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2990">issue 2990</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3050">issue 3050</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2213">issue 2213</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3048">issue 3048</a>)</p></li>
<li><p>Fix DNS resolver when <code class="docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED=False</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2811">issue 2811</a>)</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">cryptography</span></code> for Debian Jessie tox test env (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2848">issue 2848</a>)</p></li>
<li><p>Add verification to check if Request callback is callable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2766">issue 2766</a>)</p></li>
<li><p>Port <code class="docutils literal notranslate"><span class="pre">extras/qpsclient.py</span></code> to Python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2849">issue 2849</a>)</p></li>
<li><p>Use getfullargspec under the scenes for Python 3 to stop DeprecationWarning
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2862">issue 2862</a>)</p></li>
<li><p>Update deprecated test aliases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2876">issue 2876</a>)</p></li>
<li><p>Fix <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code> support for alternate links (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2853">issue 2853</a>)</p></li>
</ul>
</section>
<section id="docs">
<h5>Docs<a class="headerlink" href="#docs" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added missing bullet point for the <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>
setting. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2756">issue 2756</a>)</p></li>
<li><p>Update Contributing docs, document new support channels
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2762">issue 2762</a>, issue:<cite>3038</cite>)</p></li>
<li><p>Include references to Scrapy subreddit in the docs</p></li>
<li><p>Fix broken links; use <code class="docutils literal notranslate"><span class="pre">https://</span></code> for external links
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2978">issue 2978</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2982">issue 2982</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2958">issue 2958</a>)</p></li>
<li><p>Document CloseSpider extension better (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2759">issue 2759</a>)</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">pymongo.collection.Collection.insert_one()</span></code> in MongoDB example
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2781">issue 2781</a>)</p></li>
<li><p>Spelling mistake and typos
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2828">issue 2828</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2837">issue 2837</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2884">issue 2884</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2924">issue 2924</a>)</p></li>
<li><p>Clarify <code class="docutils literal notranslate"><span class="pre">CSVFeedSpider.headers</span></code> documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2826">issue 2826</a>)</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">DontCloseSpider</span></code> exception and clarify <code class="docutils literal notranslate"><span class="pre">spider_idle</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2791">issue 2791</a>)</p></li>
<li><p>Update “Releases” section in README (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2764">issue 2764</a>)</p></li>
<li><p>Fix rst syntax in <code class="docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code> docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2763">issue 2763</a>)</p></li>
<li><p>Small fix in description of startproject arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2866">issue 2866</a>)</p></li>
<li><p>Clarify data types in Response.body docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2922">issue 2922</a>)</p></li>
<li><p>Add a note about <code class="docutils literal notranslate"><span class="pre">request.meta['depth']</span></code> to DepthMiddleware docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2374">issue 2374</a>)</p></li>
<li><p>Add a note about <code class="docutils literal notranslate"><span class="pre">request.meta['dont_merge_cookies']</span></code> to CookiesMiddleware
docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2999">issue 2999</a>)</p></li>
<li><p>Up-to-date example of project structure (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2964">issue 2964</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2976">issue 2976</a>)</p></li>
<li><p>A better example of ItemExporters usage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2989">issue 2989</a>)</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> methods for spider and downloader middlewares
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3019">issue 3019</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-4-0-2017-05-18">
<span id="release-1-4-0"></span><h4>Scrapy 1.4.0 (2017-05-18)<a class="headerlink" href="#scrapy-1-4-0-2017-05-18" title="Permalink to this heading">¶</a></h4>
<p>Scrapy 1.4 does not bring that many breathtaking new features
but quite a few handy improvements nonetheless.</p>
<p>Scrapy now supports anonymous FTP sessions with customizable user and
password via the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_USER</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_PASSWORD</span></code></a> settings.
And if you’re using Twisted version 17.1.0 or above, FTP is now available
with Python 3.</p>
<p>There’s a new <a class="reference internal" href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow</span></code></a> method
for creating requests; <strong>it is now a recommended way to create Requests
in Scrapy spiders</strong>. This method makes it easier to write correct
spiders; <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> has several advantages over creating
<code class="docutils literal notranslate"><span class="pre">scrapy.Request</span></code> objects directly:</p>
<ul class="simple">
<li><p>it handles relative URLs;</p></li>
<li><p>it works properly with non-ascii URLs on non-UTF8 pages;</p></li>
<li><p>in addition to absolute and relative URLs it supports Selectors;
for <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> elements it can also extract their href values.</p></li>
</ul>
<p>For example, instead of this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.page a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
<p>One can now write this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.page a&#39;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think <code class="docutils literal notranslate"><span class="pre">href=&quot;</span>&#160;&#160; <span class="pre">http://example.com&quot;</span></code>) when building
<code class="docutils literal notranslate"><span class="pre">Link</span></code> objects. This whitespace-stripping also happens for <code class="docutils literal notranslate"><span class="pre">action</span></code>
attributes with <code class="docutils literal notranslate"><span class="pre">FormRequest</span></code>.</p>
<p><strong>Please also note that link extractors do not canonicalize URLs by default
anymore.</strong> This was puzzling users every now and then, and it’s not what
browsers do in fact, so we removed that extra transformation on extracted
links.</p>
<p>For those of you wanting more control on the <code class="docutils literal notranslate"><span class="pre">Referer:</span></code> header that Scrapy
sends when following links, you can set your own <code class="docutils literal notranslate"><span class="pre">Referrer</span> <span class="pre">Policy</span></code>.
Prior to Scrapy 1.4, the default <code class="docutils literal notranslate"><span class="pre">RefererMiddleware</span></code> would simply and
blindly set it to the URL of the response that generated the HTTP request
(which could leak information on your URL seeds).
By default, Scrapy now behaves much like your regular browser does.
And this policy is fully customizable with W3C standard values
(or with something really custom of your own if you wish).
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REFERRER_POLICY</span></code></a> for details.</p>
<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by default
in 1.4: memory usage stats, detailed retry stats, detailed HTTP error code
stats. A similar change is that HTTP cache path is also visible in logs now.</p>
<p>Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom indenting
offset, using the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a> setting.</p>
<p>Enjoy! (Or read on for the rest of changes in this release.)</p>
<section id="deprecations-and-backward-incompatible-changes">
<h5>Deprecations and Backward Incompatible Changes<a class="headerlink" href="#deprecations-and-backward-incompatible-changes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Default to <code class="docutils literal notranslate"><span class="pre">canonicalize=False</span></code> in
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2537">issue 2537</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1941">issue 1941</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1982">issue 1982</a>):
<strong>warning, this is technically backward-incompatible</strong></p></li>
<li><p>Enable memusage extension by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2539">issue 2539</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2187">issue 2187</a>);
<strong>this is technically backward-incompatible</strong> so please check if you have
any non-default <code class="docutils literal notranslate"><span class="pre">MEMUSAGE_***</span></code> options set.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment variable now takes precedence over <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code>
option defined in settings.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1829">issue 1829</a>); Scrapy default settings
no longer depend on environment variables. <strong>This is technically a backward
incompatible change</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Spider.make_requests_from_url</span></code> is deprecated
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1728">issue 1728</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1495">issue 1495</a>).</p></li>
</ul>
</section>
<section id="id123">
<h5>New Features<a class="headerlink" href="#id123" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Accept proxy credentials in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> request meta key (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2526">issue 2526</a>)</p></li>
<li><p>Support <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a> content; requires optional <a class="reference external" href="https://github.com/python-hyper/brotlipy/">brotlipy</a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2535">issue 2535</a>)</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#response-follow-example"><span class="std std-ref">response.follow</span></a> shortcut
for creating requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1940">issue 1940</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">flags</span></code> argument and attribute to <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
objects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2047">issue 2047</a>)</p></li>
<li><p>Support Anonymous FTP (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2342">issue 2342</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">retry/count</span></code>, <code class="docutils literal notranslate"><span class="pre">retry/max_reached</span></code> and <code class="docutils literal notranslate"><span class="pre">retry/reason_count/&lt;reason&gt;</span></code>
stats to <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2543">issue 2543</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">httperror/response_ignored_count</span></code> and <code class="docutils literal notranslate"><span class="pre">httperror/response_ignored_status_count/&lt;status&gt;</span></code>
stats to <a class="reference internal" href="index.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2566">issue 2566</a>)</p></li>
<li><p>Customizable <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">Referrer</span> <span class="pre">policy</span></code></a> in
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.referer.RefererMiddleware" title="scrapy.spidermiddlewares.referer.RefererMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RefererMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2306">issue 2306</a>)</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">data:</span></code> URI download handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2334">issue 2334</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2156">issue 2156</a>)</p></li>
<li><p>Log cache directory when HTTP Cache is used (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2611">issue 2611</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2604">issue 2604</a>)</p></li>
<li><p>Warn users when project contains duplicate spider names (fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2181">issue 2181</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.CaselessDict</span></code> now accepts <code class="docutils literal notranslate"><span class="pre">Mapping</span></code> instances and
not only dicts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2646">issue 2646</a>)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">Media downloads</span></a>, with
<a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code></a> or
<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a>, can now optionally handle
HTTP redirects using the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEDIA_ALLOW_REDIRECTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEDIA_ALLOW_REDIRECTS</span></code></a> setting
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2616">issue 2616</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2004">issue 2004</a>)</p></li>
<li><p>Accept non-complete responses from websites using a new
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2590">issue 2590</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2586">issue 2586</a>)</p></li>
<li><p>Optional pretty-printing of JSON and XML items via
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2456">issue 2456</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1327">issue 1327</a>)</p></li>
<li><p>Allow dropping fields in <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> formdata when
<code class="docutils literal notranslate"><span class="pre">None</span></code> value is passed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/667">issue 667</a>)</p></li>
<li><p>Per-request retry times with the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2642">issue 2642</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">scrapy</span></code> as a more explicit alternative to <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2740">issue 2740</a>)</p></li>
</ul>
</section>
<section id="id124">
<h5>Bug fixes<a class="headerlink" href="#id124" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>LinkExtractor now strips leading and trailing whitespaces from attributes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2547">issue 2547</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1614">issue 1614</a>)</p></li>
<li><p>Properly handle whitespaces in action attribute in
<code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2548">issue 2548</a>)</p></li>
<li><p>Buffer CONNECT response bytes from proxy until all HTTP headers are received
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2495">issue 2495</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2491">issue 2491</a>)</p></li>
<li><p>FTP downloader now works on Python 3, provided you use Twisted&gt;=17.1
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2599">issue 2599</a>)</p></li>
<li><p>Use body to choose response type after decompressing content (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2393">issue 2393</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2145">issue 2145</a>)</p></li>
<li><p>Always decompress <code class="docutils literal notranslate"><span class="pre">Content-Encoding:</span> <span class="pre">gzip</span></code> at <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a> stage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2391">issue 2391</a>)</p></li>
<li><p>Respect custom log level in <code class="docutils literal notranslate"><span class="pre">Spider.custom_settings</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2581">issue 2581</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1612">issue 1612</a>)</p></li>
<li><p>‘make htmlview’ fix for macOS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2661">issue 2661</a>)</p></li>
<li><p>Remove “commands” from the command list  (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2695">issue 2695</a>)</p></li>
<li><p>Fix duplicate Content-Length header for POST requests with empty body (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2677">issue 2677</a>)</p></li>
<li><p>Properly cancel large downloads, i.e. above <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1616">issue 1616</a>)</p></li>
<li><p>ImagesPipeline: fixed processing of transparent PNG images with palette
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2675">issue 2675</a>)</p></li>
</ul>
</section>
<section id="cleanups-refactoring">
<h5>Cleanups &amp; Refactoring<a class="headerlink" href="#cleanups-refactoring" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Tests: remove temp files and folders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2570">issue 2570</a>),
fixed ProjectUtilsTest on macOS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2569">issue 2569</a>),
use portable pypy for Linux on Travis CI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2710">issue 2710</a>)</p></li>
<li><p>Separate building request from <code class="docutils literal notranslate"><span class="pre">_requests_to_follow</span></code> in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2562">issue 2562</a>)</p></li>
<li><p>Remove “Python 3 progress” badge (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2567">issue 2567</a>)</p></li>
<li><p>Add a couple more lines to <code class="docutils literal notranslate"><span class="pre">.gitignore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2557">issue 2557</a>)</p></li>
<li><p>Remove bumpversion prerelease configuration (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2159">issue 2159</a>)</p></li>
<li><p>Add codecov.yml file (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2750">issue 2750</a>)</p></li>
<li><p>Set context factory implementation based on Twisted version (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2577">issue 2577</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2560">issue 2560</a>)</p></li>
<li><p>Add omitted <code class="docutils literal notranslate"><span class="pre">self</span></code> arguments in default project middleware template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2595">issue 2595</a>)</p></li>
<li><p>Remove redundant <code class="docutils literal notranslate"><span class="pre">slot.add_request()</span></code> call in ExecutionEngine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2617">issue 2617</a>)</p></li>
<li><p>Catch more specific <code class="docutils literal notranslate"><span class="pre">os.error</span></code> exception in
<code class="docutils literal notranslate"><span class="pre">scrapy.pipelines.files.FSFilesStore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2644">issue 2644</a>)</p></li>
<li><p>Change “localhost” test server certificate (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2720">issue 2720</a>)</p></li>
<li><p>Remove unused <code class="docutils literal notranslate"><span class="pre">MEMUSAGE_REPORT</span></code> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2576">issue 2576</a>)</p></li>
</ul>
</section>
<section id="id125">
<h5>Documentation<a class="headerlink" href="#id125" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Binary mode is required for exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2564">issue 2564</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2553">issue 2553</a>)</p></li>
<li><p>Mention issue with <code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> due to bug in lxml (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2572">issue 2572</a>)</p></li>
<li><p>Use single quotes uniformly in templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2596">issue 2596</a>)</p></li>
<li><p>Document <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-ftp_user"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">ftp_user</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-ftp_password"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">ftp_password</span></code></a> meta keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2587">issue 2587</a>)</p></li>
<li><p>Removed section on deprecated <code class="docutils literal notranslate"><span class="pre">contrib/</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2636">issue 2636</a>)</p></li>
<li><p>Recommend Anaconda when installing Scrapy on Windows
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2477">issue 2477</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2475">issue 2475</a>)</p></li>
<li><p>FAQ: rewrite note on Python 3 support on Windows (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2690">issue 2690</a>)</p></li>
<li><p>Rearrange selector sections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2705">issue 2705</a>)</p></li>
<li><p>Remove <code class="docutils literal notranslate"><span class="pre">__nonzero__</span></code> from <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>
docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2683">issue 2683</a>)</p></li>
<li><p>Mention how to disable request filtering in documentation of
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2714">issue 2714</a>)</p></li>
<li><p>Add sphinx_rtd_theme to docs setup readme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2668">issue 2668</a>)</p></li>
<li><p>Open file in text mode in JSON item writer example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2729">issue 2729</a>)</p></li>
<li><p>Clarify <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2670">issue 2670</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-3-3-2017-03-10">
<span id="release-1-3-3"></span><h4>Scrapy 1.3.3 (2017-03-10)<a class="headerlink" href="#scrapy-1-3-3-2017-03-10" title="Permalink to this heading">¶</a></h4>
<section id="id126">
<h5>Bug fixes<a class="headerlink" href="#id126" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Make <code class="docutils literal notranslate"><span class="pre">SpiderLoader</span></code> raise <code class="docutils literal notranslate"><span class="pre">ImportError</span></code> again by default for missing
dependencies and wrong <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>.
These exceptions were silenced as warnings since 1.3.0.
A new setting is introduced to toggle between warning or exception if needed ;
see <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span></code></a> for details.</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-3-2-2017-02-13">
<span id="release-1-3-2"></span><h4>Scrapy 1.3.2 (2017-02-13)<a class="headerlink" href="#scrapy-1-3-2-2017-02-13" title="Permalink to this heading">¶</a></h4>
<section id="id127">
<h5>Bug fixes<a class="headerlink" href="#id127" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Preserve request class when converting to/from dicts (utils.reqser) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2510">issue 2510</a>).</p></li>
<li><p>Use consistent selectors for author field in tutorial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2551">issue 2551</a>).</p></li>
<li><p>Fix TLS compatibility in Twisted 17+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2558">issue 2558</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-3-1-2017-02-08">
<span id="release-1-3-1"></span><h4>Scrapy 1.3.1 (2017-02-08)<a class="headerlink" href="#scrapy-1-3-1-2017-02-08" title="Permalink to this heading">¶</a></h4>
<section id="id128">
<h5>New features<a class="headerlink" href="#id128" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Support <code class="docutils literal notranslate"><span class="pre">'True'</span></code> and <code class="docutils literal notranslate"><span class="pre">'False'</span></code> string values for boolean settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2519">issue 2519</a>);
you can now do something like <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">myspider</span> <span class="pre">-s</span> <span class="pre">REDIRECT_ENABLED=False</span></code>.</p></li>
<li><p>Support kwargs with <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code> to use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors-xpath-variables"><span class="std std-ref">XPath variables</span></a>
and ad-hoc namespaces declarations ;
this requires at least Parsel v1.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2457">issue 2457</a>).</p></li>
<li><p>Add support for Python 3.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2485">issue 2485</a>).</p></li>
<li><p>Run tests on PyPy (warning: some tests still fail, so PyPy is not supported yet).</p></li>
</ul>
</section>
<section id="id129">
<h5>Bug fixes<a class="headerlink" href="#id129" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Enforce <code class="docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2496">issue 2496</a>).</p></li>
<li><p>Fix <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-view"><code class="xref std std-command docutils literal notranslate"><span class="pre">view</span></code></a> command ; it was a regression in v1.3.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2503">issue 2503</a>).</p></li>
<li><p>Fix tests regarding <code class="docutils literal notranslate"><span class="pre">*_EXPIRES</span> <span class="pre">settings</span></code> with Files/Images pipelines (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2460">issue 2460</a>).</p></li>
<li><p>Fix name of generated pipeline class when using basic project template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2466">issue 2466</a>).</p></li>
<li><p>Fix compatibility with Twisted 17+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2496">issue 2496</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2528">issue 2528</a>).</p></li>
<li><p>Fix <code class="docutils literal notranslate"><span class="pre">scrapy.Item</span></code> inheritance on Python 3.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2511">issue 2511</a>).</p></li>
<li><p>Enforce numeric values for components order in <code class="docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code>,
<code class="docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTENSIONS</span></code> and <code class="docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2420">issue 2420</a>).</p></li>
</ul>
</section>
<section id="id130">
<h5>Documentation<a class="headerlink" href="#id130" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Reword Code of Conduct section and upgrade to Contributor Covenant v1.4
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2469">issue 2469</a>).</p></li>
<li><p>Clarify that passing spider arguments converts them to spider attributes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2483">issue 2483</a>).</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">formid</span></code> argument on <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2497">issue 2497</a>).</p></li>
<li><p>Add .rst extension to README files (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2507">issue 2507</a>).</p></li>
<li><p>Mention LevelDB cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2525">issue 2525</a>).</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">yield</span></code> in sample callback code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2533">issue 2533</a>).</p></li>
<li><p>Add note about HTML entities decoding with <code class="docutils literal notranslate"><span class="pre">.re()/.re_first()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1704">issue 1704</a>).</p></li>
<li><p>Typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2512">issue 2512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2534">issue 2534</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2531">issue 2531</a>).</p></li>
</ul>
</section>
<section id="cleanups">
<h5>Cleanups<a class="headerlink" href="#cleanups" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Remove redundant check in <code class="docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2542">issue 2542</a>).</p></li>
<li><p>Faster checks in <code class="docutils literal notranslate"><span class="pre">LinkExtractor</span></code> for allow/deny patterns (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2538">issue 2538</a>).</p></li>
<li><p>Remove dead code supporting old Twisted versions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2544">issue 2544</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-3-0-2016-12-21">
<span id="release-1-3-0"></span><h4>Scrapy 1.3.0 (2016-12-21)<a class="headerlink" href="#scrapy-1-3-0-2016-12-21" title="Permalink to this heading">¶</a></h4>
<p>This release comes rather soon after 1.2.2 for one main reason:
it was found out that releases since 0.18 up to 1.2.2 (included) use
some backported code from Twisted (<code class="docutils literal notranslate"><span class="pre">scrapy.xlib.tx.*</span></code>),
even if newer Twisted modules are available.
Scrapy now uses <code class="docutils literal notranslate"><span class="pre">twisted.web.client</span></code> and <code class="docutils literal notranslate"><span class="pre">twisted.internet.endpoints</span></code> directly.
(See also cleanups below.)</p>
<p>As it is a major change, we wanted to get the bug fix out quickly
while not breaking any projects using the 1.2 series.</p>
<section id="id131">
<h5>New Features<a class="headerlink" href="#id131" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MailSender</span></code> now accepts single strings as values for <code class="docutils literal notranslate"><span class="pre">to</span></code> and <code class="docutils literal notranslate"><span class="pre">cc</span></code>
arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2272">issue 2272</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">url</span></code>, <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">url</span></code> and <code class="docutils literal notranslate"><span class="pre">fetch(url)</span></code> inside
Scrapy shell now follow HTTP redirections by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2290">issue 2290</a>);
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> for details.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code> now logs errors with <code class="docutils literal notranslate"><span class="pre">INFO</span></code> level instead of <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code>;
this is technically <strong>backward incompatible</strong> so please check your log parsers.</p></li>
<li><p>By default, logger names now use a long-form path, e.g. <code class="docutils literal notranslate"><span class="pre">[scrapy.extensions.logstats]</span></code>,
instead of the shorter “top-level” variant of prior releases (e.g. <code class="docutils literal notranslate"><span class="pre">[scrapy]</span></code>);
this is <strong>backward incompatible</strong> if you have log parsers expecting the short
logger name part. You can switch back to short logger names using <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a>
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</section>
<section id="dependencies-cleanups">
<h5>Dependencies &amp; Cleanups<a class="headerlink" href="#dependencies-cleanups" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Scrapy now requires Twisted &gt;= 13.1 which is the case for many Linux
distributions already.</p></li>
<li><p>As a consequence, we got rid of <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.tx.*</span></code> modules, which
copied some of Twisted code for users stuck with an “old” Twisted version</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ChunkedTransferMiddleware</span></code> is deprecated and removed from the default
downloader middlewares.</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-2-3-2017-03-03">
<span id="release-1-2-3"></span><h4>Scrapy 1.2.3 (2017-03-03)<a class="headerlink" href="#scrapy-1-2-3-2017-03-03" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Packaging fix: disallow unsupported Twisted versions in setup.py</p></li>
</ul>
</section>
<section id="scrapy-1-2-2-2016-12-06">
<span id="release-1-2-2"></span><h4>Scrapy 1.2.2 (2016-12-06)<a class="headerlink" href="#scrapy-1-2-2-2016-12-06" title="Permalink to this heading">¶</a></h4>
<section id="id132">
<h5>Bug fixes<a class="headerlink" href="#id132" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fix a cryptic traceback when a pipeline fails on <code class="docutils literal notranslate"><span class="pre">open_spider()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2011">issue 2011</a>)</p></li>
<li><p>Fix embedded IPython shell variables (fixing <a class="reference external" href="https://github.com/scrapy/scrapy/issues/396">issue 396</a> that re-appeared
in 1.2.0, fixed in <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2418">issue 2418</a>)</p></li>
<li><p>A couple of patches when dealing with robots.txt:</p>
<ul>
<li><p>handle (non-standard) relative sitemap URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2390">issue 2390</a>)</p></li>
<li><p>handle non-ASCII URLs and User-Agents in Python 2 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2373">issue 2373</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="id133">
<h5>Documentation<a class="headerlink" href="#id133" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Document <code class="docutils literal notranslate"><span class="pre">&quot;download_latency&quot;</span></code> key in <code class="docutils literal notranslate"><span class="pre">Request</span></code>’s <code class="docutils literal notranslate"><span class="pre">meta</span></code> dict (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2033">issue 2033</a>)</p></li>
<li><p>Remove page on (deprecated &amp; unsupported) Ubuntu packages from ToC (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2335">issue 2335</a>)</p></li>
<li><p>A few fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2346">issue 2346</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2369">issue 2369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2369">issue 2369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2380">issue 2380</a>)
and clarifications (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2354">issue 2354</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2325">issue 2325</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2414">issue 2414</a>)</p></li>
</ul>
</section>
<section id="id134">
<h5>Other changes<a class="headerlink" href="#id134" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Advertize <a class="reference external" href="https://anaconda.org/conda-forge/scrapy">conda-forge</a> as Scrapy’s official conda channel (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2387">issue 2387</a>)</p></li>
<li><p>More helpful error messages when trying to use <code class="docutils literal notranslate"><span class="pre">.css()</span></code> or <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code>
on non-Text Responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2264">issue 2264</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">startproject</span></code> command now generates a sample <code class="docutils literal notranslate"><span class="pre">middlewares.py</span></code> file (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2335">issue 2335</a>)</p></li>
<li><p>Add more dependencies’ version info in <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code> verbose output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2404">issue 2404</a>)</p></li>
<li><p>Remove all <code class="docutils literal notranslate"><span class="pre">*.pyc</span></code> files from source distribution (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2386">issue 2386</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-2-1-2016-10-21">
<span id="release-1-2-1"></span><h4>Scrapy 1.2.1 (2016-10-21)<a class="headerlink" href="#scrapy-1-2-1-2016-10-21" title="Permalink to this heading">¶</a></h4>
<section id="id135">
<h5>Bug fixes<a class="headerlink" href="#id135" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Include OpenSSL’s more permissive default ciphers when establishing
TLS/SSL connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2314">issue 2314</a>).</p></li>
<li><p>Fix “Location” HTTP header decoding on non-ASCII URL redirects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2321">issue 2321</a>).</p></li>
</ul>
</section>
<section id="id136">
<h5>Documentation<a class="headerlink" href="#id136" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fix JsonWriterPipeline example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2302">issue 2302</a>).</p></li>
<li><p>Various notes: <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2330">issue 2330</a> on spider names,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2329">issue 2329</a> on middleware methods processing order,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2327">issue 2327</a> on getting multi-valued HTTP headers as lists.</p></li>
</ul>
</section>
<section id="id137">
<h5>Other changes<a class="headerlink" href="#id137" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">www.</span></code> from <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> in built-in spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2299">issue 2299</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-2-0-2016-10-03">
<span id="release-1-2-0"></span><h4>Scrapy 1.2.0 (2016-10-03)<a class="headerlink" href="#scrapy-1-2-0-2016-10-03" title="Permalink to this heading">¶</a></h4>
<section id="id138">
<h5>New Features<a class="headerlink" href="#id138" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a> setting to customize the encoding
used when writing items to a file.
This can be used to turn off <code class="docutils literal notranslate"><span class="pre">\uXXXX</span></code> escapes in JSON output.
This is also useful for those wanting something else than UTF-8
for XML or CSV output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2034">issue 2034</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">startproject</span></code> command now supports an optional destination directory
to override the default one based on the project name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2005">issue 2005</a>).</p></li>
<li><p>New <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting to log requests serialization
failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1610">issue 1610</a>).</p></li>
<li><p>JSON encoder now supports serialization of <code class="docutils literal notranslate"><span class="pre">set</span></code> instances (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2058">issue 2058</a>).</p></li>
<li><p>Interpret <code class="docutils literal notranslate"><span class="pre">application/json-amazonui-streaming</span></code> as <code class="docutils literal notranslate"><span class="pre">TextResponse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1503">issue 1503</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy</span></code> is imported by default when using shell tools (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a>,
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">inspect_response</span></a>) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2248">issue 2248</a>).</p></li>
</ul>
</section>
<section id="id139">
<h5>Bug fixes<a class="headerlink" href="#id139" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>DefaultRequestHeaders middleware now runs before UserAgent middleware
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2088">issue 2088</a>). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</p></li>
<li><p>HTTP cache extension and plugins that use the <code class="docutils literal notranslate"><span class="pre">.scrapy</span></code> data directory now
work outside projects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1581">issue 1581</a>).  <strong>Warning: this is technically
backward incompatible</strong>, though we consider this a bug fix.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Selector</span></code> does not allow passing both <code class="docutils literal notranslate"><span class="pre">response</span></code> and <code class="docutils literal notranslate"><span class="pre">text</span></code> anymore
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2153">issue 2153</a>).</p></li>
<li><p>Fixed logging of wrong callback name with <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2169">issue 2169</a>).</p></li>
<li><p>Fix for an odd gzip decompression bug (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1606">issue 1606</a>).</p></li>
<li><p>Fix for selected callbacks when using <code class="docutils literal notranslate"><span class="pre">CrawlSpider</span></code> with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2225">issue 2225</a>).</p></li>
<li><p>Fix for invalid JSON and XML files when spider yields no items (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>).</p></li>
<li><p>Implement <code class="docutils literal notranslate"><span class="pre">flush()</span></code> for <code class="docutils literal notranslate"><span class="pre">StreamLogger</span></code> avoiding a warning in logs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2125">issue 2125</a>).</p></li>
</ul>
</section>
<section id="refactoring">
<h5>Refactoring<a class="headerlink" href="#refactoring" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">canonicalize_url</span></code> has been moved to <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url">w3lib.url</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2168">issue 2168</a>).</p></li>
</ul>
</section>
<section id="tests-requirements">
<h5>Tests &amp; Requirements<a class="headerlink" href="#tests-requirements" title="Permalink to this heading">¶</a></h5>
<p>Scrapy’s new requirements baseline is Debian 8 “Jessie”. It was previously
Ubuntu 12.04 Precise.
What this means in practice is that we run continuous integration tests
with these (main) packages versions at a minimum:
Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>
<p>Scrapy may very well work with older versions of these packages
(the code base still has switches for older Twisted versions for example)
but it is not guaranteed (because it’s not tested anymore).</p>
</section>
<section id="id140">
<h5>Documentation<a class="headerlink" href="#id140" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Grammar fixes: <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2128">issue 2128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1566">issue 1566</a>.</p></li>
<li><p>Download stats badge removed from README (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2160">issue 2160</a>).</p></li>
<li><p>New Scrapy <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-architecture"><span class="std std-ref">architecture diagram</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2165">issue 2165</a>).</p></li>
<li><p>Updated <code class="docutils literal notranslate"><span class="pre">Response</span></code> parameters documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2197">issue 2197</a>).</p></li>
<li><p>Reworded misleading <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a> description (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2190">issue 2190</a>).</p></li>
<li><p>Add StackOverflow as a support channel (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2257">issue 2257</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-1-4-2017-03-03">
<span id="release-1-1-4"></span><h4>Scrapy 1.1.4 (2017-03-03)<a class="headerlink" href="#scrapy-1-1-4-2017-03-03" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Packaging fix: disallow unsupported Twisted versions in setup.py</p></li>
</ul>
</section>
<section id="scrapy-1-1-3-2016-09-22">
<span id="release-1-1-3"></span><h4>Scrapy 1.1.3 (2016-09-22)<a class="headerlink" href="#scrapy-1-1-3-2016-09-22" title="Permalink to this heading">¶</a></h4>
<section id="id141">
<h5>Bug fixes<a class="headerlink" href="#id141" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Class attributes for subclasses of <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code>
work as they did before 1.1.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2243">issue 2243</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2198">issue 2198</a>)</p></li>
</ul>
</section>
<section id="id142">
<h5>Documentation<a class="headerlink" href="#id142" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-overview"><span class="std std-ref">Overview</span></a> and <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>
rewritten to use <a class="reference external" href="http://toscrape.com">http://toscrape.com</a> websites
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2236">issue 2236</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2249">issue 2249</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2252">issue 2252</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-1-2-2016-08-18">
<span id="release-1-1-2"></span><h4>Scrapy 1.1.2 (2016-08-18)<a class="headerlink" href="#scrapy-1-1-2-2016-08-18" title="Permalink to this heading">¶</a></h4>
<section id="id143">
<h5>Bug fixes<a class="headerlink" href="#id143" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Introduce a missing <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-IMAGES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_S3_ACL</span></code></a> setting to override
the default ACL policy in <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> when uploading images to S3
(note that default ACL policy is “private” – instead of “public-read” –
since Scrapy 1.1.0)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_EXPIRES</span></code></a> default value set back to 90
(the regression was introduced in 1.1.1)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-1-1-2016-07-13">
<span id="release-1-1-1"></span><h4>Scrapy 1.1.1 (2016-07-13)<a class="headerlink" href="#scrapy-1-1-1-2016-07-13" title="Permalink to this heading">¶</a></h4>
<section id="id144">
<h5>Bug fixes<a class="headerlink" href="#id144" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Add “Host” header in CONNECT requests to HTTPS proxies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2069">issue 2069</a>)</p></li>
<li><p>Use response <code class="docutils literal notranslate"><span class="pre">body</span></code> when choosing response class
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2001">issue 2001</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2000">issue 2000</a>)</p></li>
<li><p>Do not fail on canonicalizing URLs with wrong netlocs
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2038">issue 2038</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2010">issue 2010</a>)</p></li>
<li><p>a few fixes for <code class="docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code> (and <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code>):</p>
<ul>
<li><p>Do not decode HEAD responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2008">issue 2008</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1899">issue 1899</a>)</p></li>
<li><p>Handle charset parameter in gzip Content-Type header
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2050">issue 2050</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2049">issue 2049</a>)</p></li>
<li><p>Do not decompress gzip octet-stream responses
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2065">issue 2065</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2063">issue 2063</a>)</p></li>
</ul>
</li>
<li><p>Catch (and ignore with a warning) exception when verifying certificate
against IP-address hosts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2094">issue 2094</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2092">issue 2092</a>)</p></li>
<li><p>Make <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> backward compatible again
regarding the use of legacy class attributes for customization
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1989">issue 1989</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1985">issue 1985</a>)</p></li>
</ul>
</section>
<section id="id145">
<h5>New features<a class="headerlink" href="#id145" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Enable genspider command outside project folder (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2052">issue 2052</a>)</p></li>
<li><p>Retry HTTPS CONNECT <code class="docutils literal notranslate"><span class="pre">TunnelError</span></code> by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1974">issue 1974</a>)</p></li>
</ul>
</section>
<section id="id146">
<h5>Documentation<a class="headerlink" href="#id146" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">FEED_TEMPDIR</span></code> setting at lexicographical position (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9b3c72c">commit 9b3c72c</a>)</p></li>
<li><p>Use idiomatic <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> in overview (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1994">issue 1994</a>)</p></li>
<li><p>Update years in copyright notice (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c2c8036">commit c2c8036</a>)</p></li>
<li><p>Add information and example on errbacks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1995">issue 1995</a>)</p></li>
<li><p>Use “url” variable in downloader middleware example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2015">issue 2015</a>)</p></li>
<li><p>Grammar fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2054">issue 2054</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2120">issue 2120</a>)</p></li>
<li><p>New FAQ entry on using BeautifulSoup in spider callbacks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2048">issue 2048</a>)</p></li>
<li><p>Add notes about Scrapy not working on Windows with Python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2060">issue 2060</a>)</p></li>
<li><p>Encourage complete titles in pull requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2026">issue 2026</a>)</p></li>
</ul>
</section>
<section id="tests">
<h5>Tests<a class="headerlink" href="#tests" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2095">issue 2095</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-1-0-2016-05-11">
<span id="release-1-1-0"></span><h4>Scrapy 1.1.0 (2016-05-11)<a class="headerlink" href="#scrapy-1-1-0-2016-05-11" title="Permalink to this heading">¶</a></h4>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<ul class="simple">
<li><p>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5). See
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#news-betapy3"><span class="std std-ref">Beta Python 3 Support</span></a> for more details and some limitations.</p></li>
<li><p>Hot new features:</p>
<ul>
<li><p>Item loaders now support nested loaders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1467">issue 1467</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1382">issue 1382</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1137">issue 1137</a>).</p></li>
<li><p>Added setting <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> and improved
AutoThrottle docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1324">issue 1324</a>).</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">response.text</span></code> to get body as unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1730">issue 1730</a>).</p></li>
<li><p>Anonymous S3 connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1358">issue 1358</a>).</p></li>
<li><p>Deferreds in downloader middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1473">issue 1473</a>). This enables better
robots.txt handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1471">issue 1471</a>).</p></li>
<li><p>HTTP caching now follows RFC2616 more closely, added settings
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_ALWAYS_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_ALWAYS_STORE</span></code></a> and
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1151">issue 1151</a>).</p></li>
<li><p>Selectors were extracted to the <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a> library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1409">issue 1409</a>). This means
you can use Scrapy Selectors without Scrapy and also upgrade the
selectors engine without needing to upgrade Scrapy.</p></li>
<li><p>HTTPS downloader now does TLS protocol negotiation by default,
instead of forcing TLS 1.0. You can also set the SSL/TLS method
using the new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>.</p></li>
</ul>
</li>
<li><p>These bug fixes may require your attention:</p>
<ul>
<li><p>Don’t retry bad requests (HTTP 400) by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>).
If you need the old behavior, add <code class="docutils literal notranslate"><span class="pre">400</span></code> to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a>.</p></li>
<li><p>Fix shell files argument handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).
If you try <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> it will try to load the URL <code class="docutils literal notranslate"><span class="pre">http://index.html</span></code>,
use <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">./index.html</span></code> to load a local file.</p></li>
<li><p>Robots.txt compliance is now enabled by default for newly-created projects
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1724">issue 1724</a>). Scrapy will also wait for robots.txt to be downloaded
before proceeding with the crawl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1735">issue 1735</a>). If you want to disable
this behavior, update <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> in <code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file
after creating a new project.</p></li>
<li><p>Exporters now work on unicode, instead of bytes by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1080">issue 1080</a>).
If you use <a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a>, you may want to
update your code to disable binary mode which is now deprecated.</p></li>
<li><p>Accept XML node names containing dots as valid (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1533">issue 1533</a>).</p></li>
<li><p>When uploading files or images to S3 (with <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> or
<code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code>), the default ACL policy is now “private” instead
of “public” <strong>Warning: backward incompatible!</strong>.
You can use <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_S3_ACL</span></code></a> to change it.</p></li>
<li><p>We’ve reimplemented <code class="docutils literal notranslate"><span class="pre">canonicalize_url()</span></code> for more correct output,
especially for URLs with non-ASCII characters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1947">issue 1947</a>).
This could change link extractors output compared to previous Scrapy versions.
This may also invalidate some cache entries you could still have from pre-1.1 runs.
<strong>Warning: backward incompatible!</strong>.</p></li>
</ul>
</li>
</ul>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<section id="beta-python-3-support">
<span id="news-betapy3"></span><h5>Beta Python 3 Support<a class="headerlink" href="#beta-python-3-support" title="Permalink to this heading">¶</a></h5>
<p>We have been <a class="reference external" href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python 3</a>. As a result, now
you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted &gt;= 15.5 required). Some
features are still missing (and some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work.
However, we are aware of some limitations in Python 3:</p>
<ul class="simple">
<li><p>Scrapy does not work on Windows with Python 3</p></li>
<li><p>Sending emails is not supported</p></li>
<li><p>FTP download handler is not supported</p></li>
<li><p>Telnet console is not supported</p></li>
</ul>
</section>
<section id="additional-new-features-and-enhancements">
<h5>Additional New Features and Enhancements<a class="headerlink" href="#additional-new-features-and-enhancements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Scrapy now has a <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md">Code of Conduct</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1681">issue 1681</a>).</p></li>
<li><p>Command line tool now has completion for zsh (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/934">issue 934</a>).</p></li>
<li><p>Improvements to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span></code>:</p>
<ul>
<li><p>Support for bpython and configure preferred Python shell via
<code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1100">issue 1100</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1444">issue 1444</a>).</p></li>
<li><p>Support URLs without scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1498">issue 1498</a>)
<strong>Warning: backward incompatible!</strong></p></li>
<li><p>Bring back support for relative file path (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).</p></li>
</ul>
</li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a> setting to change default check
interval (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1282">issue 1282</a>).</p></li>
<li><p>Download handlers are now lazy-loaded on first request using their
scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1390">issue 1390</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1421">issue 1421</a>).</p></li>
<li><p>HTTPS download handlers do not force TLS 1.0 anymore; instead,
OpenSSL’s <code class="docutils literal notranslate"><span class="pre">SSLv23_method()/TLS_method()</span></code> is used allowing to try
negotiating with the remote hosts the highest TLS protocol version
it can (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1794">issue 1794</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1629">issue 1629</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code> now skips the status codes from
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> on spider attribute
or in <code class="docutils literal notranslate"><span class="pre">Request</span></code>’s <code class="docutils literal notranslate"><span class="pre">meta</span></code> key (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1334">issue 1334</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1364">issue 1364</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1447">issue 1447</a>).</p></li>
<li><p>Form submission:</p>
<ul>
<li><p>now works with <code class="docutils literal notranslate"><span class="pre">&lt;button&gt;</span></code> elements too (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1469">issue 1469</a>).</p></li>
<li><p>an empty string is now used for submit buttons without a value
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1472">issue 1472</a>)</p></li>
</ul>
</li>
<li><p>Dict-like settings now have per-key priorities
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1135">issue 1135</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1149">issue 1149</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1586">issue 1586</a>).</p></li>
<li><p>Sending non-ASCII emails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1662">issue 1662</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CloseSpider</span></code> and <code class="docutils literal notranslate"><span class="pre">SpiderState</span></code> extensions now get disabled if no relevant
setting is set (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1723">issue 1723</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1725">issue 1725</a>).</p></li>
<li><p>Added method <code class="docutils literal notranslate"><span class="pre">ExecutionEngine.close</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1423">issue 1423</a>).</p></li>
<li><p>Added method <code class="docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1528">issue 1528</a>).</p></li>
<li><p>Scheduler priority queue can now be customized via
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1822">issue 1822</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.pps</span></code> links are now ignored by default in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1835">issue 1835</a>).</p></li>
<li><p>temporary data folder for FTP and S3 feed storages can be customized
using a new <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-FEED_TEMPDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_TEMPDIR</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1847">issue 1847</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> settings are now instance attributes
instead of class attributes, enabling spider-specific behaviors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1891">issue 1891</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">JsonItemExporter</span></code> now formats opening and closing square brackets
on their own line (first and last lines of output file) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1950">issue 1950</a>).</p></li>
<li><p>If available, <code class="docutils literal notranslate"><span class="pre">botocore</span></code> is used for <code class="docutils literal notranslate"><span class="pre">S3FeedStorage</span></code>, <code class="docutils literal notranslate"><span class="pre">S3DownloadHandler</span></code>
and <code class="docutils literal notranslate"><span class="pre">S3FilesStore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1761">issue 1761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1883">issue 1883</a>).</p></li>
<li><p>Tons of documentation updates and related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1291">issue 1291</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1302">issue 1302</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1335">issue 1335</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1683">issue 1683</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1660">issue 1660</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1642">issue 1642</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1721">issue 1721</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1727">issue 1727</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1879">issue 1879</a>).</p></li>
<li><p>Other refactoring, optimizations and cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1476">issue 1476</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1481">issue 1481</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1477">issue 1477</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1315">issue 1315</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1290">issue 1290</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1750">issue 1750</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1881">issue 1881</a>).</p></li>
</ul>
</section>
<section id="deprecations-and-removals">
<h5>Deprecations and Removals<a class="headerlink" href="#deprecations-and-removals" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added <code class="docutils literal notranslate"><span class="pre">to_bytes</span></code> and <code class="docutils literal notranslate"><span class="pre">to_unicode</span></code>, deprecated <code class="docutils literal notranslate"><span class="pre">str_to_unicode</span></code> and
<code class="docutils literal notranslate"><span class="pre">unicode_to_str</span></code> functions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/778">issue 778</a>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">binary_is_text</span></code> is introduced, to replace use of <code class="docutils literal notranslate"><span class="pre">isbinarytext</span></code>
(but with inverse return value) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1851">issue 1851</a>)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">optional_features</span></code> set has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1359">issue 1359</a>).</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">--lsprof</span></code> command line option has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1689">issue 1689</a>).
<strong>Warning: backward incompatible</strong>, but doesn’t break user code.</p></li>
<li><p>The following datatypes were deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1720">issue 1720</a>):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MultiValueDictKeyError</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MultiValueDict</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.SiteNode</span></code></p></li>
</ul>
</li>
<li><p>The previously bundled <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.pydispatch</span></code> library was deprecated and
replaced by <a class="reference external" href="https://pypi.org/project/PyDispatcher/">pydispatcher</a>.</p></li>
</ul>
</section>
<section id="relocations">
<h5>Relocations<a class="headerlink" href="#relocations" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">telnetconsole</span></code> was relocated to <code class="docutils literal notranslate"><span class="pre">extensions/</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1524">issue 1524</a>).</p>
<ul>
<li><p>Note: telnet is not enabled on Python 3
(<a class="reference external" href="https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>)</p></li>
</ul>
</li>
</ul>
</section>
<section id="bugfixes">
<h5>Bugfixes<a class="headerlink" href="#bugfixes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Scrapy does not retry requests that got a <code class="docutils literal notranslate"><span class="pre">HTTP</span> <span class="pre">400</span> <span class="pre">Bad</span> <span class="pre">Request</span></code>
response anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>). <strong>Warning: backward incompatible!</strong></p></li>
<li><p>Support empty password for http_proxy config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1274">issue 1274</a>).</p></li>
<li><p>Interpret <code class="docutils literal notranslate"><span class="pre">application/x-json</span></code> as <code class="docutils literal notranslate"><span class="pre">TextResponse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1333">issue 1333</a>).</p></li>
<li><p>Support link rel attribute with multiple values (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1201">issue 1201</a>).</p></li>
<li><p>Fixed <code class="docutils literal notranslate"><span class="pre">scrapy.http.FormRequest.from_response</span></code> when there is a <code class="docutils literal notranslate"><span class="pre">&lt;base&gt;</span></code>
tag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1564">issue 1564</a>).</p></li>
<li><p>Fixed <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-TEMPLATES_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TEMPLATES_DIR</span></code></a> handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1575">issue 1575</a>).</p></li>
<li><p>Various <code class="docutils literal notranslate"><span class="pre">FormRequest</span></code> fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1595">issue 1595</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1596">issue 1596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1597">issue 1597</a>).</p></li>
<li><p>Makes <code class="docutils literal notranslate"><span class="pre">_monkeypatches</span></code> more robust (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1634">issue 1634</a>).</p></li>
<li><p>Fixed bug on <code class="docutils literal notranslate"><span class="pre">XMLItemExporter</span></code> with non-string fields in
items (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1738">issue 1738</a>).</p></li>
<li><p>Fixed startproject command in macOS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1635">issue 1635</a>).</p></li>
<li><p>Fixed <a class="reference internal" href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonItemExporter</span></code></a> and CSVExporter for
non-string item types (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1737">issue 1737</a>).</p></li>
<li><p>Various logging related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1294">issue 1294</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1419">issue 1419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1263">issue 1263</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1624">issue 1624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1654">issue 1654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1722">issue 1722</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1726">issue 1726</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1303">issue 1303</a>).</p></li>
<li><p>Fixed bug in <code class="docutils literal notranslate"><span class="pre">utils.template.render_templatefile()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1212">issue 1212</a>).</p></li>
<li><p>sitemaps extraction from <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> is now case-insensitive (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1902">issue 1902</a>).</p></li>
<li><p>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
to same remote host (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1912">issue 1912</a>).</p></li>
</ul>
</section>
</section>
<section id="scrapy-1-0-7-2017-03-03">
<span id="release-1-0-7"></span><h4>Scrapy 1.0.7 (2017-03-03)<a class="headerlink" href="#scrapy-1-0-7-2017-03-03" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Packaging fix: disallow unsupported Twisted versions in setup.py</p></li>
</ul>
</section>
<section id="scrapy-1-0-6-2016-05-04">
<span id="release-1-0-6"></span><h4>Scrapy 1.0.6 (2016-05-04)<a class="headerlink" href="#scrapy-1-0-6-2016-05-04" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>FIX: RetryMiddleware is now robust to non-standard HTTP status codes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1857">issue 1857</a>)</p></li>
<li><p>FIX: Filestorage HTTP cache was checking wrong modified time (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1875">issue 1875</a>)</p></li>
<li><p>DOC: Support for Sphinx 1.4+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1893">issue 1893</a>)</p></li>
<li><p>DOC: Consistency in selectors examples (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1869">issue 1869</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-5-2016-02-04">
<span id="release-1-0-5"></span><h4>Scrapy 1.0.5 (2016-02-04)<a class="headerlink" href="#scrapy-1-0-5-2016-02-04" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/907">issue 907</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/108195e">commit 108195e</a>)</p></li>
<li><p>TST: Changed buildbot makefile to use ‘pytest’ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1f3d90a">commit 1f3d90a</a>)</p></li>
<li><p>DOC: Fixed typos in tutorial and media-pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/808a9ea">commit 808a9ea</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/commit/803bd87">commit 803bd87</a>)</p></li>
<li><p>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in settings docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa94121">commit aa94121</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-4-2015-12-30">
<span id="release-1-0-4"></span><h4>Scrapy 1.0.4 (2015-12-30)<a class="headerlink" href="#scrapy-1-0-4-2015-12-30" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Ignoring xlib/tx folder, depending on Twisted version. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7dfa979">commit 7dfa979</a>)</p></li>
<li><p>Run on new travis-ci infra (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6e42f0b">commit 6e42f0b</a>)</p></li>
<li><p>Spelling fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/823a1cc">commit 823a1cc</a>)</p></li>
<li><p>escape nodename in xmliter regex (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/da3c155">commit da3c155</a>)</p></li>
<li><p>test xml nodename with dots (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4418fc3">commit 4418fc3</a>)</p></li>
<li><p>TST don’t use broken Pillow version in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a55078c">commit a55078c</a>)</p></li>
<li><p>disable log on version command. closes #1426 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86fc330">commit 86fc330</a>)</p></li>
<li><p>disable log on startproject command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db4c9fe">commit db4c9fe</a>)</p></li>
<li><p>Add PyPI download stats badge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/df2b944">commit df2b944</a>)</p></li>
<li><p>don’t run tests twice on Travis if a PR is made from a scrapy/scrapy branch (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a83ab41">commit a83ab41</a>)</p></li>
<li><p>Add Python 3 porting status badge to the README (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/73ac80d">commit 73ac80d</a>)</p></li>
<li><p>fixed RFPDupeFilter persistence (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97d080e">commit 97d080e</a>)</p></li>
<li><p>TST a test to show that dupefilter persistence is not working (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97f2fb3">commit 97f2fb3</a>)</p></li>
<li><p>explicit close file on <a class="reference external" href="file://">file://</a> scheme handler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d9b4850">commit d9b4850</a>)</p></li>
<li><p>Disable dupefilter in shell (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c0d0734">commit c0d0734</a>)</p></li>
<li><p>DOC: Add captions to toctrees which appear in sidebar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa239ad">commit aa239ad</a>)</p></li>
<li><p>DOC Removed pywin32 from install instructions as it’s already declared as dependency. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10eb400">commit 10eb400</a>)</p></li>
<li><p>Added installation notes about using Conda for Windows and other OSes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1c3600a">commit 1c3600a</a>)</p></li>
<li><p>Fixed minor grammar issues. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7f4ddd5">commit 7f4ddd5</a>)</p></li>
<li><p>fixed a typo in the documentation. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b71f677">commit b71f677</a>)</p></li>
<li><p>Version 1 now exists (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5456c0e">commit 5456c0e</a>)</p></li>
<li><p>fix another invalid xpath error (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a1366e">commit 0a1366e</a>)</p></li>
<li><p>fix ValueError: Invalid XPath: //div/[id=”not-exists”]/text() on selectors.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ca8d60f">commit ca8d60f</a>)</p></li>
<li><p>Typos corrections (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7067117">commit 7067117</a>)</p></li>
<li><p>fix typos in downloader-middleware.rst and exceptions.rst, middlware -&gt; middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/32f115c">commit 32f115c</a>)</p></li>
<li><p>Add note to Ubuntu install section about Debian compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/23fda69">commit 23fda69</a>)</p></li>
<li><p>Replace alternative macOS install workaround with virtualenv (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98b63ee">commit 98b63ee</a>)</p></li>
<li><p>Reference Homebrew’s homepage for installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1925db1">commit 1925db1</a>)</p></li>
<li><p>Add oldest supported tox version to contributing docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d10d6d">commit 5d10d6d</a>)</p></li>
<li><p>Note in install docs about pip being already included in python&gt;=2.7.9 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/85c980e">commit 85c980e</a>)</p></li>
<li><p>Add non-python dependencies to Ubuntu install section in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbd010d">commit fbd010d</a>)</p></li>
<li><p>Add macOS installation section to docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8f4cba">commit d8f4cba</a>)</p></li>
<li><p>DOC(ENH): specify path to rtd theme explicitly (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de73b1a">commit de73b1a</a>)</p></li>
<li><p>minor: scrapy.Spider docs grammar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ddcc7b">commit 1ddcc7b</a>)</p></li>
<li><p>Make common practices sample code match the comments (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1b85bcf">commit 1b85bcf</a>)</p></li>
<li><p>nextcall repetitive calls (heartbeats). (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/55f7104">commit 55f7104</a>)</p></li>
<li><p>Backport fix compatibility with Twisted 15.4.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b262411">commit b262411</a>)</p></li>
<li><p>pin pytest to 2.7.3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a6535c2">commit a6535c2</a>)</p></li>
<li><p>Merge pull request #1512 from mgedmin/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8876111">commit 8876111</a>)</p></li>
<li><p>Merge pull request #1513 from mgedmin/patch-2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d4daf8">commit 5d4daf8</a>)</p></li>
<li><p>Typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d0682">commit f8d0682</a>)</p></li>
<li><p>Fix list formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f83a93">commit 5f83a93</a>)</p></li>
<li><p>fix Scrapy squeue tests after recent changes to queuelib (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3365c01">commit 3365c01</a>)</p></li>
<li><p>Merge pull request #1475 from rweindl/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d688cd">commit 2d688cd</a>)</p></li>
<li><p>Update tutorial.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbc1f25">commit fbc1f25</a>)</p></li>
<li><p>Merge pull request #1449 from rhoekman/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7d6538c">commit 7d6538c</a>)</p></li>
<li><p>Small grammatical change (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8752294">commit 8752294</a>)</p></li>
<li><p>Add openssl version to version command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c45ac">commit 13c45ac</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-3-2015-08-11">
<span id="release-1-0-3"></span><h4>Scrapy 1.0.3 (2015-08-11)<a class="headerlink" href="#scrapy-1-0-3-2015-08-11" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>add service_identity to Scrapy install_requires (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cbc2501">commit cbc2501</a>)</p></li>
<li><p>Workaround for travis#296 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/66af9cd">commit 66af9cd</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-2-2015-08-06">
<span id="release-1-0-2"></span><h4>Scrapy 1.0.2 (2015-08-06)<a class="headerlink" href="#scrapy-1-0-2-2015-08-06" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Twisted 15.3.0 does not raises PicklingError serializing lambda functions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b04dd7d">commit b04dd7d</a>)</p></li>
<li><p>Minor method name fix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f85c7f">commit 6f85c7f</a>)</p></li>
<li><p>minor: scrapy.Spider grammar and clarity (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9c9d2e0">commit 9c9d2e0</a>)</p></li>
<li><p>Put a blurb about support channels in CONTRIBUTING (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c63882b">commit c63882b</a>)</p></li>
<li><p>Fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a9ae7b0">commit a9ae7b0</a>)</p></li>
<li><p>Fix doc reference. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7c8a4fe">commit 7c8a4fe</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-1-2015-07-01">
<span id="release-1-0-1"></span><h4>Scrapy 1.0.1 (2015-07-01)<a class="headerlink" href="#scrapy-1-0-1-2015-07-01" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Unquote request path before passing to FTPClient, it already escape paths (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc00ad2">commit cc00ad2</a>)</p></li>
<li><p>include tests/ to source distribution in MANIFEST.in (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eca227e">commit eca227e</a>)</p></li>
<li><p>DOC Fix SelectJmes documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b8567bc">commit b8567bc</a>)</p></li>
<li><p>DOC Bring Ubuntu and Archlinux outside of Windows subsection (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/392233f">commit 392233f</a>)</p></li>
<li><p>DOC remove version suffix from Ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5303c66">commit 5303c66</a>)</p></li>
<li><p>DOC Update release date for 1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c89fa29">commit c89fa29</a>)</p></li>
</ul>
</section>
<section id="scrapy-1-0-0-2015-06-19">
<span id="release-1-0-0"></span><h4>Scrapy 1.0.0 (2015-06-19)<a class="headerlink" href="#scrapy-1-0-0-2015-06-19" title="Permalink to this heading">¶</a></h4>
<p>You will find a lot of new features and bugfixes in this major release.  Make
sure to check our updated <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-overview"><span class="std std-ref">overview</span></a> to get a glance of
some of the changes, along with our brushed <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>.</p>
<section id="support-for-returning-dictionaries-in-spiders">
<h5>Support for returning dictionaries in spiders<a class="headerlink" href="#support-for-returning-dictionaries-in-spiders" title="Permalink to this heading">¶</a></h5>
<p>Declaring and returning Scrapy Items is no longer necessary to collect the
scraped data from your spider, you can now return explicit dictionaries
instead.</p>
<p><em>Classic version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="per-spider-settings-gsoc-2014">
<h5>Per-spider settings (GSoC 2014)<a class="headerlink" href="#per-spider-settings-gsoc-2014" title="Permalink to this heading">¶</a></h5>
<p>Last Google Summer of Code project accomplished an important redesign of the
mechanism used for populating settings, introducing explicit priorities to
override any given setting. As an extension of that goal, we included a new
level of priority for settings that act exclusively for a single spider,
allowing them to redefine project settings.</p>
<p>Start using it by defining a <code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code>
class variable in your spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">custom_settings</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;DOWNLOAD_DELAY&quot;</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="s2">&quot;RETRY_ENABLED&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Read more about settings population: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a></p>
</section>
<section id="python-logging">
<h5>Python Logging<a class="headerlink" href="#python-logging" title="Permalink to this heading">¶</a></h5>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built in’s
as default logging system. We’re maintaining backward compatibility for most
of the old custom interface to call logging functions, but you’ll get
warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>
<span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s1">&#39;MESSAGE&#39;</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;MESSAGE&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Logging with spiders remains the same, but on top of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code> method you’ll have access to a custom
<code class="xref py py-attr docutils literal notranslate"><span class="pre">logger</span></code> created for the spider to issue log
events:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Response received&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more in the logging documentation: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a></p>
</section>
<section id="crawler-api-refactoring-gsoc-2014">
<h5>Crawler API refactoring (GSoC 2014)<a class="headerlink" href="#crawler-api-refactoring-gsoc-2014" title="Permalink to this heading">¶</a></h5>
<p>Another milestone for last Google Summer of Code was a refactoring of the
internal API, seeking a simpler and easier usage. Check new core interface
in: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api"><span class="std std-ref">Core API</span></a></p>
<p>A common situation where you will face these changes is while running Scrapy
from scripts. Here’s a quick example of how to run a Spider manually with the
new API:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="kn">import</span> <span class="n">CrawlerProcess</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">({</span>
    <span class="s1">&#39;USER_AGENT&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#39;</span>
<span class="p">})</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>Bear in mind this feature is still under development and its API may change
until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-practices"><span class="std std-ref">Common Practices</span></a></p>
</section>
<section id="module-relocations">
<span id="id147"></span><h5>Module Relocations<a class="headerlink" href="#module-relocations" title="Permalink to this heading">¶</a></h5>
<p>There’s been a large rearrangement of modules trying to improve the general
structure of Scrapy. Main changes were separating various subpackages into
new projects and dissolving both <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code>
into top level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings indicating
their new place.</p>
<section id="full-list-of-relocations">
<h6>Full list of relocations<a class="headerlink" href="#full-list-of-relocations" title="Permalink to this heading">¶</a></h6>
<p>Outsourced packages</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These extensions went through some minor changes, e.g. some setting names
were changed. Please check the documentation in each new repository to
get familiar with the new usage.</p>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Old location</p></th>
<th class="head"><p>New location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scrapy.commands.deploy</p></td>
<td><p><a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a>
(See other alternatives here:
<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-deploy"><span class="std std-ref">Deploying Spiders</span></a>)</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib.djangoitem</p></td>
<td><p><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-djangoitem">scrapy-djangoitem</a></p></td>
</tr>
<tr class="row-even"><td><p>scrapy.webservice</p></td>
<td><p><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-jsonrpc">scrapy-jsonrpc</a></p></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> dissolutions</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Old location</p></th>
<th class="head"><p>New location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scrapy.contrib_exp.downloadermiddleware.decompression</p></td>
<td><p>scrapy.downloadermiddlewares.decompression</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib_exp.iterators</p></td>
<td><p>scrapy.utils.iterators</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.contrib.downloadermiddleware</p></td>
<td><p>scrapy.downloadermiddlewares</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib.exporter</p></td>
<td><p>scrapy.exporters</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.contrib.linkextractors</p></td>
<td><p>scrapy.linkextractors</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib.loader</p></td>
<td><p>scrapy.loader</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.contrib.loader.processor</p></td>
<td><p>scrapy.loader.processors</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib.pipeline</p></td>
<td><p>scrapy.pipelines</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.contrib.spidermiddleware</p></td>
<td><p>scrapy.spidermiddlewares</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.contrib.spiders</p></td>
<td><p>scrapy.spiders</p></td>
</tr>
<tr class="row-even"><td><ul class="simple">
<li><p>scrapy.contrib.closespider</p></li>
<li><p>scrapy.contrib.corestats</p></li>
<li><p>scrapy.contrib.debug</p></li>
<li><p>scrapy.contrib.feedexport</p></li>
<li><p>scrapy.contrib.httpcache</p></li>
<li><p>scrapy.contrib.logstats</p></li>
<li><p>scrapy.contrib.memdebug</p></li>
<li><p>scrapy.contrib.memusage</p></li>
<li><p>scrapy.contrib.spiderstate</p></li>
<li><p>scrapy.contrib.statsmailer</p></li>
<li><p>scrapy.contrib.throttle</p></li>
</ul>
</td>
<td><p>scrapy.extensions.*</p></td>
</tr>
</tbody>
</table>
<p>Plural renames and Modules unification</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Old location</p></th>
<th class="head"><p>New location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scrapy.command</p></td>
<td><p>scrapy.commands</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.dupefilter</p></td>
<td><p>scrapy.dupefilters</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.linkextractor</p></td>
<td><p>scrapy.linkextractors</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.spider</p></td>
<td><p>scrapy.spiders</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.squeue</p></td>
<td><p>scrapy.squeues</p></td>
</tr>
<tr class="row-odd"><td><p>scrapy.statscol</p></td>
<td><p>scrapy.statscollectors</p></td>
</tr>
<tr class="row-even"><td><p>scrapy.utils.decorator</p></td>
<td><p>scrapy.utils.decorators</p></td>
</tr>
</tbody>
</table>
<p>Class renames</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Old location</p></th>
<th class="head"><p>New location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>scrapy.spidermanager.SpiderManager</p></td>
<td><p>scrapy.spiderloader.SpiderLoader</p></td>
</tr>
</tbody>
</table>
<p>Settings renames</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Old location</p></th>
<th class="head"><p>New location</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SPIDER_MANAGER_CLASS</p></td>
<td><p>SPIDER_LOADER_CLASS</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="changelog">
<h5>Changelog<a class="headerlink" href="#changelog" title="Permalink to this heading">¶</a></h5>
<p>New Features and Enhancements</p>
<ul class="simple">
<li><p>Python logging (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1060">issue 1060</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1235">issue 1235</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1236">issue 1236</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1240">issue 1240</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1259">issue 1259</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1278">issue 1278</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1286">issue 1286</a>)</p></li>
<li><p>FEED_EXPORT_FIELDS option (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1159">issue 1159</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1224">issue 1224</a>)</p></li>
<li><p>Dns cache size and timeout options (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1132">issue 1132</a>)</p></li>
<li><p>support namespace prefix in xmliter_lxml (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/963">issue 963</a>)</p></li>
<li><p>Reactor threadpool max size setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1123">issue 1123</a>)</p></li>
<li><p>Allow spiders to return dicts. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1081">issue 1081</a>)</p></li>
<li><p>Add Response.urljoin() helper (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1086">issue 1086</a>)</p></li>
<li><p>look in ~/.config/scrapy.cfg for user config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1098">issue 1098</a>)</p></li>
<li><p>handle TLS SNI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1101">issue 1101</a>)</p></li>
<li><p>Selectorlist extract first (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/624">issue 624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1145">issue 1145</a>)</p></li>
<li><p>Added JmesSelect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1016">issue 1016</a>)</p></li>
<li><p>add gzip compression to filesystem http cache backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1020">issue 1020</a>)</p></li>
<li><p>CSS support in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/983">issue 983</a>)</p></li>
<li><p>httpcache dont_cache meta #19 #689 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/821">issue 821</a>)</p></li>
<li><p>add signal to be sent when request is dropped by the scheduler
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/961">issue 961</a>)</p></li>
<li><p>avoid download large response (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/946">issue 946</a>)</p></li>
<li><p>Allow to specify the quotechar in CSVFeedSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/882">issue 882</a>)</p></li>
<li><p>Add referer to “Spider error processing” log message (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/795">issue 795</a>)</p></li>
<li><p>process robots.txt once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/896">issue 896</a>)</p></li>
<li><p>GSoC Per-spider settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/854">issue 854</a>)</p></li>
<li><p>Add project name validation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/817">issue 817</a>)</p></li>
<li><p>GSoC API cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/816">issue 816</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1128">issue 1128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1147">issue 1147</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1148">issue 1148</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1156">issue 1156</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1185">issue 1185</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1187">issue 1187</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1258">issue 1258</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1268">issue 1268</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1276">issue 1276</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1285">issue 1285</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1284">issue 1284</a>)</p></li>
<li><p>Be more responsive with IO operations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1074">issue 1074</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1075">issue 1075</a>)</p></li>
<li><p>Do leveldb compaction for httpcache on closing (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1297">issue 1297</a>)</p></li>
</ul>
<p>Deprecations and Removals</p>
<ul class="simple">
<li><p>Deprecate htmlparser link extractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1205">issue 1205</a>)</p></li>
<li><p>remove deprecated code from FeedExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1155">issue 1155</a>)</p></li>
<li><p>a leftover for.15 compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/925">issue 925</a>)</p></li>
<li><p>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/895">issue 895</a>)</p></li>
<li><p>Drop old engine code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/911">issue 911</a>)</p></li>
<li><p>Deprecate SgmlLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/777">issue 777</a>)</p></li>
</ul>
<p>Relocations</p>
<ul class="simple">
<li><p>Move exporters/__init__.py to exporters.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1242">issue 1242</a>)</p></li>
<li><p>Move base classes to their packages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1218">issue 1218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1233">issue 1233</a>)</p></li>
<li><p>Module relocation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1181">issue 1181</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1210">issue 1210</a>)</p></li>
<li><p>rename SpiderManager to SpiderLoader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1166">issue 1166</a>)</p></li>
<li><p>Remove djangoitem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1177">issue 1177</a>)</p></li>
<li><p>remove scrapy deploy command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1102">issue 1102</a>)</p></li>
<li><p>dissolve contrib_exp (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1134">issue 1134</a>)</p></li>
<li><p>Deleted bin folder from root, fixes #913 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/914">issue 914</a>)</p></li>
<li><p>Remove jsonrpc based webservice (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/859">issue 859</a>)</p></li>
<li><p>Move Test cases under project root dir (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/827">issue 827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/841">issue 841</a>)</p></li>
<li><p>Fix backward incompatibility for relocated paths in settings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1267">issue 1267</a>)</p></li>
</ul>
<p>Documentation</p>
<ul class="simple">
<li><p>CrawlerProcess documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1190">issue 1190</a>)</p></li>
<li><p>Favoring web scraping over screen scraping in the descriptions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1188">issue 1188</a>)</p></li>
<li><p>Some improvements for Scrapy tutorial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1180">issue 1180</a>)</p></li>
<li><p>Documenting Files Pipeline together with Images Pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1150">issue 1150</a>)</p></li>
<li><p>deployment docs tweaks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1164">issue 1164</a>)</p></li>
<li><p>Added deployment section covering scrapyd-deploy and shub (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1124">issue 1124</a>)</p></li>
<li><p>Adding more settings to project template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1073">issue 1073</a>)</p></li>
<li><p>some improvements to overview page (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1106">issue 1106</a>)</p></li>
<li><p>Updated link in docs/topics/architecture.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/647">issue 647</a>)</p></li>
<li><p>DOC reorder topics (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1022">issue 1022</a>)</p></li>
<li><p>updating list of Request.meta special keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1071">issue 1071</a>)</p></li>
<li><p>DOC document download_timeout (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/898">issue 898</a>)</p></li>
<li><p>DOC simplify extension docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/893">issue 893</a>)</p></li>
<li><p>Leaks docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/894">issue 894</a>)</p></li>
<li><p>DOC document from_crawler method for item pipelines (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/904">issue 904</a>)</p></li>
<li><p>Spider_error doesn’t support deferreds (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1292">issue 1292</a>)</p></li>
<li><p>Corrections &amp; Sphinx related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1220">issue 1220</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1219">issue 1219</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1196">issue 1196</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1172">issue 1172</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1171">issue 1171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1169">issue 1169</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1160">issue 1160</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1154">issue 1154</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1127">issue 1127</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1112">issue 1112</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1105">issue 1105</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1041">issue 1041</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1082">issue 1082</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1033">issue 1033</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/944">issue 944</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/866">issue 866</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/864">issue 864</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/796">issue 796</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1260">issue 1260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1271">issue 1271</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1293">issue 1293</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1298">issue 1298</a>)</p></li>
</ul>
<p>Bugfixes</p>
<ul class="simple">
<li><p>Item multi inheritance fix (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/353">issue 353</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1228">issue 1228</a>)</p></li>
<li><p>ItemLoader.load_item: iterate over copy of fields (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/722">issue 722</a>)</p></li>
<li><p>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1131">issue 1131</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1197">issue 1197</a>)</p></li>
<li><p>Force to read DOWNLOAD_TIMEOUT as int (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/954">issue 954</a>)</p></li>
<li><p>scrapy.utils.misc.load_object should print full traceback (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/902">issue 902</a>)</p></li>
<li><p>Fix bug for “.local” host name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/878">issue 878</a>)</p></li>
<li><p>Fix for Enabled extensions, middlewares, pipelines info not printed
anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/879">issue 879</a>)</p></li>
<li><p>fix dont_merge_cookies bad behaviour when set to false on meta
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/846">issue 846</a>)</p></li>
</ul>
<p>Python 3 In Progress Support</p>
<ul class="simple">
<li><p>disable scrapy.telnet if twisted.conch is not available (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1161">issue 1161</a>)</p></li>
<li><p>fix Python 3 syntax errors in ajaxcrawl.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1162">issue 1162</a>)</p></li>
<li><p>more python3 compatibility changes for urllib (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1121">issue 1121</a>)</p></li>
<li><p>assertItemsEqual was renamed to assertCountEqual in Python 3.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1070">issue 1070</a>)</p></li>
<li><p>Import unittest.mock if available. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1066">issue 1066</a>)</p></li>
<li><p>updated deprecated cgi.parse_qsl to use six’s parse_qsl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/909">issue 909</a>)</p></li>
<li><p>Prevent Python 3 port regressions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/830">issue 830</a>)</p></li>
<li><p>PY3: use MutableMapping for python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/810">issue 810</a>)</p></li>
<li><p>PY3: use six.BytesIO and six.moves.cStringIO (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/803">issue 803</a>)</p></li>
<li><p>PY3: fix xmlrpclib and email imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/801">issue 801</a>)</p></li>
<li><p>PY3: use six for robotparser and urlparse (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/800">issue 800</a>)</p></li>
<li><p>PY3: use six.iterkeys, six.iteritems, and tempfile (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/799">issue 799</a>)</p></li>
<li><p>PY3: fix has_key and use six.moves.configparser (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/798">issue 798</a>)</p></li>
<li><p>PY3: use six.moves.cPickle (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/797">issue 797</a>)</p></li>
<li><p>PY3 make it possible to run some tests in Python3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/776">issue 776</a>)</p></li>
</ul>
<p>Tests</p>
<ul class="simple">
<li><p>remove unnecessary lines from py3-ignores (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1243">issue 1243</a>)</p></li>
<li><p>Fix remaining warnings from pytest while collecting tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1206">issue 1206</a>)</p></li>
<li><p>Add docs build to travis (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1234">issue 1234</a>)</p></li>
<li><p>TST don’t collect tests from deprecated modules. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1165">issue 1165</a>)</p></li>
<li><p>install service_identity package in tests to prevent warnings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1168">issue 1168</a>)</p></li>
<li><p>Fix deprecated settings API in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1152">issue 1152</a>)</p></li>
<li><p>Add test for webclient with POST method and no body given (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1089">issue 1089</a>)</p></li>
<li><p>py3-ignores.txt supports comments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1044">issue 1044</a>)</p></li>
<li><p>modernize some of the asserts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/835">issue 835</a>)</p></li>
<li><p>selector.__repr__ test (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/779">issue 779</a>)</p></li>
</ul>
<p>Code refactoring</p>
<ul class="simple">
<li><p>CSVFeedSpider cleanup: use iterate_spider_output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1079">issue 1079</a>)</p></li>
<li><p>remove unnecessary check from scrapy.utils.spider.iter_spider_output
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1078">issue 1078</a>)</p></li>
<li><p>Pydispatch pep8 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/992">issue 992</a>)</p></li>
<li><p>Removed unused ‘load=False’ parameter from walk_modules() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/871">issue 871</a>)</p></li>
<li><p>For consistency, use <code class="docutils literal notranslate"><span class="pre">job_dir</span></code> helper in <code class="docutils literal notranslate"><span class="pre">SpiderState</span></code> extension.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/805">issue 805</a>)</p></li>
<li><p>rename “sflo” local variables to less cryptic “log_observer” (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/775">issue 775</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-24-6-2015-04-20">
<h4>Scrapy 0.24.6 (2015-04-20)<a class="headerlink" href="#scrapy-0-24-6-2015-04-20" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>encode invalid xpath with unicode_escape under PY2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/07cb3e5">commit 07cb3e5</a>)</p></li>
<li><p>fix IPython shell scope issue and load IPython user config (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2c8e573">commit 2c8e573</a>)</p></li>
<li><p>Fix small typo in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d694019">commit d694019</a>)</p></li>
<li><p>Fix small typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f92fa83">commit f92fa83</a>)</p></li>
<li><p>Converted sel.xpath() calls to response.xpath() in Extracting the data (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c2c6d15">commit c2c6d15</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-5-2015-02-25">
<h4>Scrapy 0.24.5 (2015-02-25)<a class="headerlink" href="#scrapy-0-24-5-2015-02-25" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Support new _getEndpoint Agent signatures on Twisted 15.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/540b9bc">commit 540b9bc</a>)</p></li>
<li><p>DOC a couple more references are fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4c454b">commit b4c454b</a>)</p></li>
<li><p>DOC fix a reference (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3c1260">commit e3c1260</a>)</p></li>
<li><p>t.i.b.ThreadedResolver is now a new-style class (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9e13f42">commit 9e13f42</a>)</p></li>
<li><p>S3DownloadHandler: fix auth for requests with quoted paths/query params (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cdb9a0b">commit cdb9a0b</a>)</p></li>
<li><p>fixed the variable types in mailsender documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb3a848">commit bb3a848</a>)</p></li>
<li><p>Reset items_scraped instead of item_count (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/edb07a4">commit edb07a4</a>)</p></li>
<li><p>Tentative attention message about what document to read for contributions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7ee6f7a">commit 7ee6f7a</a>)</p></li>
<li><p>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/874fcdd">commit 874fcdd</a>)</p></li>
<li><p>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6b21f0">commit c6b21f0</a>)</p></li>
<li><p>Test the parse command locally instead of against an external url (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3a6628">commit c3a6628</a>)</p></li>
<li><p>Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0bf957">commit d0bf957</a>)</p></li>
<li><p>Updates documentation on dynamic item classes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eeb589a">commit eeb589a</a>)</p></li>
<li><p>Merge pull request #943 from Lazar-T/patch-3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5fdab02">commit 5fdab02</a>)</p></li>
<li><p>typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b0ae199">commit b0ae199</a>)</p></li>
<li><p>pywin32 is required by Twisted. closes #937 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5cb0cfb">commit 5cb0cfb</a>)</p></li>
<li><p>Update install.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/781286b">commit 781286b</a>)</p></li>
<li><p>Merge pull request #928 from Lazar-T/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b415d04">commit b415d04</a>)</p></li>
<li><p>comma instead of fullstop (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/627b9ba">commit 627b9ba</a>)</p></li>
<li><p>Merge pull request #885 from jsma/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de909ad">commit de909ad</a>)</p></li>
<li><p>Update request-response.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3f3263d">commit 3f3263d</a>)</p></li>
<li><p>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode present (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/49b40f0">commit 49b40f0</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-4-2014-08-09">
<h4>Scrapy 0.24.4 (2014-08-09)<a class="headerlink" href="#scrapy-0-24-4-2014-08-09" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>pem file is used by mockserver and required by scrapy bench (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5eddc68b63">commit 5eddc68b63</a>)</p></li>
<li><p>scrapy bench needs scrapy.tests* (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d6cb999">commit d6cb999</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-3-2014-08-09">
<h4>Scrapy 0.24.3 (2014-08-09)<a class="headerlink" href="#scrapy-0-24-3-2014-08-09" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>no need to waste travis-ci time on py3 for 0.24 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e080c1">commit 8e080c1</a>)</p></li>
<li><p>Update installation docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1d0c096">commit 1d0c096</a>)</p></li>
<li><p>There is a trove classifier for Scrapy framework! (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4c701d7">commit 4c701d7</a>)</p></li>
<li><p>update other places where w3lib version is mentioned (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d109c13">commit d109c13</a>)</p></li>
<li><p>Update w3lib requirement to 1.8.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/39d2ce5">commit 39d2ce5</a>)</p></li>
<li><p>Use w3lib.html.replace_entities() (remove_entities() is deprecated) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/180d3ad">commit 180d3ad</a>)</p></li>
<li><p>set zip_safe=False (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a51ee8b">commit a51ee8b</a>)</p></li>
<li><p>do not ship tests package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ee3b371">commit ee3b371</a>)</p></li>
<li><p>scrapy.bat is not needed anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3861cf">commit c3861cf</a>)</p></li>
<li><p>Modernize setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/362e322">commit 362e322</a>)</p></li>
<li><p>headers can not handle non-string values (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/94a5c65">commit 94a5c65</a>)</p></li>
<li><p>fix ftp test cases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274a7f">commit a274a7f</a>)</p></li>
<li><p>The sum up of travis-ci builds are taking like 50min to complete (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ae1e2cc">commit ae1e2cc</a>)</p></li>
<li><p>Update shell.rst typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e49c96a">commit e49c96a</a>)</p></li>
<li><p>removes weird indentation in the shell results (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ca489d">commit 1ca489d</a>)</p></li>
<li><p>improved explanations, clarified blog post as source, added link for XPath string functions in the spec (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/65c8f05">commit 65c8f05</a>)</p></li>
<li><p>renamed UserTimeoutError and ServerTimeouterror #583 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/037f6ab">commit 037f6ab</a>)</p></li>
<li><p>adding some xpath tips to selectors docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d103e0">commit 2d103e0</a>)</p></li>
<li><p>fix tests to account for <a class="reference external" href="https://github.com/scrapy/w3lib/pull/23">https://github.com/scrapy/w3lib/pull/23</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d366a">commit f8d366a</a>)</p></li>
<li><p>get_func_args maximum recursion fix #728 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/81344ea">commit 81344ea</a>)</p></li>
<li><p>Updated input/output processor example according to #560. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f7c4ea8">commit f7c4ea8</a>)</p></li>
<li><p>Fixed Python syntax in tutorial. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db59ed9">commit db59ed9</a>)</p></li>
<li><p>Add test case for tunneling proxy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f090260">commit f090260</a>)</p></li>
<li><p>Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8793af">commit d8793af</a>)</p></li>
<li><p>Extract links from XHTML documents with MIME-Type “application/xml” (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ed1f376">commit ed1f376</a>)</p></li>
<li><p>Merge pull request #793 from roysc/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/91a1106">commit 91a1106</a>)</p></li>
<li><p>Fix typo in commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/743e1e2">commit 743e1e2</a>)</p></li>
<li><p>better testcase for settings.overrides.setdefault (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e22daaf">commit e22daaf</a>)</p></li>
<li><p>Using CRLF as line marker according to http 1.1 definition (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ec430b">commit 5ec430b</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-2-2014-07-08">
<h4>Scrapy 0.24.2 (2014-07-08)<a class="headerlink" href="#scrapy-0-24-2-2014-07-08" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e5e8133">commit e5e8133</a>)</p></li>
<li><p>there is not support for python3 yet (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3cd6146">commit 3cd6146</a>)</p></li>
<li><p>Update python compatible version set to Debian packages (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa5d76b">commit fa5d76b</a>)</p></li>
<li><p>DOC fix formatting in release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6a9e20">commit c6a9e20</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-1-2014-06-27">
<h4>Scrapy 0.24.1 (2014-06-27)<a class="headerlink" href="#scrapy-0-24-1-2014-06-27" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Fix deprecated CrawlerSettings and increase backward compatibility with
.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e3f20a">commit 8e3f20a</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-24-0-2014-06-26">
<h4>Scrapy 0.24.0 (2014-06-26)<a class="headerlink" href="#scrapy-0-24-0-2014-06-26" title="Permalink to this heading">¶</a></h4>
<section id="enhancements">
<h5>Enhancements<a class="headerlink" href="#enhancements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Improve Scrapy top-level namespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/494">issue 494</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/684">issue 684</a>)</p></li>
<li><p>Add selector shortcuts to responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/554">issue 554</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/690">issue 690</a>)</p></li>
<li><p>Add new lxml based LinkExtractor to replace unmaintained SgmlLinkExtractor
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/559">issue 559</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/761">issue 761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/763">issue 763</a>)</p></li>
<li><p>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>)</p></li>
<li><p>Add UTF8 encoding header to templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/688">issue 688</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/762">issue 762</a>)</p></li>
<li><p>Telnet console now binds to 127.0.0.1 by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/699">issue 699</a>)</p></li>
<li><p>Update Debian/Ubuntu install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/509">issue 509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/549">issue 549</a>)</p></li>
<li><p>Disable smart strings in lxml XPath evaluations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/535">issue 535</a>)</p></li>
<li><p>Restore filesystem based cache as default for http
cache middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/571">issue 571</a>)</p></li>
<li><p>Expose current crawler in Scrapy shell (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/557">issue 557</a>)</p></li>
<li><p>Improve testsuite comparing CSV and XML exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/570">issue 570</a>)</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">offsite/filtered</span></code> and <code class="docutils literal notranslate"><span class="pre">offsite/domains</span></code> stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>)</p></li>
<li><p>Support process_links as generator in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/555">issue 555</a>)</p></li>
<li><p>Verbose logging and new stats counters for DupeFilter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/553">issue 553</a>)</p></li>
<li><p>Add a mimetype parameter to <code class="docutils literal notranslate"><span class="pre">MailSender.send()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/602">issue 602</a>)</p></li>
<li><p>Generalize file pipeline log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/622">issue 622</a>)</p></li>
<li><p>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/565">issue 565</a>)</p></li>
<li><p>Converted SEP documents to rst format (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/629">issue 629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/630">issue 630</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/638">issue 638</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/632">issue 632</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/636">issue 636</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/640">issue 640</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/635">issue 635</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/634">issue 634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/639">issue 639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/637">issue 637</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/631">issue 631</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/633">issue 633</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/641">issue 641</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/642">issue 642</a>)</p></li>
<li><p>Tests and docs for clickdata’s nr index in FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/646">issue 646</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/645">issue 645</a>)</p></li>
<li><p>Allow to disable a downloader handler just like any other component (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>)</p></li>
<li><p>Log when a request is discarded after too many redirections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>)</p></li>
<li><p>Log error responses if they are not handled by spider callbacks
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/656">issue 656</a>)</p></li>
<li><p>Add content-type check to http compression mw (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/193">issue 193</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/660">issue 660</a>)</p></li>
<li><p>Run pypy tests using latest pypi from ppa (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/674">issue 674</a>)</p></li>
<li><p>Run test suite using pytest instead of trial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/679">issue 679</a>)</p></li>
<li><p>Build docs and check for dead links in tox environment (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/687">issue 687</a>)</p></li>
<li><p>Make scrapy.version_info a tuple of integers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/681">issue 681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/692">issue 692</a>)</p></li>
<li><p>Infer exporter’s output format from filename extensions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/546">issue 546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/659">issue 659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/760">issue 760</a>)</p></li>
<li><p>Support case-insensitive domains in <code class="docutils literal notranslate"><span class="pre">url_is_from_any_domain()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>)</p></li>
<li><p>Remove pep8 warnings in project and spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/698">issue 698</a>)</p></li>
<li><p>Tests and docs for <code class="docutils literal notranslate"><span class="pre">request_fingerprint</span></code> function (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/597">issue 597</a>)</p></li>
<li><p>Update SEP-19 for GSoC project <code class="docutils literal notranslate"><span class="pre">per-spider</span> <span class="pre">settings</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>)</p></li>
<li><p>Set exit code to non-zero when contracts fails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/727">issue 727</a>)</p></li>
<li><p>Add a setting to control what class is instantiated as Downloader component
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/738">issue 738</a>)</p></li>
<li><p>Pass response in <code class="docutils literal notranslate"><span class="pre">item_dropped</span></code> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/724">issue 724</a>)</p></li>
<li><p>Improve <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> contracts command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/733">issue 733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/752">issue 752</a>)</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">spider.closed()</span></code> shortcut (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/719">issue 719</a>)</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">request_scheduled</span></code> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/746">issue 746</a>)</p></li>
<li><p>Add a note about reporting security issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/697">issue 697</a>)</p></li>
<li><p>Add LevelDB http cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/626">issue 626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>)</p></li>
<li><p>Sort spider list output of <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">list</span></code> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>)</p></li>
<li><p>Multiple documentation enhancements and fixes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/575">issue 575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/587">issue 587</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/590">issue 590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/596">issue 596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/610">issue 610</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/617">issue 617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/618">issue 618</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/627">issue 627</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/613">issue 613</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/643">issue 643</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/675">issue 675</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/663">issue 663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/711">issue 711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/714">issue 714</a>)</p></li>
</ul>
</section>
<section id="id148">
<h5>Bugfixes<a class="headerlink" href="#id148" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>)</p></li>
<li><p>Ignore None values in ItemLoader processors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/556">issue 556</a>)</p></li>
<li><p>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/485">issue 485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/574">issue 574</a>)</p></li>
<li><p>Fix wrong checks on subclassing of deprecated classes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/581">issue 581</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/584">issue 584</a>)</p></li>
<li><p>Handle errors caused by inspect.stack() failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/582">issue 582</a>)</p></li>
<li><p>Fix a reference to unexistent engine attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/593">issue 593</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/594">issue 594</a>)</p></li>
<li><p>Fix dynamic itemclass example usage of type() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/603">issue 603</a>)</p></li>
<li><p>Use lucasdemarchi/codespell to fix typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/628">issue 628</a>)</p></li>
<li><p>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>)</p></li>
<li><p>Fix XXE flaw in sitemap reader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/676">issue 676</a>)</p></li>
<li><p>Fix engine to support filtered start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/707">issue 707</a>)</p></li>
<li><p>Fix offsite middleware case on urls with no hostnames (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/745">issue 745</a>)</p></li>
<li><p>Testsuite doesn’t require PIL anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/585">issue 585</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-22-2-released-2014-02-14">
<h4>Scrapy 0.22.2 (released 2014-02-14)<a class="headerlink" href="#scrapy-0-22-2-released-2014-02-14" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>fix a reference to unexistent engine.slots. closes #593 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c099a">commit 13c099a</a>)</p></li>
<li><p>downloaderMW doc typo (spiderMW doc copy remnant) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit 8ae11bf</a>)</p></li>
<li><p>Correct typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1346037">commit 1346037</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-22-1-released-2014-02-08">
<h4>Scrapy 0.22.1 (released 2014-02-08)<a class="headerlink" href="#scrapy-0-22-1-released-2014-02-08" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>localhost666 can resolve under certain circumstances (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2ec2279">commit 2ec2279</a>)</p></li>
<li><p>test inspect.stack failure (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit cc3eda3</a>)</p></li>
<li><p>Handle cases when inspect.stack() fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit 8cb44f9</a>)</p></li>
<li><p>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/46d98d6">commit 46d98d6</a>)</p></li>
<li><p>Docs: 4-space indent for final spider example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13846de">commit 13846de</a>)</p></li>
<li><p>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/368a946">commit 368a946</a>)</p></li>
<li><p>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b566388">commit b566388</a>)</p></li>
<li><p>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c1cb418">commit c1cb418</a>)</p></li>
<li><p>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7e4d627">commit 7e4d627</a>)</p></li>
<li><p>Fix tests for Travis-CI build (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/76c7e20">commit 76c7e20</a>)</p></li>
<li><p>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f87b17">commit 5f87b17</a>)</p></li>
<li><p>RegexLinkExtractor: encode URL unicode value when creating Links (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0ee545">commit d0ee545</a>)</p></li>
<li><p>Updated the tutorial crawl output with latest output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8da65de">commit 8da65de</a>)</p></li>
<li><p>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/875b9ab">commit 875b9ab</a>)</p></li>
<li><p>PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f89efaf">commit f89efaf</a>)</p></li>
<li><p>Expose current crawler in the Scrapy shell. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5349cec">commit 5349cec</a>)</p></li>
<li><p>Unused re import and PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/387f414">commit 387f414</a>)</p></li>
<li><p>Ignore None’s values when using the ItemLoader. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0632546">commit 0632546</a>)</p></li>
<li><p>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit cde9a8c</a>)</p></li>
<li><p>show Ubuntu setup instructions as literal code (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit fb5c9c5</a>)</p></li>
<li><p>Update Ubuntu installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/70fb105">commit 70fb105</a>)</p></li>
<li><p>Merge pull request #550 from stray-leone/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit 6f70b6a</a>)</p></li>
<li><p>modify the version of Scrapy Ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/725900d">commit 725900d</a>)</p></li>
<li><p>fix 0.22.0 release date (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/af0219a">commit af0219a</a>)</p></li>
<li><p>fix typos in news.rst and remove (not released yet) header (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit b7f58f4</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-22-0-released-2014-01-17">
<h4>Scrapy 0.22.0 (released 2014-01-17)<a class="headerlink" href="#scrapy-0-22-0-released-2014-01-17" title="Permalink to this heading">¶</a></h4>
<section id="id149">
<h5>Enhancements<a class="headerlink" href="#id149" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>[<strong>Backward incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>)
To restore old backend set <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></code></p></li>
<li><p>Proxy https:// urls using CONNECT method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/392">issue 392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/397">issue 397</a>)</p></li>
<li><p>Add a middleware to crawl ajax crawlable pages as defined by google (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>)</p></li>
<li><p>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/510">issue 510</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/519">issue 519</a>)</p></li>
<li><p>Selectors register EXSLT namespaces by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/472">issue 472</a>)</p></li>
<li><p>Unify item loaders similar to selectors renaming (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/461">issue 461</a>)</p></li>
<li><p>Make <code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> class easily subclassable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/533">issue 533</a>)</p></li>
<li><p>Improve test coverage and forthcoming Python 3 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/525">issue 525</a>)</p></li>
<li><p>Promote startup info on settings and middleware to INFO level (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>)</p></li>
<li><p>Support partials in <code class="docutils literal notranslate"><span class="pre">get_func_args</span></code> util (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/506">issue 506</a>, issue:<cite>504</cite>)</p></li>
<li><p>Allow running individual tests via tox (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/503">issue 503</a>)</p></li>
<li><p>Update extensions ignored by link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/498">issue 498</a>)</p></li>
<li><p>Add middleware methods to get files/images/thumbs paths (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/490">issue 490</a>)</p></li>
<li><p>Improve offsite middleware tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/478">issue 478</a>)</p></li>
<li><p>Add a way to skip default Referer header set by RefererMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>)</p></li>
<li><p>Do not send <code class="docutils literal notranslate"><span class="pre">x-gzip</span></code> in default <code class="docutils literal notranslate"><span class="pre">Accept-Encoding</span></code> header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/469">issue 469</a>)</p></li>
<li><p>Support defining http error handling using settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/466">issue 466</a>)</p></li>
<li><p>Use modern python idioms wherever you find legacies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/497">issue 497</a>)</p></li>
<li><p>Improve and correct documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/527">issue 527</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/524">issue 524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/521">issue 521</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/517">issue 517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/512">issue 512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/505">issue 505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/502">issue 502</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/489">issue 489</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/465">issue 465</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/460">issue 460</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/425">issue 425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/536">issue 536</a>)</p></li>
</ul>
</section>
<section id="fixes">
<h5>Fixes<a class="headerlink" href="#fixes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Update Selector class imports in CrawlSpider template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/484">issue 484</a>)</p></li>
<li><p>Fix unexistent reference to <code class="docutils literal notranslate"><span class="pre">engine.slots</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/464">issue 464</a>)</p></li>
<li><p>Do not try to call <code class="docutils literal notranslate"><span class="pre">body_as_unicode()</span></code> on a non-TextResponse instance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/462">issue 462</a>)</p></li>
<li><p>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/523">issue 523</a>)</p></li>
<li><p>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/537">issue 537</a>)</p></li>
<li><p>Multiple fixes to memory stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/531">issue 531</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/530">issue 530</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/529">issue 529</a>)</p></li>
<li><p>Fix overriding url in <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/507">issue 507</a>)</p></li>
<li><p>Fix tests runner under pip 1.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/513">issue 513</a>)</p></li>
<li><p>Fix logging error when spider name is unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/479">issue 479</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-20-2-released-2013-12-09">
<h4>Scrapy 0.20.2 (released 2013-12-09)<a class="headerlink" href="#scrapy-0-20-2-released-2013-12-09" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Update CrawlSpider Template with Selector changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d1457d">commit 6d1457d</a>)</p></li>
<li><p>fix method name in tutorial. closes GH-480 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4fc359">commit b4fc359</a></p></li>
</ul>
</section>
<section id="scrapy-0-20-1-released-2013-11-28">
<h4>Scrapy 0.20.1 (released 2013-11-28)<a class="headerlink" href="#scrapy-0-20-1-released-2013-11-28" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>include_package_data is required to build wheels from published sources (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit 5ba1ad5</a>)</p></li>
<li><p>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/419a780">commit 419a780</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-20-0-released-2013-11-08">
<h4>Scrapy 0.20.0 (released 2013-11-08)<a class="headerlink" href="#scrapy-0-20-0-released-2013-11-08" title="Permalink to this heading">¶</a></h4>
<section id="id150">
<h5>Enhancements<a class="headerlink" href="#id150" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>New Selector’s API including CSS selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/395">issue 395</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/426">issue 426</a>),</p></li>
<li><p>Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)</p></li>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> is now defined as a dict (instead of a list)</p></li>
<li><p>Sitemap spider can fetch alternate URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/360">issue 360</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> now remove namespaces from element’s attributes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/416">issue 416</a>)</p></li>
<li><p>Paved the road for Python 3.3+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/435">issue 435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/436">issue 436</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/431">issue 431</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/452">issue 452</a>)</p></li>
<li><p>New item exporter using native python types with nesting support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>)</p></li>
<li><p>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit b43b5f575</a>)</p></li>
<li><p>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/327">issue 327</a>)</p></li>
<li><p>New FilesPipeline with functionality factored out from ImagesPipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/370">issue 370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/409">issue 409</a>)</p></li>
<li><p>Recommend Pillow instead of PIL for image handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/317">issue 317</a>)</p></li>
<li><p>Added Debian packages for Ubuntu Quantal and Raring (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86230c0">commit 86230c0</a>)</p></li>
<li><p>Mock server (used for tests) can listen for HTTPS requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/410">issue 410</a>)</p></li>
<li><p>Remove multi spider support from multiple core components
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/422">issue 422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/421">issue 421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/420">issue 420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/419">issue 419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/423">issue 423</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>)</p></li>
<li><p>Travis-CI now tests Scrapy changes against development versions of <code class="docutils literal notranslate"><span class="pre">w3lib</span></code> and <code class="docutils literal notranslate"><span class="pre">queuelib</span></code> python packages.</p></li>
<li><p>Add pypy 2.1 to continuous integration tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit ecfa7431</a>)</p></li>
<li><p>Pylinted, pep8 and removed old-style exceptions from source (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/430">issue 430</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/432">issue 432</a>)</p></li>
<li><p>Use importlib for parametric imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/445">issue 445</a>)</p></li>
<li><p>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/372">issue 372</a>)</p></li>
<li><p>Bugfix crawling shutdown on SIGINT (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/450">issue 450</a>)</p></li>
<li><p>Do not submit <code class="docutils literal notranslate"><span class="pre">reset</span></code> type inputs in FormRequest.from_response (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b326b87">commit b326b87</a>)</p></li>
<li><p>Do not silence download errors when request errback raises an exception (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/684cfc0">commit 684cfc0</a>)</p></li>
</ul>
</section>
<section id="id151">
<h5>Bugfixes<a class="headerlink" href="#id151" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Fix tests under Django 1.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit b6bed44c</a>)</p></li>
<li><p>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</p></li>
<li><p>Fix inconsistencies among Twisted releases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/406">issue 406</a>)</p></li>
<li><p>Fix Scrapy shell bugs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/407">issue 407</a>)</p></li>
<li><p>Fix invalid variable name in setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/429">issue 429</a>)</p></li>
<li><p>Fix tutorial references (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/387">issue 387</a>)</p></li>
<li><p>Improve request-response docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/391">issue 391</a>)</p></li>
<li><p>Improve best practices docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/399">issue 399</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/400">issue 400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/401">issue 401</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/402">issue 402</a>)</p></li>
<li><p>Improve django integration docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/404">issue 404</a>)</p></li>
<li><p>Document <code class="docutils literal notranslate"><span class="pre">bindaddress</span></code> request meta (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit 37c24e01d7</a>)</p></li>
<li><p>Improve <code class="docutils literal notranslate"><span class="pre">Request</span></code> class documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/226">issue 226</a>)</p></li>
</ul>
</section>
<section id="id152">
<h5>Other<a class="headerlink" href="#id152" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Dropped Python 2.6 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/448">issue 448</a>)</p></li>
<li><p>Add <a class="reference external" href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)"><span class="xref std std-doc">cssselect</span></a> python package as install dependency</p></li>
<li><p>Drop libxml2 and multi selector’s backend support, <a class="reference external" href="https://lxml.de/">lxml</a> is required from now on.</p></li>
<li><p>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</p></li>
<li><p>Running test suite now requires <code class="docutils literal notranslate"><span class="pre">mock</span></code> python library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/390">issue 390</a>)</p></li>
</ul>
</section>
<section id="thanks">
<h5>Thanks<a class="headerlink" href="#thanks" title="Permalink to this heading">¶</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">69</span> <span class="n">Daniel</span> <span class="n">Graña</span> <span class="o">&lt;</span><span class="n">dangra</span><span class="o">@...&gt;</span>
<span class="mi">37</span> <span class="n">Pablo</span> <span class="n">Hoffman</span> <span class="o">&lt;</span><span class="n">pablo</span><span class="o">@...&gt;</span>
<span class="mi">13</span> <span class="n">Mikhail</span> <span class="n">Korobov</span> <span class="o">&lt;</span><span class="n">kmike84</span><span class="o">@...&gt;</span>
 <span class="mi">9</span> <span class="n">Alex</span> <span class="n">Cepoi</span> <span class="o">&lt;</span><span class="n">alex</span><span class="o">.</span><span class="n">cepoi</span><span class="o">@...&gt;</span>
 <span class="mi">9</span> <span class="n">alexanderlukanin13</span> <span class="o">&lt;</span><span class="n">alexander</span><span class="o">.</span><span class="n">lukanin</span><span class="mf">.13</span><span class="o">@...&gt;</span>
 <span class="mi">8</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="n">La</span> <span class="n">fuente</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
 <span class="mi">8</span> <span class="n">Lukasz</span> <span class="n">Biedrycki</span> <span class="o">&lt;</span><span class="n">lukasz</span><span class="o">.</span><span class="n">biedrycki</span><span class="o">@...&gt;</span>
 <span class="mi">6</span> <span class="n">Nicolas</span> <span class="n">Ramirez</span> <span class="o">&lt;</span><span class="n">nramirez</span><span class="o">.</span><span class="n">uy</span><span class="o">@...&gt;</span>
 <span class="mi">3</span> <span class="n">Paul</span> <span class="n">Tremberth</span> <span class="o">&lt;</span><span class="n">paul</span><span class="o">.</span><span class="n">tremberth</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Martin</span> <span class="n">Olveyra</span> <span class="o">&lt;</span><span class="n">molveyra</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Stefan</span> <span class="o">&lt;</span><span class="n">misc</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Loren</span> <span class="n">Davie</span> <span class="o">&lt;</span><span class="n">loren</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">irgmedeiros</span> <span class="o">&lt;</span><span class="n">irgmedeiros</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Stefan</span> <span class="n">Koch</span> <span class="o">&lt;</span><span class="n">taikano</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Stefan</span> <span class="o">&lt;</span><span class="n">cct</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">scraperdragon</span> <span class="o">&lt;</span><span class="n">dragon</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Kumara</span> <span class="n">Tharmalingam</span> <span class="o">&lt;</span><span class="n">ktharmal</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Francesco</span> <span class="n">Piccinno</span> <span class="o">&lt;</span><span class="n">stack</span><span class="o">.</span><span class="n">box</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Marcos</span> <span class="n">Campal</span> <span class="o">&lt;</span><span class="n">duendex</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Dragon</span> <span class="n">Dave</span> <span class="o">&lt;</span><span class="n">dragon</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Capi</span> <span class="n">Etheriel</span> <span class="o">&lt;</span><span class="n">barraponto</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">cacovsky</span> <span class="o">&lt;</span><span class="n">amarquesferraz</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Berend</span> <span class="n">Iwema</span> <span class="o">&lt;</span><span class="n">berend</span><span class="o">@...&gt;</span>
</pre></div>
</div>
</section>
</section>
<section id="scrapy-0-18-4-released-2013-10-10">
<h4>Scrapy 0.18.4 (released 2013-10-10)<a class="headerlink" href="#scrapy-0-18-4-released-2013-10-10" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>IPython refuses to update the namespace. fix #396 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit 3d32c4f</a>)</p></li>
<li><p>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b1d8919">commit b1d8919</a>)</p></li>
<li><p>Fix start_requests laziness and early hangs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/89faf52">commit 89faf52</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-18-3-released-2013-10-03">
<h4>Scrapy 0.18.3 (released 2013-10-03)<a class="headerlink" href="#scrapy-0-18-3-released-2013-10-03" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>fix regression on lazy evaluation of start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/12693a5">commit 12693a5</a>)</p></li>
<li><p>forms: do not submit reset inputs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e429f63">commit e429f63</a>)</p></li>
<li><p>increase unittest timeouts to decrease travis false positive failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/912202e">commit 912202e</a>)</p></li>
<li><p>backport master fixes to json exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit cfc2d46</a>)</p></li>
<li><p>Fix permission and set umask before generating sdist tarball (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/06149e0">commit 06149e0</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-18-2-released-2013-09-03">
<h4>Scrapy 0.18.2 (released 2013-09-03)<a class="headerlink" href="#scrapy-0-18-2-released-2013-09-03" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Backport <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> command fixes and backward compatible multi
crawler process(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/339">issue 339</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-18-1-released-2013-08-27">
<h4>Scrapy 0.18.1 (released 2013-08-27)<a class="headerlink" href="#scrapy-0-18-1-released-2013-08-27" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>remove extra import added by cherry picked changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d20304e">commit d20304e</a>)</p></li>
<li><p>fix crawling tests under twisted pre 11.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1994f38">commit 1994f38</a>)</p></li>
<li><p>py26 can not format zero length fields {} (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/abf756f">commit abf756f</a>)</p></li>
<li><p>test PotentiaDataLoss errors on unbound responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b15470d">commit b15470d</a>)</p></li>
<li><p>Treat responses without content-length or Transfer-Encoding as good responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4bf324">commit c4bf324</a>)</p></li>
<li><p>do no include ResponseFailed if http11 handler is not enabled (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cbe684">commit 6cbe684</a>)</p></li>
<li><p>New HTTP client wraps connection lost in ResponseFailed exception. fix #373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1a20bba">commit 1a20bba</a>)</p></li>
<li><p>limit travis-ci build matrix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit 3b01bb8</a>)</p></li>
<li><p>Merge pull request #375 from peterarenot/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa766d7">commit fa766d7</a>)</p></li>
<li><p>Fixed so it refers to the correct folder (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3283809">commit 3283809</a>)</p></li>
<li><p>added Quantal &amp; Raring to support Ubuntu releases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1411923">commit 1411923</a>)</p></li>
<li><p>fix retry middleware which didn’t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit bb35ed0</a>)</p></li>
<li><p>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de3e451">commit de3e451</a>)</p></li>
<li><p>minor updates to 0.18 release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit c45e5f1</a>)</p></li>
<li><p>fix contributors list format (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0b60031">commit 0b60031</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-18-0-released-2013-08-09">
<h4>Scrapy 0.18.0 (released 2013-08-09)<a class="headerlink" href="#scrapy-0-18-0-released-2013-08-09" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Lot of improvements to testsuite run using Tox, including a way to test on pypi</p></li>
<li><p>Handle GET parameters for AJAX crawlable urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit 3fe2a32</a>)</p></li>
<li><p>Use lxml recover option to parse sitemaps (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/347">issue 347</a>)</p></li>
<li><p>Bugfix cookie merging by hostname and not by netloc (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/352">issue 352</a>)</p></li>
<li><p>Support disabling <code class="docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code> using a flag setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/359">issue 359</a>)</p></li>
<li><p>Support xml namespaces using <code class="docutils literal notranslate"><span class="pre">iternodes</span></code> parser in <code class="docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/12">issue 12</a>)</p></li>
<li><p>Support <code class="docutils literal notranslate"><span class="pre">dont_cache</span></code> request meta flag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/19">issue 19</a>)</p></li>
<li><p>Bugfix <code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.gunzip</span></code> broken by changes in python 2.7.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4dc76e">commit 4dc76e</a>)</p></li>
<li><p>Bugfix url encoding on <code class="docutils literal notranslate"><span class="pre">SgmlLinkExtractor</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/24">issue 24</a>)</p></li>
<li><p>Bugfix <code class="docutils literal notranslate"><span class="pre">TakeFirst</span></code> processor shouldn’t discard zero (0) value (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/59">issue 59</a>)</p></li>
<li><p>Support nested items in xml exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/66">issue 66</a>)</p></li>
<li><p>Improve cookies handling performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/77">issue 77</a>)</p></li>
<li><p>Log dupe filtered requests once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/105">issue 105</a>)</p></li>
<li><p>Split redirection middleware into status and meta based middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>)</p></li>
<li><p>Use HTTP1.1 as default downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/109">issue 109</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/318">issue 318</a>)</p></li>
<li><p>Support xpath form selection on <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>)</p></li>
<li><p>Bugfix unicode decoding error on <code class="docutils literal notranslate"><span class="pre">SgmlLinkExtractor</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/199">issue 199</a>)</p></li>
<li><p>Bugfix signal dispatching on pypi interpreter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/205">issue 205</a>)</p></li>
<li><p>Improve request delay and concurrency handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/206">issue 206</a>)</p></li>
<li><p>Add RFC2616 cache policy to <code class="docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/212">issue 212</a>)</p></li>
<li><p>Allow customization of messages logged by engine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/214">issue 214</a>)</p></li>
<li><p>Multiples improvements to <code class="docutils literal notranslate"><span class="pre">DjangoItem</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/217">issue 217</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/218">issue 218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/221">issue 221</a>)</p></li>
<li><p>Extend Scrapy commands using setuptools entry points (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</p></li>
<li><p>Allow spider <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> value to be set/tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/261">issue 261</a>)</p></li>
<li><p>Support <code class="docutils literal notranslate"><span class="pre">settings.getdict</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>)</p></li>
<li><p>Simplify internal <code class="docutils literal notranslate"><span class="pre">scrapy.core.scraper</span></code> slot handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/271">issue 271</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">Item.copy</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/290">issue 290</a>)</p></li>
<li><p>Collect idle downloader slots (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/297">issue 297</a>)</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">ftp://</span></code> scheme downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/329">issue 329</a>)</p></li>
<li><p>Added downloader benchmark webserver and spider tools <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#benchmarking"><span class="std std-ref">Benchmarking</span></a></p></li>
<li><p>Moved persistent (on disk) queues to a separate project (<a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a>) which Scrapy now depends on</p></li>
<li><p>Add Scrapy commands using external libraries (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">--pdb</span></code> option to <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command line tool</p></li>
<li><p>Added <a class="reference internal" href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><code class="xref py py-meth docutils literal notranslate"><span class="pre">XPathSelector.remove_namespaces</span></code></a> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a>.</p></li>
<li><p>Several improvements to spider contracts</p></li>
<li><p>New default middleware named MetaRefreshMiddleware that handles meta-refresh html tag redirections,</p></li>
<li><p>MetaRefreshMiddleware and RedirectMiddleware have different priorities to address #62</p></li>
<li><p>added from_crawler method to spiders</p></li>
<li><p>added system tests with mock server</p></li>
<li><p>more improvements to macOS compatibility (thanks Alex Cepoi)</p></li>
<li><p>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</p></li>
<li><p>support custom download slots</p></li>
<li><p>added –spider option to “shell” command.</p></li>
<li><p>log overridden settings when Scrapy starts</p></li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">130</span> <span class="n">Pablo</span> <span class="n">Hoffman</span> <span class="o">&lt;</span><span class="n">pablo</span><span class="o">@...&gt;</span>
 <span class="mi">97</span> <span class="n">Daniel</span> <span class="n">Graña</span> <span class="o">&lt;</span><span class="n">dangra</span><span class="o">@...&gt;</span>
 <span class="mi">20</span> <span class="n">Nicolás</span> <span class="n">Ramírez</span> <span class="o">&lt;</span><span class="n">nramirez</span><span class="o">.</span><span class="n">uy</span><span class="o">@...&gt;</span>
 <span class="mi">13</span> <span class="n">Mikhail</span> <span class="n">Korobov</span> <span class="o">&lt;</span><span class="n">kmike84</span><span class="o">@...&gt;</span>
 <span class="mi">12</span> <span class="n">Pedro</span> <span class="n">Faustino</span> <span class="o">&lt;</span><span class="n">pedrobandim</span><span class="o">@...&gt;</span>
 <span class="mi">11</span> <span class="n">Steven</span> <span class="n">Almeroth</span> <span class="o">&lt;</span><span class="n">sroth77</span><span class="o">@...&gt;</span>
  <span class="mi">5</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="n">La</span> <span class="n">fuente</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Michal</span> <span class="n">Danilak</span> <span class="o">&lt;</span><span class="n">mimino</span><span class="o">.</span><span class="n">coder</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Alex</span> <span class="n">Cepoi</span> <span class="o">&lt;</span><span class="n">alex</span><span class="o">.</span><span class="n">cepoi</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Alexandr</span> <span class="n">N</span> <span class="n">Zamaraev</span> <span class="p">(</span><span class="n">aka</span> <span class="n">tonal</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">tonal</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">paul</span> <span class="o">&lt;</span><span class="n">paul</span><span class="o">.</span><span class="n">tremberth</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">Martin</span> <span class="n">Olveyra</span> <span class="o">&lt;</span><span class="n">molveyra</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">Jordi</span> <span class="n">Llonch</span> <span class="o">&lt;</span><span class="n">llonchj</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">arijitchakraborty</span> <span class="o">&lt;</span><span class="n">myself</span><span class="o">.</span><span class="n">arijit</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Shane</span> <span class="n">Evans</span> <span class="o">&lt;</span><span class="n">shane</span><span class="o">.</span><span class="n">evans</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">joehillen</span> <span class="o">&lt;</span><span class="n">joehillen</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Hart</span> <span class="o">&lt;</span><span class="n">HartSimha</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Dan</span> <span class="o">&lt;</span><span class="n">ellisd23</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Zuhao</span> <span class="n">Wan</span> <span class="o">&lt;</span><span class="n">wanzuhao</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">whodatninja</span> <span class="o">&lt;</span><span class="n">blake</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">vkrest</span> <span class="o">&lt;</span><span class="n">v</span><span class="o">.</span><span class="n">krestiannykov</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">tpeng</span> <span class="o">&lt;</span><span class="n">pengtaoo</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Tom</span> <span class="n">Mortimer</span><span class="o">-</span><span class="n">Jones</span> <span class="o">&lt;</span><span class="n">tom</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Rocio</span> <span class="n">Aramberri</span> <span class="o">&lt;</span><span class="n">roschegel</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Pedro</span> <span class="o">&lt;</span><span class="n">pedro</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">notsobad</span> <span class="o">&lt;</span><span class="n">wangxiaohugg</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Natan</span> <span class="n">L</span> <span class="o">&lt;</span><span class="n">kuyanatan</span><span class="o">.</span><span class="n">nlao</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Mark</span> <span class="n">Grey</span> <span class="o">&lt;</span><span class="n">mark</span><span class="o">.</span><span class="n">grey</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Luan</span> <span class="o">&lt;</span><span class="n">luanpab</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Libor</span> <span class="n">Nenadál</span> <span class="o">&lt;</span><span class="n">libor</span><span class="o">.</span><span class="n">nenadal</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Juan</span> <span class="n">M</span> <span class="n">Uys</span> <span class="o">&lt;</span><span class="n">opyate</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Jonas</span> <span class="n">Brunsgaard</span> <span class="o">&lt;</span><span class="n">jonas</span><span class="o">.</span><span class="n">brunsgaard</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Ilya</span> <span class="n">Baryshev</span> <span class="o">&lt;</span><span class="n">baryshev</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Hasnain</span> <span class="n">Lakhani</span> <span class="o">&lt;</span><span class="n">m</span><span class="o">.</span><span class="n">hasnain</span><span class="o">.</span><span class="n">lakhani</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Emanuel</span> <span class="n">Schorsch</span> <span class="o">&lt;</span><span class="n">emschorsch</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Chris</span> <span class="n">Tilden</span> <span class="o">&lt;</span><span class="n">chris</span><span class="o">.</span><span class="n">tilden</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Capi</span> <span class="n">Etheriel</span> <span class="o">&lt;</span><span class="n">barraponto</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">cacovsky</span> <span class="o">&lt;</span><span class="n">amarquesferraz</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Berend</span> <span class="n">Iwema</span> <span class="o">&lt;</span><span class="n">berend</span><span class="o">@...&gt;</span>
</pre></div>
</div>
</section>
<section id="scrapy-0-16-5-released-2013-05-30">
<h4>Scrapy 0.16.5 (released 2013-05-30)<a class="headerlink" href="#scrapy-0-16-5-released-2013-05-30" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>obey request method when Scrapy deploy is redirected to a new endpoint (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit 8c4fcee</a>)</p></li>
<li><p>fix inaccurate downloader middleware documentation. refs #280 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/40667cb">commit 40667cb</a>)</p></li>
<li><p>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit bd58bfa</a>)</p></li>
<li><p>Find form nodes in invalid html5 documents (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3d6945">commit e3d6945</a>)</p></li>
<li><p>Fix typo labeling attrs type bool instead of list (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274276">commit a274276</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-16-4-released-2013-01-23">
<h4>Scrapy 0.16.4 (released 2013-01-23)<a class="headerlink" href="#scrapy-0-16-4-released-2013-01-23" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>fixes spelling errors in documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit 6d2b3aa</a>)</p></li>
<li><p>add doc about disabling an extension. refs #132 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c90de33">commit c90de33</a>)</p></li>
<li><p>Fixed error message formatting. log.err() doesn’t support cool formatting and when error occurred, the message was:    “ERROR: Error processing %(item)s” (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c16150c">commit c16150c</a>)</p></li>
<li><p>lint and improve images pipeline error logging (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/56b45fc">commit 56b45fc</a>)</p></li>
<li><p>fixed doc typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/243be84">commit 243be84</a>)</p></li>
<li><p>add documentation topics: Broad Crawls &amp; Common Practices (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1fbb715">commit 1fbb715</a>)</p></li>
<li><p>fix bug in Scrapy parse command when spider is not specified explicitly. closes #209 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c72e682">commit c72e682</a>)</p></li>
<li><p>Update docs/topics/commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/28eac7a">commit 28eac7a</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-16-3-released-2012-12-07">
<h4>Scrapy 0.16.3 (released 2012-12-07)<a class="headerlink" href="#scrapy-0-16-3-released-2012-12-07" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/487b9b5">commit 487b9b5</a>)</p></li>
<li><p>add error details when image pipeline fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8232569">commit 8232569</a>)</p></li>
<li><p>improve macOS compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit 8dcf8aa</a>)</p></li>
<li><p>setup.py: use README.rst to populate long_description (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7b5310d">commit 7b5310d</a>)</p></li>
<li><p>doc: removed obsolete references to ClientForm (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit 80f9bb6</a>)</p></li>
<li><p>correct docs for default storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2aa491b">commit 2aa491b</a>)</p></li>
<li><p>doc: removed broken proxyhub link from FAQ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit bdf61c4</a>)</p></li>
<li><p>Fixed docs typo in SpiderOpenCloseLogging example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7184094">commit 7184094</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-16-2-released-2012-11-09">
<h4>Scrapy 0.16.2 (released 2012-11-09)<a class="headerlink" href="#scrapy-0-16-2-released-2012-11-09" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Scrapy contracts: python2.6 compat (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a4a9199">commit a4a9199</a>)</p></li>
<li><p>Scrapy contracts verbose option (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ec41673">commit ec41673</a>)</p></li>
<li><p>proper unittest-like output for Scrapy contracts (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86635e4">commit 86635e4</a>)</p></li>
<li><p>added open_in_browser to debugging doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c9b690d">commit c9b690d</a>)</p></li>
<li><p>removed reference to global Scrapy stats from settings doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/dd55067">commit dd55067</a>)</p></li>
<li><p>Fix SpiderState bug in Windows platforms (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/58998f4">commit 58998f4</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-16-1-released-2012-10-26">
<h4>Scrapy 0.16.1 (released 2012-10-26)<a class="headerlink" href="#scrapy-0-16-1-released-2012-10-26" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c780fd">commit 8c780fd</a>)</p></li>
<li><p>better backward compatibility for scrapy.conf.settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3403089">commit 3403089</a>)</p></li>
<li><p>extended documentation on how to access crawler stats from extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit c4da0b5</a>)</p></li>
<li><p>removed .hgtags (no longer needed now that Scrapy uses git) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d52c188">commit d52c188</a>)</p></li>
<li><p>fix dashes under rst headers (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit fa4f7f9</a>)</p></li>
<li><p>set release date for 0.16.0 in news (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e292246">commit e292246</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-16-0-released-2012-10-18">
<h4>Scrapy 0.16.0 (released 2012-10-18)<a class="headerlink" href="#scrapy-0-16-0-released-2012-10-18" title="Permalink to this heading">¶</a></h4>
<p>Scrapy changes:</p>
<ul class="simple">
<li><p>added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>, a mechanism for testing spiders in a formal/reproducible way</p></li>
<li><p>added options <code class="docutils literal notranslate"><span class="pre">-o</span></code> and <code class="docutils literal notranslate"><span class="pre">-t</span></code> to the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command</p></li>
<li><p>documented <a class="reference internal" href="index.html#document-topics/autothrottle"><span class="doc">AutoThrottle extension</span></a> and added to extensions installed by default. You still need to enable it with <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></p></li>
<li><p>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<code class="docutils literal notranslate"><span class="pre">stats_spider_opened</span></code>, etc). Stats are much simpler now, backward compatibility is kept on the Stats Collector API and signals.</p></li>
<li><p>added <a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_start_requests()</span></code></a> method to spider middlewares</p></li>
<li><p>dropped Signals singleton. Signals should now be accessed through the Crawler.signals attribute. See the signals documentation for more info.</p></li>
<li><p>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</p></li>
<li><p>documented <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-api"><span class="std std-ref">Core API</span></a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lxml</span></code> is now the default selectors backend instead of <code class="docutils literal notranslate"><span class="pre">libxml2</span></code></p></li>
<li><p>ported FormRequest.from_response() to use <a class="reference external" href="https://lxml.de/">lxml</a> instead of <a class="reference external" href="https://pypi.org/project/ClientForm/">ClientForm</a></p></li>
<li><p>removed modules: <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.BeautifulSoup</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.ClientForm</span></code></p></li>
<li><p>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10ed28b">commit 10ed28b</a>)</p></li>
<li><p>StackTraceDump extension: also dump trackref live references (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit fe2ce93</a>)</p></li>
<li><p>nested items now fully supported in JSON and JSONLines exporters</p></li>
<li><p>added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> Request meta key to support multiple cookie sessions per spider</p></li>
<li><p>decoupled encoding detection code to <a class="reference external" href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>, and ported Scrapy code to use that module</p></li>
<li><p>dropped support for Python 2.5. See <a class="reference external" href="https://www.zyte.com/blog/scrapy-0-15-dropping-support-for-python-2-5/">https://www.zyte.com/blog/scrapy-0-15-dropping-support-for-python-2-5/</a></p></li>
<li><p>dropped support for Twisted 2.5</p></li>
<li><p>added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REFERER_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REFERER_ENABLED</span></code></a> setting, to control referer middleware</p></li>
<li><p>changed default user agent to: <code class="docutils literal notranslate"><span class="pre">Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)</span></code></p></li>
<li><p>removed (undocumented) <code class="docutils literal notranslate"><span class="pre">HTMLImageLinkExtractor</span></code> class from <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.linkextractors.image</span></code></p></li>
<li><p>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">USER_AGENT</span></code> spider attribute will no longer work, use <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> attribute instead</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code> spider attribute will no longer work, use <code class="docutils literal notranslate"><span class="pre">download_timeout</span></code> attribute instead</p></li>
<li><p>removed <code class="docutils literal notranslate"><span class="pre">ENCODING_ALIASES</span></code> setting, as encoding auto-detection has been moved to the <a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> library</p></li>
<li><p>promoted <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-djangoitem"><span class="std std-ref">DjangoItem</span></a> to main contrib</p></li>
<li><p>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/164">issue 164</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit dcef7b0</a>)</p></li>
<li><p>downloader handlers (<a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> setting) now receive settings as the first argument of the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p>replaced memory usage accounting with (more portable) <a class="reference external" href="https://docs.python.org/2/library/resource.html">resource</a> module, removed <code class="docutils literal notranslate"><span class="pre">scrapy.utils.memory</span></code> module</p></li>
<li><p>removed signal: <code class="docutils literal notranslate"><span class="pre">scrapy.mail.mail_sent</span></code></p></li>
<li><p>removed <code class="docutils literal notranslate"><span class="pre">TRACK_REFS</span></code> setting, now <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">trackrefs</span></a> is always enabled</p></li>
<li><p>DBM is now the default storage backend for HTTP cache middleware</p></li>
<li><p>number of log messages (per level) are now tracked through Scrapy stats (stat name: <code class="docutils literal notranslate"><span class="pre">log_count/LEVEL</span></code>)</p></li>
<li><p>number received responses are now tracked through Scrapy stats (stat name: <code class="docutils literal notranslate"><span class="pre">response_received_count</span></code>)</p></li>
<li><p>removed <code class="docutils literal notranslate"><span class="pre">scrapy.log.started</span></code> attribute</p></li>
</ul>
</section>
<section id="scrapy-0-14-4">
<h4>Scrapy 0.14.4<a class="headerlink" href="#scrapy-0-14-4" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>added precise to supported Ubuntu distros (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7e46df">commit b7e46df</a>)</p></li>
<li><p>fixed bug in json-rpc webservice reported in <a class="reference external" href="https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported ‘run’ command from extras/scrapy-ws.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/340fbdb">commit 340fbdb</a>)</p></li>
<li><p>meta tag attributes for content-type http equiv can be in any order. #123 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0cb68af">commit 0cb68af</a>)</p></li>
<li><p>replace “import Image” by more standard “from PIL import Image”. closes #88 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4d17048">commit 4d17048</a>)</p></li>
<li><p>return trial status as bin/runtests.sh exit value. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit b7b2e7f</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-14-3">
<h4>Scrapy 0.14.3<a class="headerlink" href="#scrapy-0-14-3" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>forgot to include pydispatch license. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit fd85f9c</a>)</p></li>
<li><p>include egg files used by testsuite in source distribution. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c897793">commit c897793</a>)</p></li>
<li><p>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2548dcc">commit 2548dcc</a>)</p></li>
<li><p>added note to docs/topics/firebug.rst about google directory being shut down (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/668e352">commit 668e352</a>)</p></li>
<li><p>don’t discard slot when empty, just save in another dict in order to recycle if needed again. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e9f607">commit 8e9f607</a>)</p></li>
<li><p>do not fail handling unicode xpaths in libxml2 backed selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b830e95">commit b830e95</a>)</p></li>
<li><p>fixed minor mistake in Request objects documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit bf3c9ee</a>)</p></li>
<li><p>fixed minor defect in link extractors documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ba14f38">commit ba14f38</a>)</p></li>
<li><p>removed some obsolete remaining code related to sqlite support in Scrapy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0665175">commit 0665175</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-14-2">
<h4>Scrapy 0.14.2<a class="headerlink" href="#scrapy-0-14-2" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>move buffer pointing to start of file before computing checksum. refs #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit 6a5bef2</a>)</p></li>
<li><p>Compute image checksum before persisting images. closes #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9817df1">commit 9817df1</a>)</p></li>
<li><p>remove leaking references in cached failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/673a120">commit 673a120</a>)</p></li>
<li><p>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/11133e9">commit 11133e9</a>)</p></li>
<li><p>fixed struct.error on http compression middleware. closes #87 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1423140">commit 1423140</a>)</p></li>
<li><p>ajax crawling wasn’t expanding for unicode urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit 0de3fb4</a>)</p></li>
<li><p>Catch start_requests iterator errors. refs #83 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/454a21d">commit 454a21d</a>)</p></li>
<li><p>Speed-up libxml2 XPathSelector (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2fbd662">commit 2fbd662</a>)</p></li>
<li><p>updated versioning doc according to recent changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a070f5">commit 0a070f5</a>)</p></li>
<li><p>scrapyd: fixed documentation link (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit 2b4e4c3</a>)</p></li>
<li><p>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-14-1">
<h4>Scrapy 0.14.1<a class="headerlink" href="#scrapy-0-14-1" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</p></li>
<li><p>bumped version to 0.14.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit 6cb9e1c</a>)</p></li>
<li><p>fixed reference to tutorial directory (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit 4b86bd6</a>)</p></li>
<li><p>doc: removed duplicated callback argument from Request.replace() (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit 1aeccdd</a>)</p></li>
<li><p>fixed formatting of scrapyd doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit 8bf19e6</a>)</p></li>
<li><p>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit 14a8e6e</a>)</p></li>
<li><p>added comment about why we disable ssl on boto images upload (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5223575">commit 5223575</a>)</p></li>
<li><p>SSL handshaking hangs when doing too many parallel connections to S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/63d583d">commit 63d583d</a>)</p></li>
<li><p>change tutorial to follow changes on dmoz site (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bcb3198">commit bcb3198</a>)</p></li>
<li><p>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98f3f87">commit 98f3f87</a>)</p></li>
<li><p>allow spider to set autothrottle max concurrency (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/175a4b5">commit 175a4b5</a>)</p></li>
</ul>
</section>
<section id="scrapy-0-14">
<h4>Scrapy 0.14<a class="headerlink" href="#scrapy-0-14" title="Permalink to this heading">¶</a></h4>
<section id="new-features-and-settings">
<h5>New features and settings<a class="headerlink" href="#new-features-and-settings" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Support for AJAX crawlable urls</p></li>
<li><p>New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>)</p></li>
<li><p>added <code class="docutils literal notranslate"><span class="pre">-o</span></code> option to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>, a shortcut for dumping scraped items into a file (or standard output using <code class="docutils literal notranslate"><span class="pre">-</span></code>)</p></li>
<li><p>Added support for passing custom settings to Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> api (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>)</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">ChunkedTransferMiddleware</span></code> (enabled by default) to support <a class="reference external" href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>)</p></li>
<li><p>Add boto 2.0 support for S3 downloader handler (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>)</p></li>
<li><p>Added <a class="reference external" href="https://docs.python.org/2/library/marshal.html">marshal</a> to formats supported by feed exports (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>)</p></li>
<li><p>In request errbacks, offending requests are now received in <code class="docutils literal notranslate"><span class="pre">failure.request</span></code> attribute (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>)</p></li>
<li><dl class="simple">
<dt>Big downloader refactoring to support per domain/ip concurrency limits (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2732">r2732</a>)</dt><dd><ul>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> setting has been deprecated and replaced by:</dt><dd><ul>
<li><p><a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>, <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>check the documentation for more details</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Added builtin caching DNS resolver (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>)</p></li>
<li><p>Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class="reference external" href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>)</p></li>
<li><p>Moved spider queues to scrapyd: <code class="docutils literal notranslate"><span class="pre">scrapy.spiderqueue</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">scrapyd.spiderqueue</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>)</p></li>
<li><p>Moved sqlite utils to scrapyd: <code class="docutils literal notranslate"><span class="pre">scrapy.utils.sqlite</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">scrapyd.sqlite</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>)</p></li>
<li><p>Real support for returning iterators on <code class="docutils literal notranslate"><span class="pre">start_requests()</span></code> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_ENABLED</span></code></a> setting to quickly enable/disable the redirect middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a> setting to quickly enable/disable the retry middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">CloseSpider</span></code> exception to manually close spiders (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>)</p></li>
<li><p>Improved encoding detection by adding support for HTML5 meta charset declaration (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>)</p></li>
<li><p>Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code> (see documentation in Spiders page) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">LogStats</span></code> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>)</p></li>
<li><p>Make handling of gzipped responses more robust (#319, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <code class="docutils literal notranslate"><span class="pre">IOError</span></code>.</p></li>
<li><p>Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>)</p></li>
<li><p>Added new command to edit spiders: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">edit</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>) and <code class="docutils literal notranslate"><span class="pre">-e</span></code> flag to <code class="docutils literal notranslate"><span class="pre">genspider</span></code> command that uses it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>)</p></li>
<li><p>Changed default representation of items to pretty-printed dicts. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-spider_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_error</span></code></a> signal (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>)</p></li>
<li><p>Stats are now dumped to Scrapy log (default value of <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-STATS_DUMP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_DUMP</span></code></a> setting has been changed to <code class="docutils literal notranslate"><span class="pre">True</span></code>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</p></li>
<li><p>Added support for dynamically adjusting download delay and maximum concurrent requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>)</p></li>
<li><p>Added new DBM HTTP cache storage backend (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">listjobs.json</span></code> API to Scrapyd (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CsvItemExporter</span></code>: added <code class="docutils literal notranslate"><span class="pre">join_multivalued</span></code> parameter (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>)</p></li>
<li><p>Added namespace support to <code class="docutils literal notranslate"><span class="pre">xmliter_lxml</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>)</p></li>
<li><p>Improved cookies middleware by making <code class="docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code> nicer and documenting it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>)</p></li>
<li><p>Several improvements to Scrapyd and Link extractors</p></li>
</ul>
</section>
<section id="code-rearranged-and-removed">
<h5>Code rearranged and removed<a class="headerlink" href="#code-rearranged-and-removed" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><dl class="simple">
<dt>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2630">r2630</a>)</dt><dd><ul>
<li><p>original item_scraped signal was removed</p></li>
<li><p>original item_passed signal was renamed to item_scraped</p></li>
<li><p>old log lines <code class="docutils literal notranslate"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> were removed</p></li>
<li><p>old log lines <code class="docutils literal notranslate"><span class="pre">Passed</span> <span class="pre">Item...</span></code> were renamed to <code class="docutils literal notranslate"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> lines and downgraded to <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:</dt><dd><ul>
<li><p><a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> (several functions from <code class="docutils literal notranslate"><span class="pre">scrapy.utils.{http,markup,multipart,response,url}</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2584">r2584</a>)</p></li>
<li><p><a class="reference external" href="https://github.com/scrapy/scrapely">scrapely</a> (was <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.ibl</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2586">r2586</a>)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Removed unused function: <code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_info()</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>)</p></li>
<li><p>Removed googledir project from <code class="docutils literal notranslate"><span class="pre">examples/googledir</span></code>. There’s now a new example project called <code class="docutils literal notranslate"><span class="pre">dirbot</span></code> available on GitHub: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></p></li>
<li><p>Removed support for default field values in Scrapy items (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>)</p></li>
<li><p>Removed experimental crawlspider v2 (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>)</p></li>
<li><p>Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe filtering class as before (<code class="docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code> setting) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>)</p></li>
<li><p>Removed support for passing urls to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code> command (use <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p></li>
<li><p>Removed deprecated Execution Queue (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</p></li>
<li><p>Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>)</p></li>
<li><p>removed <code class="docutils literal notranslate"><span class="pre">CONCURRENT_SPIDERS</span></code> setting (use scrapyd maxproc instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>)</p></li>
<li><p>Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>)</p></li>
<li><p>Renamed setting <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMPASSED</span></code> to <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>). Backward compatibility kept.</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-12">
<h4>Scrapy 0.12<a class="headerlink" href="#scrapy-0-12" title="Permalink to this heading">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<section id="new-features-and-improvements">
<h5>New features and improvements<a class="headerlink" href="#new-features-and-improvements" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Passed item is now sent in the <code class="docutils literal notranslate"><span class="pre">item</span></code> argument of the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-signal-item_scraped"><code class="xref std std-signal docutils literal notranslate"><span class="pre">item_passed</span></code></a> (#273)</p></li>
<li><p>Added verbose option to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code> command, useful for bug reports (#298)</p></li>
<li><p>HTTP cache now stored by default in the project data dir (#279)</p></li>
<li><p>Added project data storage directory (#276, #277)</p></li>
<li><p>Documented file structure of Scrapy projects (see command-line tool doc)</p></li>
<li><p>New lxml backend for XPath selectors (#147)</p></li>
<li><p>Per-spider settings (#245)</p></li>
<li><p>Support exit codes to signal errors in Scrapy commands (#248)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">-c</span></code> argument to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span></code> command</p></li>
<li><p>Made <code class="docutils literal notranslate"><span class="pre">libxml2</span></code> optional (#260)</p></li>
<li><p>New <code class="docutils literal notranslate"><span class="pre">deploy</span></code> command (#261)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a> setting (#253)</p></li>
<li><p>Added <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#std-setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a> setting (#254)</p></li>
</ul>
</section>
<section id="scrapyd-changes">
<h5>Scrapyd changes<a class="headerlink" href="#scrapyd-changes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Scrapyd now uses one process per spider</p></li>
<li><p>It stores one log file per spider run, and rotate them keeping the latest 5 logs per spider (by default)</p></li>
<li><p>A minimal web ui was added, available at <a class="reference external" href="http://localhost:6800">http://localhost:6800</a> by default</p></li>
<li><p>There is now a <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">server</span></code> command to start a Scrapyd server of the current project</p></li>
</ul>
</section>
<section id="changes-to-settings">
<h5>Changes to settings<a class="headerlink" href="#changes-to-settings" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>added <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_ENABLED</span></code> setting (False by default) to enable HTTP cache middleware</p></li>
<li><p>changed <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_EXPIRATION_SECS</span></code> semantics: now zero means “never expire”.</p></li>
</ul>
</section>
<section id="deprecated-obsoleted-functionality">
<h5>Deprecated/obsoleted functionality<a class="headerlink" href="#deprecated-obsoleted-functionality" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Deprecated <code class="docutils literal notranslate"><span class="pre">runserver</span></code> command in favor of <code class="docutils literal notranslate"><span class="pre">server</span></code> command which starts a Scrapyd server. See also: Scrapyd changes</p></li>
<li><p>Deprecated <code class="docutils literal notranslate"><span class="pre">queue</span></code> command in favor of using Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> API. See also: Scrapyd changes</p></li>
<li><p>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-10">
<h4>Scrapy 0.10<a class="headerlink" href="#scrapy-0-10" title="Permalink to this heading">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<section id="id153">
<h5>New features and improvements<a class="headerlink" href="#id153" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>New Scrapy service called <code class="docutils literal notranslate"><span class="pre">scrapyd</span></code> for deploying Scrapy crawlers in production (#218) (documentation available)</p></li>
<li><p>Simplified Images pipeline usage which doesn’t require subclassing your own images pipeline now (#217)</p></li>
<li><p>Scrapy shell now shows the Scrapy log by default (#206)</p></li>
<li><p>Refactored execution queue in a common base code and pluggable backends called “spider queues” (#220)</p></li>
<li><p>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</p></li>
<li><p>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</p></li>
<li><p>Feed exporters with pluggable backends (#197) (documentation available)</p></li>
<li><p>Deferred signals (#193)</p></li>
<li><p>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</p></li>
<li><p>Support for overriding default request headers per spider (#181)</p></li>
<li><p>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</p></li>
<li><p>Split Debian package into two packages - the library and the service (#187)</p></li>
<li><p>Scrapy log refactoring (#188)</p></li>
<li><p>New extension for keeping persistent spider contexts among different runs (#203)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">dont_redirect</span></code> request.meta key for avoiding redirects (#233)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> request.meta key for avoiding retries (#234)</p></li>
</ul>
</section>
<section id="command-line-tool-changes">
<h5>Command-line tool changes<a class="headerlink" href="#command-line-tool-changes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>New <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command which replaces the old <code class="docutils literal notranslate"><span class="pre">scrapy-ctl.py</span></code> (#199)
- there is only one global <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command now, instead of one <code class="docutils literal notranslate"><span class="pre">scrapy-ctl.py</span></code> per project
- Added <code class="docutils literal notranslate"><span class="pre">scrapy.bat</span></code> script for running more conveniently from Windows</p></li>
<li><p>Added bash completion to command-line tool (#210)</p></li>
<li><p>Renamed command <code class="docutils literal notranslate"><span class="pre">start</span></code> to <code class="docutils literal notranslate"><span class="pre">runserver</span></code> (#209)</p></li>
</ul>
</section>
<section id="api-changes">
<h5>API changes<a class="headerlink" href="#api-changes" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">url</span></code> and <code class="docutils literal notranslate"><span class="pre">body</span></code> attributes of Request objects are now read-only (#230)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Request.copy()</span></code> and <code class="docutils literal notranslate"><span class="pre">Request.replace()</span></code> now also copies their <code class="docutils literal notranslate"><span class="pre">callback</span></code> and <code class="docutils literal notranslate"><span class="pre">errback</span></code> attributes (#231)</p></li>
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">UrlFilterMiddleware</span></code> from <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> (already disabled by default)</p></li>
<li><p>Offsite middleware doesn’t filter out any request coming from a spider that doesn’t have a allowed_domains attribute (#225)</p></li>
<li><p>Removed Spider Manager <code class="docutils literal notranslate"><span class="pre">load()</span></code> method. Now spiders are loaded in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method itself.</p></li>
<li><dl class="simple">
<dt>Changes to Scrapy Manager (now called “Crawler”):</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.core.manager.ScrapyManager</span></code> class renamed to <code class="docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.core.manager.scrapymanager</span></code> singleton moved to <code class="docutils literal notranslate"><span class="pre">scrapy.project.crawler</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Moved module: <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.spidermanager</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.spidermanager</span></code></p></li>
<li><p>Spider Manager singleton moved from <code class="docutils literal notranslate"><span class="pre">scrapy.spider.spiders</span></code> to the <code class="docutils literal notranslate"><span class="pre">spiders`</span> <span class="pre">attribute</span> <span class="pre">of</span> <span class="pre">``scrapy.project.crawler</span></code> singleton.</p></li>
<li><dl class="simple">
<dt>moved Stats Collector classes: (#204)</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.stats.collector.StatsCollector</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.statscol.StatsCollector</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.stats.collector.SimpledbStatsCollector</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.statscol.SimpledbStatsCollector</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>default per-command settings are now specified in the <code class="docutils literal notranslate"><span class="pre">default_settings</span></code> attribute of command object class (#201)</p></li>
<li><dl class="simple">
<dt>changed arguments of Item pipeline <code class="docutils literal notranslate"><span class="pre">process_item()</span></code> method from <code class="docutils literal notranslate"><span class="pre">(spider,</span> <span class="pre">item)</span></code> to <code class="docutils literal notranslate"><span class="pre">(item,</span> <span class="pre">spider)</span></code></dt><dd><ul>
<li><p>backward compatibility kept (with deprecation warning)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>moved <code class="docutils literal notranslate"><span class="pre">scrapy.core.signals</span></code> module to <code class="docutils literal notranslate"><span class="pre">scrapy.signals</span></code></dt><dd><ul>
<li><p>backward compatibility kept (with deprecation warning)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>moved <code class="docutils literal notranslate"><span class="pre">scrapy.core.exceptions</span></code> module to <code class="docutils literal notranslate"><span class="pre">scrapy.exceptions</span></code></dt><dd><ul>
<li><p>backward compatibility kept (with deprecation warning)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>added <code class="docutils literal notranslate"><span class="pre">handles_request()</span></code> class method to <code class="docutils literal notranslate"><span class="pre">BaseSpider</span></code></p></li>
<li><p>dropped <code class="docutils literal notranslate"><span class="pre">scrapy.log.exc()</span></code> function (use <code class="docutils literal notranslate"><span class="pre">scrapy.log.err()</span></code> instead)</p></li>
<li><p>dropped <code class="docutils literal notranslate"><span class="pre">component</span></code> argument of <code class="docutils literal notranslate"><span class="pre">scrapy.log.msg()</span></code> function</p></li>
<li><p>dropped <code class="docutils literal notranslate"><span class="pre">scrapy.log.log_level</span></code> attribute</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> class methods to Spider Manager, and Item Pipeline Manager</p></li>
</ul>
</section>
<section id="id154">
<h5>Changes to settings<a class="headerlink" href="#id154" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_IGNORE_SCHEMES</span></code> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">SPIDER_QUEUE_CLASS</span></code> setting which defines the spider queue to use (#220)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">KEEP_ALIVE</span></code> setting (#220)</p></li>
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">SERVICE_QUEUE</span></code> setting (#220)</p></li>
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">COMMANDS_SETTINGS_MODULE</span></code> setting (#201)</p></li>
<li><p>Renamed <code class="docutils literal notranslate"><span class="pre">REQUEST_HANDLERS</span></code> to <code class="docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code> and make download handlers classes (instead of functions)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-9">
<h4>Scrapy 0.9<a class="headerlink" href="#scrapy-0-9" title="Permalink to this heading">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<section id="id155">
<h5>New features and improvements<a class="headerlink" href="#id155" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added SMTP-AUTH support to scrapy.mail</p></li>
<li><p>New settings added: <code class="docutils literal notranslate"><span class="pre">MAIL_USER</span></code>, <code class="docutils literal notranslate"><span class="pre">MAIL_PASS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a> | #149)</p></li>
<li><p>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</p></li>
<li><p>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a> | #167)</p></li>
<li><p>Support for running Scrapy as a service, for production systems (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a> | #168)</p></li>
<li><p>Added wrapper induction library (documentation only available in source code for now). (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>)</p></li>
<li><p>Simplified and improved response encoding support (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>, documentation available)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code> setting (enabled by default) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>, doc available)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MailSender</span></code> is no longer IO-blocking (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a> | #146)</p></li>
<li><p>Linkextractors and new Crawlspider now handle relative base tag urls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a> | #148)</p></li>
<li><p>Several improvements to Item Loaders and processors (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>)</p></li>
<li><p>Added support for adding variables to telnet console (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a> | #165)</p></li>
<li><p>Support for requests without callbacks (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a> | #166)</p></li>
</ul>
</section>
<section id="id156">
<h5>API changes<a class="headerlink" href="#id156" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Change <code class="docutils literal notranslate"><span class="pre">Spider.domain_name</span></code> to <code class="docutils literal notranslate"><span class="pre">Spider.name</span></code> (SEP-012, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Response.encoding</span></code> is now the detected encoding (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code> now returns None or raises an exception (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a> | #157)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scrapy.command</span></code> modules relocation (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">ExecutionQueue</span></code> for feeding spiders to scrape (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>)</p></li>
<li><p>Removed <code class="docutils literal notranslate"><span class="pre">ExecutionEngine</span></code> singleton (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</p></li>
<li><p>Ported <code class="docutils literal notranslate"><span class="pre">S3ImagesStore</span></code> (images pipeline) to use boto and threads (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>)</p></li>
<li><p>Moved module: <code class="docutils literal notranslate"><span class="pre">scrapy.management.telnet</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.telnet</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>)</p></li>
</ul>
</section>
<section id="changes-to-default-settings">
<h5>Changes to default settings<a class="headerlink" href="#changes-to-default-settings" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Changed default <code class="docutils literal notranslate"><span class="pre">SCHEDULER_ORDER</span></code> to <code class="docutils literal notranslate"><span class="pre">DFO</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-8">
<h4>Scrapy 0.8<a class="headerlink" href="#scrapy-0-8" title="Permalink to this heading">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<section id="id157">
<h5>New features<a class="headerlink" href="#id157" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Added DEFAULT_RESPONSE_ENCODING setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">dont_click</span></code> argument to <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>)</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">clickdata</span></code> argument to <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>)</p></li>
<li><p>Added support for HTTP proxies (<code class="docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>)</p></li>
<li><p>Offsite spider middleware now logs messages when filtering out requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>)</p></li>
</ul>
</section>
<section id="id158">
<h5>Backward-incompatible changes<a class="headerlink" href="#id158" title="Permalink to this heading">¶</a></h5>
<ul class="simple">
<li><p>Changed <code class="docutils literal notranslate"><span class="pre">scrapy.utils.response.get_meta_refresh()</span></code> signature (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>)</p></li>
<li><p>Removed deprecated <code class="docutils literal notranslate"><span class="pre">scrapy.item.ScrapedItem</span></code> class - use <code class="docutils literal notranslate"><span class="pre">scrapy.item.Item</span> <span class="pre">instead</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>)</p></li>
<li><p>Removed deprecated <code class="docutils literal notranslate"><span class="pre">scrapy.xpath</span></code> module - use <code class="docutils literal notranslate"><span class="pre">scrapy.selector</span></code> instead. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>)</p></li>
<li><p>Removed deprecated <code class="docutils literal notranslate"><span class="pre">core.signals.domain_open</span></code> signal - use <code class="docutils literal notranslate"><span class="pre">core.signals.domain_opened</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</p></li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">log.msg()</span></code> now receives a <code class="docutils literal notranslate"><span class="pre">spider</span></code> argument (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</dt><dd><ul>
<li><p>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <code class="docutils literal notranslate"><span class="pre">spider</span></code> argument and pass spider references. If you really want to pass a string, use the <code class="docutils literal notranslate"><span class="pre">component</span></code> argument instead.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Changed core signals <code class="docutils literal notranslate"><span class="pre">domain_opened</span></code>, <code class="docutils literal notranslate"><span class="pre">domain_closed</span></code>, <code class="docutils literal notranslate"><span class="pre">domain_idle</span></code></p></li>
<li><dl class="simple">
<dt>Changed Item pipeline to use spiders instead of domains</dt><dd><ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">domain</span></code> argument of  <code class="docutils literal notranslate"><span class="pre">process_item()</span></code> item pipeline method was changed to  <code class="docutils literal notranslate"><span class="pre">spider</span></code>, the new signature is: <code class="docutils literal notranslate"><span class="pre">process_item(spider,</span> <span class="pre">item)</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1827">r1827</a> | #105)</p></li>
<li><p>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal notranslate"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal notranslate"><span class="pre">domain</span></code>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Changed Stats API to use spiders instead of domains (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1849">r1849</a> | #113)</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">StatsCollector</span></code> was changed to receive spider references (instead of domains) in its methods (<code class="docutils literal notranslate"><span class="pre">set_value</span></code>, <code class="docutils literal notranslate"><span class="pre">inc_value</span></code>, etc).</p></li>
<li><p>added <code class="docutils literal notranslate"><span class="pre">StatsCollector.iter_spider_stats()</span></code> method</p></li>
<li><p>removed <code class="docutils literal notranslate"><span class="pre">StatsCollector.list_domains()</span></code> method</p></li>
<li><p>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here’s a summary of the changes:</p></li>
<li><p>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal notranslate"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal notranslate"><span class="pre">domain</span></code>. <code class="docutils literal notranslate"><span class="pre">spider_stats</span></code> contains exactly the same data as <code class="docutils literal notranslate"><span class="pre">domain_stats</span></code>.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">CloseDomain</span></code> extension moved to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.closespider.CloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1833">r1833</a>)</dt><dd><ul>
<li><dl class="simple">
<dt>Its settings were also renamed:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">CLOSEDOMAIN_TIMEOUT</span></code> to <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CLOSEDOMAIN_ITEMCOUNT</span></code> to <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Removed deprecated <code class="docutils literal notranslate"><span class="pre">SCRAPYSETTINGS_MODULE</span></code> environment variable - use <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>)</p></li>
<li><p>Renamed setting: <code class="docutils literal notranslate"><span class="pre">REQUESTS_PER_DOMAIN</span></code> to <code class="docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>)</p></li>
<li><p>Renamed setting: <code class="docutils literal notranslate"><span class="pre">CONCURRENT_DOMAINS</span></code> to <code class="docutils literal notranslate"><span class="pre">CONCURRENT_SPIDERS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>)</p></li>
<li><p>Refactored HTTP Cache middleware</p></li>
<li><p>HTTP Cache middleware has been heavily refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a> )</p></li>
<li><p>Renamed exception: <code class="docutils literal notranslate"><span class="pre">DontCloseDomain</span></code> to <code class="docutils literal notranslate"><span class="pre">DontCloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a> | #120)</p></li>
<li><p>Renamed extension: <code class="docutils literal notranslate"><span class="pre">DelayedCloseDomain</span></code> to <code class="docutils literal notranslate"><span class="pre">SpiderCloseDelay</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a> | #121)</p></li>
<li><p>Removed obsolete <code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup.remove_escape_chars</span></code> function - use <code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup.replace_escape_chars</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>)</p></li>
</ul>
</section>
</section>
<section id="scrapy-0-7">
<h4>Scrapy 0.7<a class="headerlink" href="#scrapy-0-7" title="Permalink to this heading">¶</a></h4>
<p>First release of Scrapy.</p>
</section>
</section>
<span id="document-contributing"></span><section id="contributing-to-scrapy">
<span id="topics-contributing"></span><h3>Contributing to Scrapy<a class="headerlink" href="#contributing-to-scrapy" title="Permalink to this heading">¶</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Double check that you are reading the most recent version of this document at
<a class="reference external" href="https://docs.scrapy.org/en/master/contributing.html">https://docs.scrapy.org/en/master/contributing.html</a></p>
</div>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul class="simple">
<li><p>Report bugs and request features in the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>, trying to follow
the guidelines detailed in <a class="reference internal" href="#reporting-bugs">Reporting bugs</a> below.</p></li>
<li><p>Submit patches for new functionalities and/or bug fixes. Please read
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#writing-patches"><span class="std std-ref">Writing patches</span></a> and <a class="reference internal" href="#id2">Submitting patches</a> below for details on how to
write and submit a patch.</p></li>
<li><p>Blog about Scrapy. Tell the world how you’re using Scrapy. This will help
newcomers with more examples and will help the Scrapy project to increase its
visibility.</p></li>
<li><p>Join the <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> and share your ideas on how to
improve Scrapy. We’re always open to suggestions.</p></li>
<li><p>Answer Scrapy questions at
<a class="reference external" href="https://stackoverflow.com/questions/tagged/scrapy">Stack Overflow</a>.</p></li>
</ul>
<section id="reporting-bugs">
<h4>Reporting bugs<a class="headerlink" href="#reporting-bugs" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security&#37;&#52;&#48;googlegroups&#46;com">scrapy-security<span>&#64;</span>googlegroups<span>&#46;</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
</div>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when you’re going to report a new bug.</p>
<ul class="simple">
<li><p>check the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#faq"><span class="std std-ref">FAQ</span></a> first to see if your issue is addressed in a
well-known question</p></li>
<li><p>if you have a general question about Scrapy usage, please ask it at
<a class="reference external" href="https://stackoverflow.com/questions/tagged/scrapy">Stack Overflow</a>
(use “scrapy” tag).</p></li>
<li><p>check the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">open issues</a> to see if the issue has already been reported. If it
has, don’t dismiss the report, but check the ticket history and comments. If
you have additional useful information, please leave a comment, or consider
<a class="hxr-hoverxref hxr-tooltip reference internal" href="#writing-patches"><span class="std std-ref">sending a pull request</span></a> with a fix.</p></li>
<li><p>search the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a> list and <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> to see if it has
been discussed there, or if you’re not sure if what you’re seeing is a bug.
You can also ask in the <code class="docutils literal notranslate"><span class="pre">#scrapy</span></code> IRC channel.</p></li>
<li><p>write <strong>complete, reproducible, specific bug reports</strong>. The smaller the test
case, the better. Remember that other developers won’t have your project to
reproduce the bug, so please include all relevant files required to reproduce
it. See for example StackOverflow’s guide on creating a
<a class="reference external" href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable example</a> exhibiting the issue.</p></li>
<li><p>the most awesome way to provide a complete reproducible example is to
send a pull request which adds a failing test case to the
Scrapy testing suite (see <a class="hxr-hoverxref hxr-tooltip reference internal" href="#submitting-patches"><span class="std std-ref">Submitting patches</span></a>).
This is helpful even if you don’t have an intention to
fix the issue yourselves.</p></li>
<li><p>include the output of <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">-v</span></code> so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.</p></li>
</ul>
</section>
<section id="writing-patches">
<span id="id1"></span><h4>Writing patches<a class="headerlink" href="#writing-patches" title="Permalink to this heading">¶</a></h4>
<p>Scrapy has a list of <a class="reference external" href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first issues</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">help wanted issues</a> that you
can work on. These issues are a great way to get started with contributing to
Scrapy. If you’re new to the codebase, you may want to focus on documentation
or testing-related issues, as they are always useful and can help you get
more familiar with the project. You can also check Scrapy’s <a class="reference external" href="https://app.codecov.io/gh/scrapy/scrapy">test coverage</a>
to see which areas may benefit from more tests.</p>
<p>The better a patch is written, the higher the chances that it’ll get accepted and the sooner it will be merged.</p>
<p>Well-written patches should:</p>
<ul>
<li><p>contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you’re doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.</p></li>
<li><p>pass all unit-tests. See <a class="reference internal" href="#id6">Running tests</a> below.</p></li>
<li><p>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a class="reference internal" href="#writing-tests">Writing tests</a> below.</p></li>
<li><p>if you’re adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See <a class="reference internal" href="#id5">Documentation policies</a>
below.</p></li>
<li><p>if you’re adding a private API, please add a regular expression to the
<code class="docutils literal notranslate"><span class="pre">coverage_ignore_pyobjects</span></code> variable of <code class="docutils literal notranslate"><span class="pre">docs/conf.py</span></code> to exclude the new
private API from documentation coverage checks.</p>
<p>To see if your private API is skipped properly, generate a documentation
coverage report as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">docs</span><span class="o">-</span><span class="n">coverage</span>
</pre></div>
</div>
</li>
<li><p>if you are removing deprecated code, first make sure that at least 1 year
(12 months) has passed since the release that introduced the deprecation.
See <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#deprecation-policy"><span class="std std-ref">Deprecation policy</span></a>.</p></li>
</ul>
</section>
<section id="submitting-patches">
<span id="id2"></span><h4>Submitting patches<a class="headerlink" href="#submitting-patches" title="Permalink to this heading">¶</a></h4>
<p>The best way to submit a patch is to issue a <a class="reference external" href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request">pull request</a> on GitHub,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it’s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it’s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on GitHub. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can start a
conversation in the <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> to discuss your idea first.</p>
<p>Sometimes there is an existing pull request for the problem you’d like to
solve, which is stalled for some reason. Often the pull request is in a
right direction, but changes are requested by Scrapy maintainers, and the
original pull request author hasn’t had time to address them.
In this case consider picking up this pull request: open
a new pull request with all commits from the original pull request, as well as
additional changes to address the raised issues. Doing so helps a lot; it is
not considered rude as long as the original author is acknowledged by keeping
his/her commits.</p>
<p>You can pull an existing pull request to a local branch
by running <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">fetch</span> <span class="pre">upstream</span> <span class="pre">pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE</span></code>
(replace ‘upstream’ with a remote name for scrapy repository,
<code class="docutils literal notranslate"><span class="pre">$PR_NUMBER</span></code> with an ID of the pull request, and <code class="docutils literal notranslate"><span class="pre">$BRANCH_NAME_TO_CREATE</span></code>
with a name of the branch you want to create locally).
See also: <a class="reference external" href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally">https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally</a>.</p>
<p>When writing GitHub pull requests, try to keep titles short but descriptive.
E.g. For bug #411: “Scrapy hangs if an exception raises in start_requests”
prefer “Fix hanging when exception occurs in start_requests (#411)”
instead of “Fix for #411”. Complete titles make it easy to skim through
the issue tracker.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="https://peps.python.org/pep-0008/"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits from functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
</section>
<section id="coding-style">
<span id="id3"></span><h4>Coding style<a class="headerlink" href="#coding-style" title="Permalink to this heading">¶</a></h4>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<ul class="simple">
<li><p>We use <a class="reference external" href="https://black.readthedocs.io/en/stable/">black</a> for code formatting.
There is a hook in the pre-commit config
that will automatically format your code before every commit. You can also
run black manually with <code class="docutils literal notranslate"><span class="pre">tox</span> <span class="pre">-e</span> <span class="pre">pre-commit</span></code>.</p></li>
<li><p>Don’t put your name in the code you contribute; git provides enough
metadata to identify author of the code.
See <a class="reference external" href="https://docs.github.com/en/get-started/getting-started-with-git/setting-your-username-in-git">https://docs.github.com/en/get-started/getting-started-with-git/setting-your-username-in-git</a>
for setup instructions.</p></li>
</ul>
</section>
<section id="pre-commit">
<span id="scrapy-pre-commit"></span><h4>Pre-commit<a class="headerlink" href="#pre-commit" title="Permalink to this heading">¶</a></h4>
<p>We use <a class="reference external" href="https://pre-commit.com/">pre-commit</a> to automatically address simple code issues before every
commit.</p>
<p>After your create a local clone of your fork of the Scrapy repository:</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://pre-commit.com/#installation">Install pre-commit</a>.</p></li>
<li><p>On the root of your local clone of the Scrapy repository, run the following
command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pre-commit<span class="w"> </span>install
</pre></div>
</div>
</li>
</ol>
<p>Now pre-commit will check your changes every time you create a Git commit. Upon
finding issues, pre-commit aborts your commit, and either fixes those issues
automatically, or only reports them to you. If it fixes those issues
automatically, creating your commit again should succeed. Otherwise, you may
need to address the corresponding issues manually first.</p>
</section>
<section id="documentation-policies">
<span id="id5"></span><h4>Documentation policies<a class="headerlink" href="#documentation-policies" title="Permalink to this heading">¶</a></h4>
<p>For reference documentation of API members (classes, methods, etc.) use
docstrings and make sure that the Sphinx documentation uses the
<a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v8.2.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">autodoc</span></code></a> extension to pull the docstrings. API reference
documentation should follow docstring conventions (<a class="reference external" href="https://peps.python.org/pep-0257/">PEP 257</a>) and be
IDE-friendly: short, to the point, and it may provide short examples.</p>
<p>Other types of documentation, such as tutorials or topics, should be covered in
files within the <code class="docutils literal notranslate"><span class="pre">docs/</span></code> directory. This includes documentation that is
specific to an API member, but goes beyond API reference documentation.</p>
<p>In any case, if something is covered in a docstring, use the
<a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v8.2.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">autodoc</span></code></a> extension to pull the docstring into the
documentation instead of duplicating the docstring in files within the
<code class="docutils literal notranslate"><span class="pre">docs/</span></code> directory.</p>
<p>Documentation updates that cover new or modified features must use Sphinx’s
<a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v8.2.0)"><code class="xref rst rst-dir docutils literal notranslate"><span class="pre">versionadded</span></code></a> and <a class="reference external" href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v8.2.0)"><code class="xref rst rst-dir docutils literal notranslate"><span class="pre">versionchanged</span></code></a> directives. Use
<code class="docutils literal notranslate"><span class="pre">VERSION</span></code> as version, we will replace it with the actual version right before
the corresponding release. When we release a new major or minor version of
Scrapy, we remove these directives if they are older than 3 years.</p>
<p>Documentation about deprecated features must be removed as those features are
deprecated, so that new readers do not run into it. New deprecations and
deprecation removals are documented in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#news"><span class="std std-ref">release notes</span></a>.</p>
</section>
<section id="tests">
<h4>Tests<a class="headerlink" href="#tests" title="Permalink to this heading">¶</a></h4>
<p>Tests are implemented using the <a class="reference external" href="https://docs.twisted.org/en/stable/development/test-standard.html" title="(in Twisted v24.10)"><span class="xref std std-doc">Twisted unit-testing framework</span></a>. Running tests requires
<a class="reference external" href="https://tox.wiki/en/latest/index.html" title="(in Project name not set v4.23)"><span class="xref std std-doc">tox</span></a>.</p>
<section id="running-tests">
<span id="id6"></span><h5>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this heading">¶</a></h5>
<p>To run all tests:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span>
</pre></div>
</div>
<p>To run a specific test (say <code class="docutils literal notranslate"><span class="pre">tests/test_loader.py</span></code>) use:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">tox</span> <span class="pre">--</span> <span class="pre">tests/test_loader.py</span></code></p>
</div></blockquote>
<p>To run the tests on a specific <a class="reference external" href="https://tox.wiki/en/latest/index.html" title="(in Project name not set v4.23)"><span class="xref std std-doc">tox</span></a> environment, use
<code class="docutils literal notranslate"><span class="pre">-e</span> <span class="pre">&lt;name&gt;</span></code> with an environment name from <code class="docutils literal notranslate"><span class="pre">tox.ini</span></code>. For example, to run
the tests with Python 3.10 use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py310</span>
</pre></div>
</div>
<p>You can also specify a comma-separated list of environments, and use <a class="reference external" href="https://tox.wiki/en/latest/user_guide.html#parallel-mode" title="(in Project name not set v4.23)"><span class="xref std std-ref">tox’s
parallel mode</span></a> to run the tests on multiple environments in
parallel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py39</span><span class="p">,</span><span class="n">py310</span> <span class="o">-</span><span class="n">p</span> <span class="n">auto</span>
</pre></div>
</div>
<p>To pass command-line options to <a class="reference external" href="https://docs.pytest.org/en/latest/index.html" title="(in pytest v8.4.0.dev200)"><span class="xref std std-doc">pytest</span></a>, add them after
<code class="docutils literal notranslate"><span class="pre">--</span></code> in your call to <a class="reference external" href="https://tox.wiki/en/latest/index.html" title="(in Project name not set v4.23)"><span class="xref std std-doc">tox</span></a>. Using <code class="docutils literal notranslate"><span class="pre">--</span></code> overrides the
default positional arguments defined in <code class="docutils literal notranslate"><span class="pre">tox.ini</span></code>, so you must include those
default positional arguments (<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">tests</span></code>) after <code class="docutils literal notranslate"><span class="pre">--</span></code> as well:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">--</span> <span class="n">scrapy</span> <span class="n">tests</span> <span class="o">-</span><span class="n">x</span>  <span class="c1"># stop after first failure</span>
</pre></div>
</div>
<p>You can also use the <a class="reference external" href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a> plugin. For example, to run all tests on
the Python 3.10 <a class="reference external" href="https://tox.wiki/en/latest/index.html" title="(in Project name not set v4.23)"><span class="xref std std-doc">tox</span></a> environment using all your CPU cores:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py310</span> <span class="o">--</span> <span class="n">scrapy</span> <span class="n">tests</span> <span class="o">-</span><span class="n">n</span> <span class="n">auto</span>
</pre></div>
</div>
<p>To see coverage report install <a class="reference external" href="https://coverage.readthedocs.io/en/latest/index.html" title="(in Coverage.py v7.6.7)"><span class="xref std std-doc">coverage</span></a>
(<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">coverage</span></code>) and run:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">coverage</span> <span class="pre">report</span></code></p>
</div></blockquote>
<p>see output of <code class="docutils literal notranslate"><span class="pre">coverage</span> <span class="pre">--help</span></code> for more options like html or xml report.</p>
</section>
<section id="writing-tests">
<h5>Writing tests<a class="headerlink" href="#writing-tests" title="Permalink to this heading">¶</a></h5>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a> directory.
Their module name typically resembles the full path of the module they’re
testing. For example, the item loaders code is in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span><span class="o">.</span><span class="n">loader</span>
</pre></div>
</div>
<p>And their unit-tests are in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tests</span><span class="o">/</span><span class="n">test_loader</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-versioning"></span><section id="versioning-and-api-stability">
<span id="versioning"></span><h3>Versioning and API stability<a class="headerlink" href="#versioning-and-api-stability" title="Permalink to this heading">¶</a></h3>
<section id="id1">
<h4>Versioning<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h4>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul class="simple">
<li><p><em>A</em> is the major version. This will rarely change and will signify very
large changes.</p></li>
<li><p><em>B</em> is the release number. This will include many changes including features
and things that possibly break backward compatibility, although we strive to
keep these cases at a minimum.</p></li>
<li><p><em>C</em> is the bugfix release number.</p></li>
</ul>
<p>Backward-incompatibilities are explicitly mentioned in the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#news"><span class="std std-ref">release notes</span></a>,
and may require special attention before upgrading.</p>
<p>Development releases do not follow 3-numbers version and are generally
released as <code class="docutils literal notranslate"><span class="pre">dev</span></code> suffixed versions, e.g. <code class="docutils literal notranslate"><span class="pre">1.3dev</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>With Scrapy 0.* series, Scrapy used <a class="reference external" href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.
This is not the case anymore from Scrapy 1.0 onwards.</p>
<p>Starting with Scrapy 1.0, all releases should be considered production-ready.</p>
</div>
<p>For example:</p>
<ul class="simple">
<li><p><em>1.1.1</em> is the first bugfix release of the <em>1.1</em> series (safe to use in
production)</p></li>
</ul>
</section>
<section id="api-stability">
<h4>API stability<a class="headerlink" href="#api-stability" title="Permalink to this heading">¶</a></h4>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash (<code class="docutils literal notranslate"><span class="pre">_</span></code>) are private and
should never be relied as stable.</p>
<p>Also, keep in mind that stable doesn’t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
</section>
<section id="deprecation-policy">
<span id="id2"></span><h4>Deprecation policy<a class="headerlink" href="#deprecation-policy" title="Permalink to this heading">¶</a></h4>
<p>We aim to maintain support for deprecated Scrapy features for at least 1 year.</p>
<p>For example, if a feature is deprecated in a Scrapy version released on
June 15th 2020, that feature should continue to work in versions released on
June 14th 2021 or before that.</p>
<p>Any new Scrapy release after a year <em>may</em> remove support for that deprecated
feature.</p>
<p>All deprecated features removed in a Scrapy release are explicitly mentioned in
the <a class="hxr-hoverxref hxr-tooltip reference internal" href="index.html#news"><span class="std std-ref">release notes</span></a>.</p>
</section>
</section>
</div>
<dl class="simple">
<dt><a class="reference internal" href="index.html#document-news"><span class="doc">Release notes</span></a></dt><dd><p>See what has changed in recent Scrapy versions.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-contributing"><span class="doc">Contributing to Scrapy</span></a></dt><dd><p>Learn how to contribute to the Scrapy project.</p>
</dd>
<dt><a class="reference internal" href="index.html#document-versioning"><span class="doc">Versioning and API stability</span></a></dt><dd><p>Understand Scrapy versioning and API stability.</p>
</dd>
</dl>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Scrapy developers.
      <span class="lastupdated">Last updated on Nov 19, 2024.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>